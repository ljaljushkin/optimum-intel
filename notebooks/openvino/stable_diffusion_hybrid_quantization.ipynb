{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaed3927-e315-46d3-8889-df3f3bbcbf6b",
   "metadata": {},
   "source": [
    "# Quantize a Stable Diffusion Model with OpenVINO\n",
    "\n",
    "This notebook shows how to quantize a diffusion model with OpenVINO's [Neural Network Compression Framework](https://github.com/openvinotoolkit/nncf) (NNCF).\n",
    "\n",
    "With quantization, we reduce the precision of the model's weights and activations from floating point (FP32) to integer (INT8). This results in a smaller model with faster inference times with OpenVINO Runtime.\n",
    "\n",
    "Traditional optimization methods like post-training 8-bit quantization do not work well for Stable Diffusion models and can lead to poor generation results. On the other hand, weight compression does not improve performance significantly when applied to Stable Diffusion models, as the size of activations is comparable to weights. The UNet model takes up most of the overall execution time of the pipeline. Thus, optimizing just one model brings substantial benefits in terms of inference speed while keeping acceptable accuracy without fine-tuning. Quantizing the rest of the diffusion pipeline does not significantly improve inference performance but could potentially lead to substantial degradation of accuracy.\n",
    "\n",
    "Therefore, the proposal is to apply quantization in *hybrid mode* for the UNet model and weight-only quantization for the rest of the pipeline components. The hybrid mode involves the quantization of weights in MatMul and Embedding layers, and activations of other layers, facilitating accuracy preservation post-optimization while reducing the model size.\n",
    "For more details refers to [documentation](https://github.com/huggingface/optimum-intel/blob/main/docs/source/optimization_ov.mdx).\n",
    "\n",
    "The notebook demonstrates post-training quantization in *hybrid mode*, which does not require specific hardware to execute. A laptop or desktop with a recent Intel Core processor is recommended for best results. To install the requirements for this notebook, please do `pip install -r requirements.txt` or uncomment the cell below to install the requirements in your current Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffab375-a730-4015-8d17-360b76a0718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"optimum-intel[openvino]\" datasets ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407fc92-c052-47b7-8721-01836adf3b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:23.169301Z",
     "iopub.status.busy": "2022-12-04T14:40:23.169096Z",
     "iopub.status.idle": "2022-12-04T14:40:25.146758Z",
     "shell.execute_reply": "2022-12-04T14:40:25.146281Z",
     "shell.execute_reply.started": "2022-12-04T14:40:23.169248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from openvino.runtime import Core\n",
    "from optimum.intel import OVConfig, OVQuantizer, OVStableDiffusionPipeline, OVWeightQuantizationConfig\n",
    "from optimum.intel.openvino.configuration import OVQuantizationMethod\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a16fe-2bc0-477e-b8d6-02a4f7508f03",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "For this tutorial, we use the [jxie/coco_captions](https://huggingface.co/datasets/jxie/coco_captions), dataset contains over 120,000 images, each annotated with multiple captions describing the objects and activities depicted in the image. The notebook was tested with the [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) model. Other [stable diffusion models](https://huggingface.co/models?pipeline_tag=text-to-image) for text-to-image generation should also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f9a76-414b-43d9-9769-af131223f1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:25.147946Z",
     "iopub.status.busy": "2022-12-04T14:40:25.147671Z",
     "iopub.status.idle": "2022-12-04T14:40:25.150427Z",
     "shell.execute_reply": "2022-12-04T14:40:25.150085Z",
     "shell.execute_reply.started": "2022-12-04T14:40:25.147933Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"stabilityai/stable-diffusion-2-1\"\n",
    "DATASET_NAME = \"jxie/coco_captions\"\n",
    "\n",
    "base_model_path = Path(f\"models/{MODEL_ID}\")\n",
    "fp32_model_path = base_model_path.with_name(base_model_path.name + \"_FP32\")\n",
    "int8_model_path = base_model_path.with_name(base_model_path.name + \"_INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dfc2d-f007-4455-8043-edc5468b87e2",
   "metadata": {},
   "source": [
    "## Post-training Hybrid Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483309f-0b12-4c10-ab0b-58c50981f495",
   "metadata": {
    "tags": []
   },
   "source": [
    "To perform post-training optimization on the `OVStableDiffusionPipeline`, you need to provide the `quantization_config` to the `from_pretrained` method.\n",
    "The `quantization_config` is utilized to define optimization parameters for optimizing the Stable Diffusion pipeline. To enable hybrid quantization, specify the quantization dataset in the `quantization_config`. Otherwise, weight-only quantization to a specified data type (8 or 4 bits) is applied to UNet model.\n",
    "That's all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14899a-3aec-46b6-9d4b-332452e9cb25",
   "metadata": {},
   "source": [
    "### Prepare the Dataset\n",
    "\n",
    "We need a small representative calibration dataset to quantize the model. For text-to-image generation we should collect text descriptions or prompts that provide information about the desired image. The `COCO Captions` dataset primarily consists of images paired with multiple human-generated captions describing the contents of the image. Thus, we'll take the `caption` column, which contains descriptions corresponding to the images, as our calibration data.\n",
    "\n",
    "The `datasets` library makes it easy to load datasets. Common datasets can be loaded from the Hugging Face Hub by providing the name of the dataset. See https://github.com/huggingface/datasets. We can load the `jxie/coco_captions` dataset with `load_dataset` and show a random dataset item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(DATASET_NAME, split=\"train\", streaming=True).shuffle(seed=42)\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(example):\n",
    "    return {\"prompt\": example[\"caption\"]}\n",
    "\n",
    "NUM_SAMPLES = 200\n",
    "dataset = dataset.take(NUM_SAMPLES)\n",
    "calibration_dataset = dataset.map(lambda x: preprocess_fn(x), remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe55de8",
   "metadata": {},
   "source": [
    "### Quantize the Model\n",
    "\n",
    "We load the model from the Hugging Face Hub. The model will be automatically downloaded if it has not been downloaded before, or loaded from the cache otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5415e-e22b-4ab9-b903-8791e80b188d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:31.402637Z",
     "iopub.status.busy": "2022-12-04T14:40:31.402463Z",
     "iopub.status.idle": "2022-12-04T14:42:55.379929Z",
     "shell.execute_reply": "2022-12-04T14:42:55.379439Z",
     "shell.execute_reply.started": "2022-12-04T14:40:31.402625Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# int8_pipe = OVStableDiffusionPipeline.from_pretrained(model_id=MODEL_ID, export=True)\n",
    "quantization_config = OVWeightQuantizationConfig(bits=8, num_samples=NUM_SAMPLES, quant_method=OVQuantizationMethod.HYBRID)\n",
    "quantizer = OVQuantizer(int8_pipe)\n",
    "quantizer.quantize(\n",
    "    ov_config=OVConfig(quantization_config=quantization_config),\n",
    "    calibration_dataset=calibration_dataset,\n",
    "    save_directory=int8_model_path\n",
    ")\n",
    "# int8_pipe = OVStableDiffusionPipeline.from_pretrained(model_id=int8_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574cc63-aad3-4c28-aa6f-e553de911ce5",
   "metadata": {},
   "source": [
    "## Compare INT8 and FP32 models\n",
    "\n",
    "We create two pipelines: `int8_pipe` and `fp32_pipe` to compare the FP32 and INT8 OpenVINO models. These pipelines will be used for benchmarking later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddc37f-5504-4eb1-91c7-063d6f1a8174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:42:55.380868Z",
     "iopub.status.busy": "2022-12-04T14:42:55.380738Z",
     "iopub.status.idle": "2022-12-04T14:42:58.064848Z",
     "shell.execute_reply": "2022-12-04T14:42:58.064382Z",
     "shell.execute_reply.started": "2022-12-04T14:42:55.380857Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp32_pipe = OVStableDiffusionPipeline.from_pretrained(model_id=MODEL_ID, export=True)\n",
    "fp32_pipe.save_pretrained(fp32_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a52092-e352-47ef-9ed2-89508bc48d70",
   "metadata": {},
   "source": [
    "### Visual Accuracy\n",
    "Let us check predictions between the original and optimized pipelines using the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f615a-19e3-4ee2-9309-2ae1392c7f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:42:58.497409Z",
     "iopub.status.busy": "2022-12-04T14:42:58.496740Z",
     "iopub.status.idle": "2022-12-04T14:43:47.884825Z",
     "shell.execute_reply": "2022-12-04T14:43:47.884058Z",
     "shell.execute_reply.started": "2022-12-04T14:42:58.497370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_results(fp32_img, int8_img):\n",
    "    im_w, im_h = fp32_img.size\n",
    "    is_horizontal = im_h <= im_w\n",
    "    figsize = (20, 30) if is_horizontal else (30, 20)\n",
    "    fig, axs = plt.subplots(1 if is_horizontal else 2, 2 if is_horizontal else 1, figsize=figsize, sharex='all', sharey='all')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    list_axes = list(axs.flat)\n",
    "    for a in list_axes:\n",
    "        a.set_xticklabels([])\n",
    "        a.set_yticklabels([])\n",
    "        a.get_xaxis().set_visible(False)\n",
    "        a.get_yaxis().set_visible(False)\n",
    "        a.grid(False)\n",
    "    list_axes[0].imshow(np.array(fp32_img))\n",
    "    list_axes[1].imshow(np.array(int8_img))\n",
    "    img1_title = \"FP32 result\"\n",
    "    img2_title = \"INT8 result\"\n",
    "    list_axes[0].set_title(img1_title, fontsize=20)\n",
    "    list_axes[1].set_title(img2_title, fontsize=20)\n",
    "    fig.subplots_adjust(wspace=0.0 if is_horizontal else 0.01 , hspace=0.01 if is_horizontal else 0.0)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90902149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "# prompt = \"close-up photography of old man standing in the rain at night, in a street lit by lamps, leica 35mm summilux\"\n",
    "# prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "prompt = \"the best place in Bayern\"\n",
    "\n",
    "def generate_image(pipeline, prompt):\n",
    "    transformers.set_seed(1)\n",
    "    return pipeline(\n",
    "        prompt=prompt,\n",
    "        guidance_scale=8.0,\n",
    "        output_type=\"pil\"\n",
    "    ).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f01fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_img = generate_image(fp32_pipe, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_img = generate_image(int8_pipe, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(fp32_img, int8_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df445d-af43-4ba1-8195-7d8f00b8f82f",
   "metadata": {},
   "source": [
    "### Model Size\n",
    "\n",
    "We save the FP32 PyTorch model and define a function to show the model size for the PyTorch and OpenVINO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeaa81f-7fc5-49ba-80b8-2d95a1310a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:44:36.261080Z",
     "iopub.status.busy": "2022-12-04T14:44:36.260880Z",
     "iopub.status.idle": "2022-12-04T14:44:37.575213Z",
     "shell.execute_reply": "2022-12-04T14:44:37.574836Z",
     "shell.execute_reply.started": "2022-12-04T14:44:36.261063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_model_size(model_folder, framework):\n",
    "#     \"\"\"\n",
    "#     Return OpenVINO or PyTorch model size in Mb.\n",
    "\n",
    "#     Arguments:\n",
    "#         model_folder:\n",
    "#             Directory containing a model.\n",
    "#         framework:\n",
    "#             Define whether the model is a PyTorch or an OpenVINO model.\n",
    "#     \"\"\"\n",
    "#     model_size = 0\n",
    "#     if framework.lower() == \"openvino\":\n",
    "#         for model_path in Path(model_folder).rglob(\"*.xml\"):\n",
    "#             model_size += model_path.stat().st_size\n",
    "\n",
    "#     extension = \"*.bin\" if framework.lower() == \"openvino\" else \"*.safetensors\"\n",
    "#     for model_path in Path(model_folder).rglob(extension):\n",
    "#         model_size += model_path.stat().st_size\n",
    "#     model_size /= 2**20\n",
    "#     return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91134d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp32_model_size = get_model_size(fp32_model_path, \"openvino\")\n",
    "# int8_model_size = get_model_size(int8_model_path, \"openvino\")\n",
    "# print(f\"FP32 model size: {fp32_model_size:.2f} MB\")\n",
    "# print(f\"INT8 model size: {int8_model_size:.2f} MB\")\n",
    "# print(f\"INT8 size decrease: {fp32_model_size / int8_model_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int8_pipe = OVStableDiffusionPipeline.from_pretrained(model_id=int8_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8697a2-0a51-427f-8245-cda56bb8cf18",
   "metadata": {},
   "source": [
    "### Latency\n",
    "\n",
    "Compare the inference speed of the quantized OpenVINO model with that of the original PyTorch model.\n",
    "\n",
    "This benchmark provides an estimate of performance, but keep in mind that other programs running on the computer, as well as power management settings, can affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806da79-0b3b-403e-a40c-61db6a0f482d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:44:37.575841Z",
     "iopub.status.busy": "2022-12-04T14:44:37.575681Z",
     "iopub.status.idle": "2022-12-04T14:44:53.148782Z",
     "shell.execute_reply": "2022-12-04T14:44:53.147790Z",
     "shell.execute_reply.started": "2022-12-04T14:44:37.575830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_val_dataset(num_items=3):\n",
    "#     return [item[\"caption\"] for item in dataset.take(num_items)]\n",
    "\n",
    "# def benchmark(pipeline, dataset):\n",
    "#     \"\"\"\n",
    "#     Benchmark PyTorch or OpenVINO model. This function does inference on `num_items`\n",
    "#     dataset items and returns the median latency in milliseconds\n",
    "#     \"\"\"\n",
    "#     latencies = []\n",
    "#     for prompt in dataset:\n",
    "#         start_time = time.perf_counter()\n",
    "#         _ = pipeline(prompt=prompt)\n",
    "#         end_time = time.perf_counter()\n",
    "#         latencies.append(end_time - start_time)\n",
    "#     return np.median(latencies) * 1000\n",
    "\n",
    "\n",
    "# cpu_device_name = Core().get_property(\"CPU\", \"FULL_DEVICE_NAME\")\n",
    "# print(cpu_device_name)\n",
    "\n",
    "# val_subset = get_val_dataset()\n",
    "# original_latency = benchmark(fp32_pipe, val_subset)\n",
    "# print(f\"Latency of original FP32 model: {original_latency:.2f} ms\")\n",
    "\n",
    "# quantized_latency = benchmark(int8_pipe, val_subset)\n",
    "# print(f\"Latency of quantized model: {quantized_latency:.2f} ms\")\n",
    "# print(f\"Speedup: {(original_latency/quantized_latency):.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
