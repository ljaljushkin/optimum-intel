INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino
06/19/2023 19:14:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
Resolving data files:   0%|          | 0/172341 [00:00<?, ?it/s]Resolving data files:  10%|▉         | 16854/172341 [00:00<00:00, 168520.07it/s]Resolving data files:  20%|█▉        | 33707/172341 [00:00<00:00, 141392.97it/s]Resolving data files:  28%|██▊       | 48121/172341 [00:00<00:00, 124515.77it/s]Resolving data files:  35%|███▌      | 60830/172341 [00:00<00:01, 102610.87it/s]Resolving data files:  42%|████▏     | 71558/172341 [00:00<00:01, 72826.22it/s] Resolving data files:  46%|████▋     | 79965/172341 [00:00<00:01, 69498.34it/s]Resolving data files:  51%|█████     | 87988/172341 [00:01<00:01, 63382.87it/s]Resolving data files:  55%|█████▍    | 94763/172341 [00:01<00:01, 51550.85it/s]Resolving data files:  58%|█████▊    | 100398/172341 [00:01<00:01, 52492.31it/s]Resolving data files:  64%|██████▍   | 110514/172341 [00:01<00:01, 57310.69it/s]Resolving data files:  68%|██████▊   | 116525/172341 [00:01<00:01, 54100.64it/s]Resolving data files:  71%|███████   | 122094/172341 [00:01<00:00, 52878.02it/s]Resolving data files:  75%|███████▍  | 128662/172341 [00:01<00:00, 52198.67it/s]Resolving data files:  78%|███████▊  | 133940/172341 [00:02<00:00, 50232.85it/s]Resolving data files:  81%|████████▏ | 140330/172341 [00:02<00:00, 52388.68it/s]Resolving data files:  85%|████████▌ | 146615/172341 [00:02<00:00, 46893.36it/s]Resolving data files:  89%|████████▉ | 153622/172341 [00:02<00:00, 47978.60it/s]Resolving data files:  94%|█████████▎| 161488/172341 [00:02<00:00, 53912.74it/s]Resolving data files:  97%|█████████▋| 167865/172341 [00:02<00:00, 55818.02it/s]Resolving data files: 100%|██████████| 172341/172341 [00:03<00:00, 53095.22it/s]
Resolving data files:   0%|          | 0/50000 [00:00<?, ?it/s]Resolving data files:  91%|█████████ | 45545/50000 [00:00<00:00, 438407.69it/s]Resolving data files: 100%|██████████| 50000/50000 [00:01<00:00, 44349.68it/s] 
Downloading and preparing dataset imagefolder/default to /home/nlyaly/.cache/huggingface/datasets/imagefolder/default-0570dc2d3bea4db1/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...
Downloading data files:   0%|          | 0/172341 [00:00<?, ?it/s]Downloading data files:   7%|▋         | 12767/172341 [00:00<00:01, 127660.99it/s]Downloading data files:  15%|█▍        | 25753/172341 [00:00<00:01, 128949.68it/s]Downloading data files:  22%|██▏       | 38755/172341 [00:00<00:01, 129433.94it/s]Downloading data files:  30%|███       | 51773/172341 [00:00<00:00, 129723.99it/s]Downloading data files:  38%|███▊      | 64749/172341 [00:00<00:00, 129732.45it/s]Downloading data files:  45%|████▌     | 77757/172341 [00:00<00:00, 129846.76it/s]Downloading data files:  53%|█████▎    | 90742/172341 [00:00<00:00, 129811.13it/s]Downloading data files:  60%|██████    | 103739/172341 [00:00<00:00, 129858.98it/s]Downloading data files:  68%|██████▊   | 116725/172341 [00:00<00:00, 129451.95it/s]Downloading data files:  75%|███████▌  | 129671/172341 [00:01<00:00, 129144.63it/s]Downloading data files:  83%|████████▎ | 142699/172341 [00:01<00:00, 129488.85it/s]Downloading data files:  90%|█████████ | 155689/172341 [00:01<00:00, 129609.55it/s]Downloading data files:  98%|█████████▊| 168651/172341 [00:01<00:00, 129555.67it/s]Downloading data files: 100%|██████████| 172341/172341 [00:01<00:00, 129492.84it/s]
Downloading data files: 0it [00:00, ?it/s]Downloading data files: 0it [00:00, ?it/s]
Extracting data files: 0it [00:00, ?it/s]Extracting data files: 0it [00:00, ?it/s]
Downloading data files:   0%|          | 0/50000 [00:00<?, ?it/s]Downloading data files:  26%|██▌       | 12832/50000 [00:00<00:00, 128313.09it/s]Downloading data files:  52%|█████▏    | 25773/50000 [00:00<00:00, 128950.79it/s]Downloading data files:  77%|███████▋  | 38669/50000 [00:00<00:00, 128544.20it/s]Downloading data files: 100%|██████████| 50000/50000 [00:00<00:00, 128731.11it/s]
Downloading data files: 0it [00:00, ?it/s]Downloading data files: 0it [00:00, ?it/s]
Extracting data files: 0it [00:00, ?it/s]Extracting data files: 0it [00:00, ?it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 2000 examples [00:00, 19023.94 examples/s]Generating train split: 4083 examples [00:00, 20061.39 examples/s]Generating train split: 6149 examples [00:00, 20327.74 examples/s]Generating train split: 8243 examples [00:00, 20564.75 examples/s]Generating train split: 10335 examples [00:00, 20688.93 examples/s]Generating train split: 13394 examples [00:00, 20554.19 examples/s]Generating train split: 15466 examples [00:00, 20601.78 examples/s]Generating train split: 18559 examples [00:00, 20605.92 examples/s]Generating train split: 20656 examples [00:01, 20702.28 examples/s]Generating train split: 23758 examples [00:01, 20688.77 examples/s]Generating train split: 26855 examples [00:01, 20673.68 examples/s]Generating train split: 28952 examples [00:01, 20743.16 examples/s]Generating train split: 31043 examples [00:01, 20686.54 examples/s]Generating train split: 33116 examples [00:01, 20695.52 examples/s]Generating train split: 35188 examples [00:01, 20700.91 examples/s]Generating train split: 38261 examples [00:01, 20618.60 examples/s]Generating train split: 41323 examples [00:02, 20542.80 examples/s]Generating train split: 44415 examples [00:02, 20562.06 examples/s]Generating train split: 47474 examples [00:02, 20504.83 examples/s]Generating train split: 49534 examples [00:02, 20523.77 examples/s]Generating train split: 51588 examples [00:02, 20526.34 examples/s]Generating train split: 54672 examples [00:02, 20533.90 examples/s]Generating train split: 56750 examples [00:02, 20591.01 examples/s]Generating train split: 58812 examples [00:02, 20596.58 examples/s]Generating train split: 61879 examples [00:03, 20538.79 examples/s]Generating train split: 64912 examples [00:03, 20428.40 examples/s]Generating train split: 67000 examples [00:03, 20452.99 examples/s]Generating train split: 69077 examples [00:03, 20533.27 examples/s]Generating train split: 71166 examples [00:03, 20627.40 examples/s]Generating train split: 73249 examples [00:03, 20679.67 examples/s]Generating train split: 75332 examples [00:03, 20719.35 examples/s]Generating train split: 77416 examples [00:03, 20751.96 examples/s]Generating train split: 79503 examples [00:03, 20783.08 examples/s]Generating train split: 82612 examples [00:04, 20758.44 examples/s]Generating train split: 85705 examples [00:04, 20707.38 examples/s]Generating train split: 87796 examples [00:04, 20756.71 examples/s]Generating train split: 90893 examples [00:04, 20713.93 examples/s]Generating train split: 93977 examples [00:04, 20659.05 examples/s]Generating train split: 97039 examples [00:04, 20498.70 examples/s]Generating train split: 99111 examples [00:04, 20547.63 examples/s]Generating train split: 101181 examples [00:04, 20584.46 examples/s]Generating train split: 103245 examples [00:05, 20597.77 examples/s]Generating train split: 106320 examples [00:05, 20560.54 examples/s]Generating train split: 108379 examples [00:05, 20564.89 examples/s]Generating train split: 111441 examples [00:05, 20508.45 examples/s]Generating train split: 114490 examples [00:05, 20445.32 examples/s]Generating train split: 116559 examples [00:05, 20502.55 examples/s]Generating train split: 118644 examples [00:05, 20590.05 examples/s]Generating train split: 121710 examples [00:05, 20533.97 examples/s]Generating train split: 123779 examples [00:06, 20573.17 examples/s]Generating train split: 125845 examples [00:06, 20592.82 examples/s]Generating train split: 128927 examples [00:06, 20570.64 examples/s]Generating train split: 132000 examples [00:06, 20488.29 examples/s]Generating train split: 134062 examples [00:06, 20518.85 examples/s]Generating train split: 137146 examples [00:06, 20530.32 examples/s]Generating train split: 139218 examples [00:06, 20576.32 examples/s]Generating train split: 141278 examples [00:06, 20577.97 examples/s]Generating train split: 144343 examples [00:07, 20521.66 examples/s]Generating train split: 147364 examples [00:07, 20389.92 examples/s]Generating train split: 149441 examples [00:07, 20479.51 examples/s]Generating train split: 151501 examples [00:07, 20507.88 examples/s]Generating train split: 153559 examples [00:07, 20522.59 examples/s]Generating train split: 155618 examples [00:07, 20540.99 examples/s]Generating train split: 157680 examples [00:07, 20560.40 examples/s]Generating train split: 160752 examples [00:07, 20525.83 examples/s]Generating train split: 162824 examples [00:07, 20573.76 examples/s]Generating train split: 164907 examples [00:08, 20642.36 examples/s]Generating train split: 168000 examples [00:08, 20542.12 examples/s]Generating train split: 170070 examples [00:08, 20580.44 examples/s]Generating train split: 172341 examples [00:08, 20545.58 examples/s]                                                                    Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 2016 examples [00:00, 20056.16 examples/s]Generating validation split: 4041 examples [00:00, 20165.72 examples/s]Generating validation split: 6060 examples [00:00, 20170.92 examples/s]Generating validation split: 8081 examples [00:00, 20183.12 examples/s]Generating validation split: 10111 examples [00:00, 20221.87 examples/s]Generating validation split: 12178 examples [00:00, 20371.71 examples/s]Generating validation split: 14224 examples [00:00, 20395.56 examples/s]Generating validation split: 16281 examples [00:00, 20446.10 examples/s]Generating validation split: 19329 examples [00:00, 20388.37 examples/s]Generating validation split: 21397 examples [00:01, 20468.86 examples/s]Generating validation split: 23457 examples [00:01, 20502.60 examples/s]Generating validation split: 26528 examples [00:01, 20486.19 examples/s]Generating validation split: 29608 examples [00:01, 20499.29 examples/s]Generating validation split: 32655 examples [00:01, 20433.99 examples/s]Generating validation split: 35717 examples [00:01, 20423.38 examples/s]Generating validation split: 37782 examples [00:01, 20473.71 examples/s]Generating validation split: 40834 examples [00:02, 20427.31 examples/s]Generating validation split: 42882 examples [00:02, 20438.76 examples/s]Generating validation split: 45933 examples [00:02, 20402.97 examples/s]Generating validation split: 47986 examples [00:02, 20433.11 examples/s]                                                                        Dataset imagefolder downloaded and prepared to /home/nlyaly/.cache/huggingface/datasets/imagefolder/default-0570dc2d3bea4db1/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  1.63it/s]100%|██████████| 2/2 [00:01<00:00,  1.65it/s]100%|██████████| 2/2 [00:01<00:00,  1.64it/s]
Casting the dataset:   0%|          | 0/172341 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 172341/172341 [00:00<00:00, 268487.02 examples/s]                                                                                       Casting the dataset:   0%|          | 0/50000 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 50000/50000 [00:00<00:00, 80377.26 examples/s]                                                                                    create_compressed_model

/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.
  warnings.warn(
ViTForImageClassification(
  (vit): ViTModel(
    (embeddings): ViTEmbeddings(
      (patch_embeddings): ViTPatchEmbeddings(
        (projection): NNCFConv2d(
          3, 768, kernel_size=(16, 16), stride=(16, 16)
          (pre_ops): ModuleDict()
          (post_ops): ModuleDict()
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): ViTEncoder(
      (layer): ModuleList(
        (0): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (1): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (2): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (3): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (4): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (5): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (6): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (7): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (8): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (9): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (10): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (11): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
      )
    )
    (layernorm): NNCFLayerNorm(
      (768,), eps=1e-12, elementwise_affine=True
      (pre_ops): ModuleDict()
      (post_ops): ModuleDict()
    )
  )
  (classifier): NNCFLinear(
    in_features=768, out_features=1000, bias=True
    (pre_ops): ModuleDict(
      (0): UpdateWeight(
        (op): SymmetricQuantizer(bit=8, ch=True)
      )
    )
    (post_ops): ModuleDict()
  )
  (_nncf): NNCFNetworkInterface(
    (external_quantizers): ModuleDict(
      (ViTForImageClassification/ViTModel[vit]/NNCFLayerNorm[layernorm]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
    )
  )
)/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(

process layer0
 With wd:
attention.attention.query.weight
attention.attention.key.weight
attention.attention.value.weight
attention.output.dense.weight
intermediate.dense.weight
output.dense.weight
layernorm_before.weight
layernorm_after.weight
 No_wd:
attention.attention.query.bias
attention.attention.key.bias
attention.attention.value.bias
attention.output.dense.bias
intermediate.dense.bias
output.dense.bias
layernorm_before.bias
layernorm_after.bias
=) 46.11% for 0
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:04<03:23,  2.07s/it]  3%|▎         | 3/100 [00:09<05:32,  3.43s/it]  4%|▍         | 4/100 [00:15<06:50,  4.27s/it]  5%|▌         | 5/100 [00:20<07:07,  4.50s/it]  6%|▌         | 6/100 [00:23<06:29,  4.15s/it]  7%|▋         | 7/100 [00:27<06:07,  3.95s/it]  8%|▊         | 8/100 [00:31<06:11,  4.04s/it]  9%|▉         | 9/100 [00:35<05:58,  3.94s/it] 10%|█         | 10/100 [00:38<05:50,  3.89s/it] 11%|█         | 11/100 [00:42<05:38,  3.80s/it] 12%|█▏        | 12/100 [00:45<05:27,  3.72s/it] 13%|█▎        | 13/100 [00:49<05:28,  3.77s/it] 14%|█▍        | 14/100 [00:53<05:19,  3.72s/it] 15%|█▌        | 15/100 [00:56<05:10,  3.65s/it] 16%|█▌        | 16/100 [01:00<05:01,  3.59s/it] 17%|█▋        | 17/100 [01:03<04:57,  3.58s/it] 18%|█▊        | 18/100 [01:07<04:53,  3.58s/it] 19%|█▉        | 19/100 [01:10<04:46,  3.53s/it] 20%|██        | 20/100 [01:14<04:41,  3.52s/it] 21%|██        | 21/100 [01:17<04:35,  3.48s/it] 22%|██▏       | 22/100 [01:21<04:35,  3.53s/it] 23%|██▎       | 23/100 [01:24<04:28,  3.49s/it] 24%|██▍       | 24/100 [01:28<04:29,  3.55s/it] 25%|██▌       | 25/100 [01:32<04:24,  3.53s/it] 26%|██▌       | 26/100 [01:35<04:22,  3.55s/it] 27%|██▋       | 27/100 [01:39<04:26,  3.65s/it] 28%|██▊       | 28/100 [01:43<04:22,  3.65s/it] 29%|██▉       | 29/100 [01:46<04:17,  3.62s/it] 30%|███       | 30/100 [01:50<04:16,  3.66s/it] 31%|███       | 31/100 [01:54<04:11,  3.64s/it] 32%|███▏      | 32/100 [01:57<04:08,  3.66s/it] 33%|███▎      | 33/100 [02:01<04:08,  3.72s/it] 34%|███▍      | 34/100 [02:05<04:00,  3.65s/it] 35%|███▌      | 35/100 [02:08<03:57,  3.65s/it] 36%|███▌      | 36/100 [02:12<03:51,  3.62s/it] 37%|███▋      | 37/100 [02:15<03:43,  3.55s/it] 38%|███▊      | 38/100 [02:19<03:45,  3.64s/it] 39%|███▉      | 39/100 [02:23<03:47,  3.74s/it] 40%|████      | 40/100 [02:28<04:05,  4.09s/it] 41%|████      | 41/100 [02:31<03:51,  3.92s/it] 42%|████▏     | 42/100 [02:35<03:38,  3.77s/it] 43%|████▎     | 43/100 [02:38<03:31,  3.71s/it] 44%|████▍     | 44/100 [02:42<03:21,  3.61s/it] 45%|████▌     | 45/100 [02:45<03:15,  3.55s/it] 46%|████▌     | 46/100 [02:49<03:14,  3.60s/it] 47%|████▋     | 47/100 [02:53<03:16,  3.70s/it] 48%|████▊     | 48/100 [02:56<03:07,  3.61s/it] 49%|████▉     | 49/100 [03:00<03:02,  3.57s/it] 50%|█████     | 50/100 [03:03<02:57,  3.55s/it] 51%|█████     | 51/100 [03:07<02:54,  3.55s/it] 52%|█████▏    | 52/100 [03:10<02:47,  3.50s/it] 53%|█████▎    | 53/100 [03:14<02:50,  3.63s/it] 54%|█████▍    | 54/100 [03:18<02:45,  3.59s/it] 55%|█████▌    | 55/100 [03:21<02:39,  3.54s/it] 56%|█████▌    | 56/100 [03:25<02:35,  3.53s/it] 57%|█████▋    | 57/100 [03:28<02:31,  3.53s/it] 58%|█████▊    | 58/100 [03:32<02:27,  3.51s/it] 59%|█████▉    | 59/100 [03:35<02:26,  3.57s/it] 60%|██████    | 60/100 [03:39<02:24,  3.62s/it] 61%|██████    | 61/100 [03:42<02:18,  3.54s/it] 62%|██████▏   | 62/100 [03:46<02:13,  3.52s/it] 63%|██████▎   | 63/100 [03:49<02:09,  3.50s/it] 64%|██████▍   | 64/100 [03:53<02:07,  3.53s/it] 65%|██████▌   | 65/100 [03:56<02:03,  3.53s/it] 66%|██████▌   | 66/100 [04:00<02:05,  3.68s/it] 67%|██████▋   | 67/100 [04:06<02:22,  4.31s/it] 68%|██████▊   | 68/100 [04:11<02:19,  4.36s/it] 69%|██████▉   | 69/100 [04:14<02:07,  4.11s/it] 70%|███████   | 70/100 [04:18<01:58,  3.93s/it] 71%|███████   | 71/100 [04:21<01:50,  3.82s/it] 72%|███████▏  | 72/100 [04:25<01:45,  3.76s/it] 73%|███████▎  | 73/100 [04:29<01:41,  3.75s/it] 74%|███████▍  | 74/100 [04:32<01:36,  3.73s/it] 75%|███████▌  | 75/100 [04:36<01:33,  3.74s/it] 76%|███████▌  | 76/100 [04:40<01:28,  3.70s/it] 77%|███████▋  | 77/100 [04:43<01:23,  3.64s/it] 78%|███████▊  | 78/100 [04:47<01:20,  3.66s/it] 79%|███████▉  | 79/100 [04:50<01:16,  3.64s/it] 80%|████████  | 80/100 [04:54<01:11,  3.57s/it] 81%|████████  | 81/100 [04:57<01:07,  3.57s/it] 82%|████████▏ | 82/100 [05:01<01:04,  3.58s/it] 83%|████████▎ | 83/100 [05:05<01:00,  3.57s/it] 84%|████████▍ | 84/100 [05:09<00:59,  3.69s/it] 85%|████████▌ | 85/100 [05:14<01:01,  4.13s/it] 86%|████████▌ | 86/100 [05:17<00:54,  3.91s/it] 87%|████████▋ | 87/100 [05:21<00:49,  3.78s/it] 88%|████████▊ | 88/100 [05:24<00:44,  3.71s/it] 89%|████████▉ | 89/100 [05:28<00:40,  3.65s/it] 90%|█████████ | 90/100 [05:31<00:36,  3.70s/it] 91%|█████████ | 91/100 [05:35<00:32,  3.66s/it] 92%|█████████▏| 92/100 [05:38<00:28,  3.59s/it] 93%|█████████▎| 93/100 [05:42<00:24,  3.54s/it] 94%|█████████▍| 94/100 [05:46<00:21,  3.57s/it] 95%|█████████▌| 95/100 [05:49<00:17,  3.56s/it] 96%|█████████▌| 96/100 [05:52<00:14,  3.52s/it] 97%|█████████▋| 97/100 [05:56<00:10,  3.60s/it] 98%|█████████▊| 98/100 [06:00<00:07,  3.64s/it] 99%|█████████▉| 99/100 [06:04<00:03,  3.71s/it]100%|██████████| 100/100 [06:08<00:00,  3.84s/it]100%|██████████| 100/100 [06:08<00:00,  3.69s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer1
=) 44.41% for 1
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:15,  1.99s/it]  3%|▎         | 3/100 [00:08<04:39,  2.88s/it]  4%|▍         | 4/100 [00:11<05:02,  3.15s/it]  5%|▌         | 5/100 [00:15<05:12,  3.29s/it]  6%|▌         | 6/100 [00:18<05:20,  3.41s/it]  7%|▋         | 7/100 [00:22<05:23,  3.48s/it]  8%|▊         | 8/100 [00:26<05:31,  3.61s/it]  9%|▉         | 9/100 [00:29<05:25,  3.58s/it] 10%|█         | 10/100 [00:33<05:25,  3.61s/it] 11%|█         | 11/100 [00:37<05:17,  3.57s/it] 12%|█▏        | 12/100 [00:40<05:12,  3.55s/it] 13%|█▎        | 13/100 [00:44<05:16,  3.64s/it] 14%|█▍        | 14/100 [00:48<05:13,  3.65s/it] 15%|█▌        | 15/100 [00:51<05:06,  3.61s/it] 16%|█▌        | 16/100 [00:55<05:00,  3.58s/it] 17%|█▋        | 17/100 [00:58<04:55,  3.56s/it] 18%|█▊        | 18/100 [01:02<04:52,  3.57s/it] 19%|█▉        | 19/100 [01:05<04:45,  3.52s/it] 20%|██        | 20/100 [01:09<04:41,  3.52s/it] 21%|██        | 21/100 [01:12<04:37,  3.51s/it] 22%|██▏       | 22/100 [01:16<04:34,  3.52s/it] 23%|██▎       | 23/100 [01:19<04:28,  3.49s/it] 24%|██▍       | 24/100 [01:23<04:24,  3.49s/it] 25%|██▌       | 25/100 [01:26<04:26,  3.55s/it] 26%|██▌       | 26/100 [01:30<04:23,  3.57s/it] 27%|██▋       | 27/100 [01:34<04:21,  3.59s/it] 28%|██▊       | 28/100 [01:38<04:29,  3.75s/it] 29%|██▉       | 29/100 [01:42<04:31,  3.82s/it] 30%|███       | 30/100 [01:45<04:27,  3.82s/it] 31%|███       | 31/100 [01:49<04:18,  3.75s/it] 32%|███▏      | 32/100 [01:53<04:13,  3.72s/it] 33%|███▎      | 33/100 [01:57<04:17,  3.85s/it] 34%|███▍      | 34/100 [02:00<04:02,  3.68s/it] 35%|███▌      | 35/100 [02:04<03:54,  3.61s/it] 36%|███▌      | 36/100 [02:08<04:00,  3.76s/it] 37%|███▋      | 37/100 [02:13<04:27,  4.24s/it] 38%|███▊      | 38/100 [02:18<04:37,  4.48s/it] 39%|███▉      | 39/100 [02:23<04:43,  4.65s/it] 40%|████      | 40/100 [02:29<04:52,  4.88s/it] 41%|████      | 41/100 [02:32<04:28,  4.56s/it] 42%|████▏     | 42/100 [02:36<04:04,  4.22s/it] 43%|████▎     | 43/100 [02:39<03:50,  4.05s/it] 44%|████▍     | 44/100 [02:43<03:38,  3.89s/it] 45%|████▌     | 45/100 [02:46<03:26,  3.76s/it] 46%|████▌     | 46/100 [02:50<03:23,  3.76s/it] 47%|████▋     | 47/100 [02:54<03:21,  3.80s/it] 48%|████▊     | 48/100 [02:58<03:12,  3.70s/it] 49%|████▉     | 49/100 [03:01<03:04,  3.62s/it] 50%|█████     | 50/100 [03:05<03:01,  3.63s/it] 51%|█████     | 51/100 [03:08<02:57,  3.62s/it] 52%|█████▏    | 52/100 [03:12<02:53,  3.61s/it] 53%|█████▎    | 53/100 [03:16<02:51,  3.64s/it] 54%|█████▍    | 54/100 [03:19<02:46,  3.61s/it] 55%|█████▌    | 55/100 [03:23<02:46,  3.70s/it] 56%|█████▌    | 56/100 [03:27<02:42,  3.68s/it] 57%|█████▋    | 57/100 [03:30<02:36,  3.63s/it] 58%|█████▊    | 58/100 [03:34<02:31,  3.60s/it] 59%|█████▉    | 59/100 [03:38<02:37,  3.83s/it] 60%|██████    | 60/100 [03:44<03:01,  4.53s/it] 61%|██████    | 61/100 [03:50<03:09,  4.86s/it] 62%|██████▏   | 62/100 [03:55<03:02,  4.80s/it] 63%|██████▎   | 63/100 [03:59<02:51,  4.64s/it] 64%|██████▍   | 64/100 [04:02<02:33,  4.28s/it] 65%|██████▌   | 65/100 [04:06<02:22,  4.08s/it] 66%|██████▌   | 66/100 [04:09<02:11,  3.88s/it] 67%|██████▋   | 67/100 [04:13<02:04,  3.79s/it] 68%|██████▊   | 68/100 [04:16<01:58,  3.69s/it] 69%|██████▉   | 69/100 [04:20<01:52,  3.64s/it] 70%|███████   | 70/100 [04:23<01:48,  3.61s/it] 71%|███████   | 71/100 [04:27<01:43,  3.58s/it] 72%|███████▏  | 72/100 [04:31<01:41,  3.62s/it] 73%|███████▎  | 73/100 [04:34<01:37,  3.60s/it] 74%|███████▍  | 74/100 [04:38<01:33,  3.60s/it] 75%|███████▌  | 75/100 [04:41<01:29,  3.58s/it] 76%|███████▌  | 76/100 [04:45<01:26,  3.61s/it] 77%|███████▋  | 77/100 [04:48<01:22,  3.57s/it] 78%|███████▊  | 78/100 [04:52<01:19,  3.60s/it] 79%|███████▉  | 79/100 [04:56<01:16,  3.63s/it] 80%|████████  | 80/100 [04:59<01:10,  3.55s/it] 81%|████████  | 81/100 [05:03<01:07,  3.56s/it] 82%|████████▏ | 82/100 [05:06<01:05,  3.62s/it] 83%|████████▎ | 83/100 [05:10<01:00,  3.57s/it] 84%|████████▍ | 84/100 [05:14<00:57,  3.60s/it] 85%|████████▌ | 85/100 [05:17<00:54,  3.63s/it] 86%|████████▌ | 86/100 [05:21<00:50,  3.62s/it] 87%|████████▋ | 87/100 [05:25<00:47,  3.63s/it] 88%|████████▊ | 88/100 [05:28<00:43,  3.61s/it] 89%|████████▉ | 89/100 [05:32<00:39,  3.63s/it] 90%|█████████ | 90/100 [05:36<00:37,  3.74s/it] 91%|█████████ | 91/100 [05:40<00:35,  3.93s/it] 92%|█████████▏| 92/100 [05:44<00:31,  3.93s/it] 93%|█████████▎| 93/100 [05:49<00:29,  4.26s/it] 94%|█████████▍| 94/100 [05:54<00:25,  4.32s/it] 95%|█████████▌| 95/100 [05:57<00:20,  4.19s/it] 96%|█████████▌| 96/100 [06:02<00:16,  4.15s/it] 97%|█████████▋| 97/100 [06:05<00:11,  3.98s/it] 98%|█████████▊| 98/100 [06:09<00:07,  3.91s/it] 99%|█████████▉| 99/100 [06:12<00:03,  3.82s/it]100%|██████████| 100/100 [06:17<00:00,  3.90s/it]100%|██████████| 100/100 [06:17<00:00,  3.77s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer2
=) 36.04% for 2
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:04<03:29,  2.14s/it]  3%|▎         | 3/100 [00:09<05:26,  3.36s/it]  4%|▍         | 4/100 [00:15<07:12,  4.51s/it]  5%|▌         | 5/100 [00:20<07:25,  4.69s/it]  6%|▌         | 6/100 [00:24<06:47,  4.33s/it]  7%|▋         | 7/100 [00:27<06:19,  4.08s/it]  8%|▊         | 8/100 [00:31<06:06,  3.99s/it]  9%|▉         | 9/100 [00:35<05:48,  3.83s/it] 10%|█         | 10/100 [00:38<05:35,  3.72s/it] 11%|█         | 11/100 [00:42<05:25,  3.65s/it] 12%|█▏        | 12/100 [00:46<05:29,  3.74s/it] 13%|█▎        | 13/100 [00:50<05:28,  3.78s/it] 14%|█▍        | 14/100 [00:53<05:26,  3.80s/it] 15%|█▌        | 15/100 [00:57<05:14,  3.70s/it] 16%|█▌        | 16/100 [01:00<05:05,  3.64s/it] 17%|█▋        | 17/100 [01:04<05:07,  3.70s/it] 18%|█▊        | 18/100 [01:10<05:50,  4.27s/it] 19%|█▉        | 19/100 [01:14<05:33,  4.11s/it] 20%|██        | 20/100 [01:17<05:13,  3.92s/it] 21%|██        | 21/100 [01:21<05:16,  4.01s/it] 22%|██▏       | 22/100 [01:26<05:34,  4.29s/it] 23%|██▎       | 23/100 [01:30<05:23,  4.21s/it] 24%|██▍       | 24/100 [01:37<06:27,  5.10s/it] 25%|██▌       | 25/100 [01:44<06:54,  5.52s/it] 26%|██▌       | 26/100 [01:48<06:17,  5.10s/it] 27%|██▋       | 27/100 [01:51<05:37,  4.62s/it] 28%|██▊       | 28/100 [01:55<05:10,  4.31s/it] 29%|██▉       | 29/100 [01:59<04:55,  4.16s/it] 30%|███       | 30/100 [02:03<04:45,  4.08s/it] 31%|███       | 31/100 [02:06<04:32,  3.94s/it] 32%|███▏      | 32/100 [02:10<04:28,  3.95s/it] 33%|███▎      | 33/100 [02:14<04:26,  3.97s/it] 34%|███▍      | 34/100 [02:18<04:14,  3.86s/it] 35%|███▌      | 35/100 [02:22<04:12,  3.88s/it] 36%|███▌      | 36/100 [02:26<04:13,  3.96s/it] 37%|███▋      | 37/100 [02:30<04:07,  3.93s/it] 38%|███▊      | 38/100 [02:34<03:59,  3.87s/it] 39%|███▉      | 39/100 [02:37<03:54,  3.85s/it] 40%|████      | 40/100 [02:41<03:48,  3.81s/it] 41%|████      | 41/100 [02:45<03:37,  3.69s/it] 42%|████▏     | 42/100 [02:48<03:30,  3.63s/it] 43%|████▎     | 43/100 [02:52<03:25,  3.61s/it] 44%|████▍     | 44/100 [02:55<03:21,  3.60s/it] 45%|████▌     | 45/100 [02:59<03:22,  3.69s/it] 46%|████▌     | 46/100 [03:03<03:24,  3.78s/it] 47%|████▋     | 47/100 [03:07<03:27,  3.92s/it] 48%|████▊     | 48/100 [03:11<03:15,  3.76s/it] 49%|████▉     | 49/100 [03:14<03:06,  3.66s/it] 50%|█████     | 50/100 [03:18<03:02,  3.64s/it] 51%|█████     | 51/100 [03:22<03:01,  3.70s/it] 52%|█████▏    | 52/100 [03:25<02:53,  3.61s/it] 53%|█████▎    | 53/100 [03:29<02:51,  3.64s/it] 54%|█████▍    | 54/100 [03:32<02:45,  3.60s/it] 55%|█████▌    | 55/100 [03:36<02:42,  3.60s/it] 56%|█████▌    | 56/100 [03:40<02:41,  3.66s/it] 57%|█████▋    | 57/100 [03:43<02:35,  3.61s/it] 58%|█████▊    | 58/100 [03:47<02:33,  3.65s/it] 59%|█████▉    | 59/100 [03:51<02:33,  3.74s/it] 60%|██████    | 60/100 [03:55<02:36,  3.90s/it] 61%|██████    | 61/100 [04:00<02:41,  4.15s/it] 62%|██████▏   | 62/100 [04:06<02:56,  4.65s/it] 63%|██████▎   | 63/100 [04:10<02:46,  4.50s/it] 64%|██████▍   | 64/100 [04:14<02:43,  4.54s/it] 65%|██████▌   | 65/100 [04:20<02:47,  4.77s/it] 66%|██████▌   | 66/100 [04:24<02:32,  4.49s/it] 67%|██████▋   | 67/100 [04:28<02:30,  4.55s/it] 68%|██████▊   | 68/100 [04:33<02:25,  4.54s/it] 69%|██████▉   | 69/100 [04:37<02:19,  4.48s/it] 70%|███████   | 70/100 [04:41<02:08,  4.27s/it] 71%|███████   | 71/100 [04:44<01:57,  4.04s/it] 72%|███████▏  | 72/100 [04:48<01:52,  4.00s/it] 73%|███████▎  | 73/100 [04:52<01:46,  3.94s/it] 74%|███████▍  | 74/100 [04:56<01:41,  3.90s/it] 75%|███████▌  | 75/100 [04:59<01:34,  3.79s/it] 76%|███████▌  | 76/100 [05:03<01:29,  3.74s/it] 77%|███████▋  | 77/100 [05:07<01:27,  3.80s/it] 78%|███████▊  | 78/100 [05:11<01:24,  3.84s/it] 79%|███████▉  | 79/100 [05:15<01:23,  3.96s/it] 80%|████████  | 80/100 [05:19<01:18,  3.93s/it] 81%|████████  | 81/100 [05:23<01:13,  3.87s/it] 82%|████████▏ | 82/100 [05:26<01:08,  3.81s/it] 83%|████████▎ | 83/100 [05:30<01:03,  3.71s/it] 84%|████████▍ | 84/100 [05:34<00:58,  3.68s/it] 85%|████████▌ | 85/100 [05:37<00:54,  3.63s/it] 86%|████████▌ | 86/100 [05:40<00:50,  3.57s/it] 87%|████████▋ | 87/100 [05:44<00:46,  3.57s/it] 88%|████████▊ | 88/100 [05:48<00:43,  3.63s/it] 89%|████████▉ | 89/100 [05:51<00:39,  3.59s/it] 90%|█████████ | 90/100 [05:55<00:35,  3.58s/it] 91%|█████████ | 91/100 [05:59<00:32,  3.64s/it] 92%|█████████▏| 92/100 [06:02<00:29,  3.67s/it] 93%|█████████▎| 93/100 [06:06<00:26,  3.74s/it] 94%|█████████▍| 94/100 [06:11<00:24,  4.06s/it] 95%|█████████▌| 95/100 [06:15<00:20,  4.00s/it] 96%|█████████▌| 96/100 [06:18<00:15,  3.78s/it] 97%|█████████▋| 97/100 [06:22<00:11,  3.75s/it] 98%|█████████▊| 98/100 [06:26<00:07,  3.87s/it] 99%|█████████▉| 99/100 [06:30<00:04,  4.03s/it]100%|██████████| 100/100 [06:35<00:00,  4.32s/it]100%|██████████| 100/100 [06:36<00:00,  3.96s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer3
=) 32.97% for 3
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:04<03:33,  2.18s/it]  3%|▎         | 3/100 [00:08<04:42,  2.92s/it]  4%|▍         | 4/100 [00:11<05:05,  3.18s/it]  5%|▌         | 5/100 [00:15<05:12,  3.29s/it]  6%|▌         | 6/100 [00:18<05:16,  3.36s/it]  7%|▋         | 7/100 [00:22<05:21,  3.46s/it]  8%|▊         | 8/100 [00:26<05:41,  3.71s/it]  9%|▉         | 9/100 [00:30<05:33,  3.67s/it] 10%|█         | 10/100 [00:34<05:29,  3.66s/it] 11%|█         | 11/100 [00:37<05:20,  3.61s/it] 12%|█▏        | 12/100 [00:41<05:13,  3.57s/it] 13%|█▎        | 13/100 [00:44<05:18,  3.67s/it] 14%|█▍        | 14/100 [00:48<05:15,  3.67s/it] 15%|█▌        | 15/100 [00:52<05:20,  3.77s/it] 16%|█▌        | 16/100 [00:56<05:13,  3.73s/it] 17%|█▋        | 17/100 [00:59<05:03,  3.66s/it] 18%|█▊        | 18/100 [01:03<04:56,  3.62s/it] 19%|█▉        | 19/100 [01:06<04:55,  3.65s/it] 20%|██        | 20/100 [01:11<05:02,  3.79s/it] 21%|██        | 21/100 [01:14<04:49,  3.66s/it] 22%|██▏       | 22/100 [01:18<04:43,  3.63s/it] 23%|██▎       | 23/100 [01:21<04:35,  3.58s/it] 24%|██▍       | 24/100 [01:25<04:33,  3.59s/it] 25%|██▌       | 25/100 [01:28<04:32,  3.63s/it] 26%|██▌       | 26/100 [01:32<04:25,  3.59s/it] 27%|██▋       | 27/100 [01:36<04:24,  3.62s/it] 28%|██▊       | 28/100 [01:40<04:49,  4.02s/it] 29%|██▉       | 29/100 [01:46<05:20,  4.51s/it] 30%|███       | 30/100 [01:51<05:18,  4.54s/it] 31%|███       | 31/100 [01:54<04:53,  4.25s/it] 32%|███▏      | 32/100 [01:58<04:36,  4.07s/it] 33%|███▎      | 33/100 [02:02<04:26,  3.98s/it] 34%|███▍      | 34/100 [02:05<04:10,  3.80s/it] 35%|███▌      | 35/100 [02:09<04:00,  3.70s/it] 36%|███▌      | 36/100 [02:12<03:54,  3.67s/it] 37%|███▋      | 37/100 [02:16<03:55,  3.74s/it] 38%|███▊      | 38/100 [02:21<04:20,  4.21s/it] 39%|███▉      | 39/100 [02:27<04:49,  4.75s/it] 40%|████      | 40/100 [02:32<04:40,  4.68s/it] 41%|████      | 41/100 [02:35<04:11,  4.26s/it] 42%|████▏     | 42/100 [02:39<03:56,  4.07s/it] 43%|████▎     | 43/100 [02:43<03:47,  3.99s/it] 44%|████▍     | 44/100 [02:46<03:33,  3.81s/it] 45%|████▌     | 45/100 [02:50<03:24,  3.73s/it] 46%|████▌     | 46/100 [02:53<03:20,  3.71s/it] 47%|████▋     | 47/100 [02:57<03:16,  3.71s/it] 48%|████▊     | 48/100 [03:00<03:08,  3.63s/it] 49%|████▉     | 49/100 [03:04<03:06,  3.66s/it] 50%|█████     | 50/100 [03:08<03:04,  3.69s/it] 51%|█████     | 51/100 [03:12<03:00,  3.69s/it] 52%|█████▏    | 52/100 [03:15<02:53,  3.62s/it] 53%|█████▎    | 53/100 [03:19<02:51,  3.66s/it] 54%|█████▍    | 54/100 [03:22<02:45,  3.59s/it] 55%|█████▌    | 55/100 [03:26<02:43,  3.62s/it] 56%|█████▌    | 56/100 [03:29<02:38,  3.60s/it] 57%|█████▋    | 57/100 [03:33<02:34,  3.58s/it] 58%|█████▊    | 58/100 [03:36<02:29,  3.55s/it] 59%|█████▉    | 59/100 [03:40<02:27,  3.61s/it] 60%|██████    | 60/100 [03:44<02:29,  3.75s/it] 61%|██████    | 61/100 [03:48<02:23,  3.68s/it] 62%|██████▏   | 62/100 [03:51<02:18,  3.65s/it] 63%|██████▎   | 63/100 [03:55<02:13,  3.62s/it] 64%|██████▍   | 64/100 [03:58<02:08,  3.58s/it] 65%|██████▌   | 65/100 [04:03<02:11,  3.75s/it] 66%|██████▌   | 66/100 [04:06<02:06,  3.71s/it] 67%|██████▋   | 67/100 [04:10<02:00,  3.66s/it] 68%|██████▊   | 68/100 [04:13<01:58,  3.70s/it] 69%|██████▉   | 69/100 [04:17<01:54,  3.68s/it] 70%|███████   | 70/100 [04:21<01:50,  3.69s/it] 71%|███████   | 71/100 [04:24<01:46,  3.66s/it] 72%|███████▏  | 72/100 [04:28<01:41,  3.62s/it] 73%|███████▎  | 73/100 [04:31<01:37,  3.60s/it] 74%|███████▍  | 74/100 [04:35<01:32,  3.58s/it] 75%|███████▌  | 75/100 [04:38<01:27,  3.50s/it] 76%|███████▌  | 76/100 [04:42<01:23,  3.50s/it] 77%|███████▋  | 77/100 [04:45<01:21,  3.54s/it] 78%|███████▊  | 78/100 [04:50<01:21,  3.69s/it] 79%|███████▉  | 79/100 [04:53<01:17,  3.67s/it] 80%|████████  | 80/100 [04:57<01:11,  3.59s/it] 81%|████████  | 81/100 [05:00<01:08,  3.62s/it] 82%|████████▏ | 82/100 [05:04<01:05,  3.65s/it] 83%|████████▎ | 83/100 [05:08<01:02,  3.69s/it] 84%|████████▍ | 84/100 [05:11<00:57,  3.62s/it] 85%|████████▌ | 85/100 [05:16<00:58,  3.91s/it] 86%|████████▌ | 86/100 [05:20<00:56,  4.01s/it] 87%|████████▋ | 87/100 [05:23<00:49,  3.80s/it] 88%|████████▊ | 88/100 [05:28<00:46,  3.91s/it] 89%|████████▉ | 89/100 [05:33<00:48,  4.37s/it] 90%|█████████ | 90/100 [05:38<00:44,  4.44s/it] 91%|█████████ | 91/100 [05:42<00:39,  4.41s/it] 92%|█████████▏| 92/100 [05:47<00:36,  4.54s/it] 93%|█████████▎| 93/100 [05:50<00:29,  4.18s/it] 94%|█████████▍| 94/100 [05:54<00:24,  4.02s/it] 95%|█████████▌| 95/100 [05:57<00:19,  3.90s/it] 96%|█████████▌| 96/100 [06:01<00:15,  3.78s/it] 97%|█████████▋| 97/100 [06:04<00:11,  3.71s/it] 98%|█████████▊| 98/100 [06:08<00:07,  3.76s/it] 99%|█████████▉| 99/100 [06:12<00:03,  3.78s/it]100%|██████████| 100/100 [06:16<00:00,  3.78s/it]100%|██████████| 100/100 [06:16<00:00,  3.77s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer4
=) 36.20% for 4
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:58,  1.82s/it]  3%|▎         | 3/100 [00:07<04:16,  2.65s/it]  4%|▍         | 4/100 [00:10<04:46,  2.98s/it]  5%|▌         | 5/100 [00:14<05:01,  3.17s/it]  6%|▌         | 6/100 [00:18<05:08,  3.28s/it]  7%|▋         | 7/100 [00:21<05:13,  3.37s/it]  8%|▊         | 8/100 [00:25<05:31,  3.61s/it]  9%|▉         | 9/100 [00:29<05:25,  3.58s/it] 10%|█         | 10/100 [00:32<05:19,  3.55s/it] 11%|█         | 11/100 [00:36<05:13,  3.52s/it] 12%|█▏        | 12/100 [00:39<05:09,  3.52s/it] 13%|█▎        | 13/100 [00:43<05:12,  3.59s/it] 14%|█▍        | 14/100 [00:47<05:12,  3.64s/it] 15%|█▌        | 15/100 [00:50<05:10,  3.66s/it] 16%|█▌        | 16/100 [00:55<05:33,  3.97s/it] 17%|█▋        | 17/100 [00:59<05:26,  3.93s/it] 18%|█▊        | 18/100 [01:03<05:14,  3.83s/it] 19%|█▉        | 19/100 [01:07<05:20,  3.95s/it] 20%|██        | 20/100 [01:12<05:35,  4.20s/it] 21%|██        | 21/100 [01:15<05:26,  4.13s/it] 22%|██▏       | 22/100 [01:19<05:08,  3.96s/it] 23%|██▎       | 23/100 [01:23<04:54,  3.82s/it] 24%|██▍       | 24/100 [01:26<04:52,  3.85s/it] 25%|██▌       | 25/100 [01:30<04:44,  3.79s/it] 26%|██▌       | 26/100 [01:33<04:29,  3.64s/it] 27%|██▋       | 27/100 [01:37<04:28,  3.68s/it] 28%|██▊       | 28/100 [01:41<04:38,  3.86s/it] 29%|██▉       | 29/100 [01:45<04:29,  3.79s/it] 30%|███       | 30/100 [01:49<04:30,  3.86s/it] 31%|███       | 31/100 [01:54<04:44,  4.13s/it] 32%|███▏      | 32/100 [02:00<05:19,  4.69s/it] 33%|███▎      | 33/100 [02:06<05:45,  5.15s/it] 34%|███▍      | 34/100 [02:10<05:05,  4.63s/it] 35%|███▌      | 35/100 [02:13<04:38,  4.29s/it] 36%|███▌      | 36/100 [02:17<04:24,  4.14s/it] 37%|███▋      | 37/100 [02:20<04:08,  3.95s/it] 38%|███▊      | 38/100 [02:24<04:02,  3.91s/it] 39%|███▉      | 39/100 [02:28<04:01,  3.96s/it] 40%|████      | 40/100 [02:33<04:04,  4.07s/it] 41%|████      | 41/100 [02:36<03:54,  3.98s/it] 42%|████▏     | 42/100 [02:40<03:40,  3.80s/it] 43%|████▎     | 43/100 [02:43<03:33,  3.74s/it] 44%|████▍     | 44/100 [02:48<03:41,  3.96s/it] 45%|████▌     | 45/100 [02:52<03:42,  4.04s/it] 46%|████▌     | 46/100 [02:57<03:52,  4.30s/it] 47%|████▋     | 47/100 [03:02<04:07,  4.67s/it] 48%|████▊     | 48/100 [03:07<04:01,  4.64s/it] 49%|████▉     | 49/100 [03:11<03:42,  4.37s/it] 50%|█████     | 50/100 [03:15<03:30,  4.21s/it] 51%|█████     | 51/100 [03:18<03:17,  4.03s/it] 52%|█████▏    | 52/100 [03:22<03:04,  3.84s/it] 53%|█████▎    | 53/100 [03:25<02:59,  3.81s/it] 54%|█████▍    | 54/100 [03:29<02:50,  3.71s/it] 55%|█████▌    | 55/100 [03:32<02:43,  3.63s/it] 56%|█████▌    | 56/100 [03:36<02:40,  3.64s/it] 57%|█████▋    | 57/100 [03:39<02:35,  3.61s/it] 58%|█████▊    | 58/100 [03:43<02:30,  3.58s/it] 59%|█████▉    | 59/100 [03:48<02:40,  3.92s/it] 60%|██████    | 60/100 [03:54<03:00,  4.52s/it] 61%|██████    | 61/100 [03:58<02:52,  4.42s/it] 62%|██████▏   | 62/100 [04:02<02:43,  4.31s/it] 63%|██████▎   | 63/100 [04:05<02:29,  4.05s/it] 64%|██████▍   | 64/100 [04:09<02:21,  3.94s/it] 65%|██████▌   | 65/100 [04:13<02:14,  3.85s/it] 66%|██████▌   | 66/100 [04:16<02:06,  3.73s/it] 67%|██████▋   | 67/100 [04:20<02:01,  3.68s/it] 68%|██████▊   | 68/100 [04:23<01:56,  3.64s/it] 69%|██████▉   | 69/100 [04:27<01:50,  3.58s/it] 70%|███████   | 70/100 [04:30<01:47,  3.58s/it] 71%|███████   | 71/100 [04:34<01:43,  3.57s/it] 72%|███████▏  | 72/100 [04:37<01:41,  3.62s/it] 73%|███████▎  | 73/100 [04:42<01:44,  3.86s/it] 74%|███████▍  | 74/100 [04:45<01:38,  3.78s/it] 75%|███████▌  | 75/100 [04:49<01:31,  3.67s/it] 76%|███████▌  | 76/100 [04:53<01:28,  3.68s/it] 77%|███████▋  | 77/100 [04:56<01:24,  3.67s/it] 78%|███████▊  | 78/100 [05:00<01:21,  3.72s/it] 79%|███████▉  | 79/100 [05:04<01:17,  3.70s/it] 80%|████████  | 80/100 [05:07<01:12,  3.61s/it] 81%|████████  | 81/100 [05:11<01:08,  3.62s/it] 82%|████████▏ | 82/100 [05:14<01:05,  3.64s/it] 83%|████████▎ | 83/100 [05:18<01:01,  3.59s/it] 84%|████████▍ | 84/100 [05:22<00:57,  3.61s/it] 85%|████████▌ | 85/100 [05:26<00:56,  3.73s/it] 86%|████████▌ | 86/100 [05:29<00:52,  3.78s/it] 87%|████████▋ | 87/100 [05:35<00:56,  4.36s/it] 88%|████████▊ | 88/100 [05:41<00:58,  4.91s/it] 89%|████████▉ | 89/100 [05:48<00:59,  5.42s/it] 90%|█████████ | 90/100 [05:54<00:57,  5.74s/it] 91%|█████████ | 91/100 [06:01<00:52,  5.84s/it] 92%|█████████▏| 92/100 [06:05<00:42,  5.31s/it] 93%|█████████▎| 93/100 [06:08<00:33,  4.78s/it] 94%|█████████▍| 94/100 [06:12<00:26,  4.47s/it] 95%|█████████▌| 95/100 [06:16<00:21,  4.21s/it] 96%|█████████▌| 96/100 [06:19<00:15,  3.99s/it] 97%|█████████▋| 97/100 [06:23<00:11,  3.87s/it] 98%|█████████▊| 98/100 [06:26<00:07,  3.81s/it] 99%|█████████▉| 99/100 [06:30<00:03,  3.73s/it]100%|██████████| 100/100 [06:34<00:00,  3.73s/it]100%|██████████| 100/100 [06:34<00:00,  3.94s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer5
=) 37.96% for 5
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:55,  1.79s/it]  3%|▎         | 3/100 [00:07<04:31,  2.80s/it]  4%|▍         | 4/100 [00:11<04:55,  3.08s/it]  5%|▌         | 5/100 [00:14<05:10,  3.27s/it]  6%|▌         | 6/100 [00:18<05:14,  3.34s/it]  7%|▋         | 7/100 [00:21<05:15,  3.39s/it]  8%|▊         | 8/100 [00:25<05:26,  3.55s/it]  9%|▉         | 9/100 [00:29<05:20,  3.52s/it] 10%|█         | 10/100 [00:32<05:20,  3.56s/it] 11%|█         | 11/100 [00:36<05:16,  3.55s/it] 12%|█▏        | 12/100 [00:40<05:22,  3.67s/it] 13%|█▎        | 13/100 [00:44<05:36,  3.86s/it] 14%|█▍        | 14/100 [00:48<05:30,  3.84s/it] 15%|█▌        | 15/100 [00:52<05:22,  3.80s/it] 16%|█▌        | 16/100 [00:55<05:17,  3.78s/it] 17%|█▋        | 17/100 [00:59<05:05,  3.68s/it] 18%|█▊        | 18/100 [01:03<05:18,  3.89s/it] 19%|█▉        | 19/100 [01:07<05:12,  3.86s/it] 20%|██        | 20/100 [01:11<04:59,  3.74s/it] 21%|██        | 21/100 [01:14<04:51,  3.68s/it] 22%|██▏       | 22/100 [01:18<05:01,  3.86s/it] 23%|██▎       | 23/100 [01:23<05:03,  3.95s/it] 24%|██▍       | 24/100 [01:26<04:58,  3.93s/it] 25%|██▌       | 25/100 [01:30<04:48,  3.85s/it] 26%|██▌       | 26/100 [01:34<04:46,  3.87s/it] 27%|██▋       | 27/100 [01:38<04:35,  3.78s/it] 28%|██▊       | 28/100 [01:41<04:28,  3.72s/it] 29%|██▉       | 29/100 [01:45<04:30,  3.82s/it] 30%|███       | 30/100 [01:51<05:13,  4.48s/it] 31%|███       | 31/100 [01:56<05:12,  4.52s/it] 32%|███▏      | 32/100 [02:01<05:14,  4.62s/it] 33%|███▎      | 33/100 [02:06<05:15,  4.71s/it] 34%|███▍      | 34/100 [02:10<05:06,  4.65s/it] 35%|███▌      | 35/100 [02:14<04:41,  4.32s/it] 36%|███▌      | 36/100 [02:17<04:20,  4.07s/it] 37%|███▋      | 37/100 [02:22<04:27,  4.24s/it] 38%|███▊      | 38/100 [02:27<04:48,  4.65s/it] 39%|███▉      | 39/100 [02:32<04:38,  4.57s/it] 40%|████      | 40/100 [02:37<04:50,  4.84s/it] 41%|████      | 41/100 [02:41<04:30,  4.59s/it] 42%|████▏     | 42/100 [02:45<04:04,  4.22s/it] 43%|████▎     | 43/100 [02:48<03:51,  4.06s/it] 44%|████▍     | 44/100 [02:52<03:38,  3.89s/it] 45%|████▌     | 45/100 [02:56<03:36,  3.94s/it] 46%|████▌     | 46/100 [03:01<03:45,  4.18s/it] 47%|████▋     | 47/100 [03:06<04:08,  4.69s/it] 48%|████▊     | 48/100 [03:11<04:05,  4.73s/it] 49%|████▉     | 49/100 [03:17<04:13,  4.97s/it] 50%|█████     | 50/100 [03:22<04:07,  4.95s/it] 51%|█████     | 51/100 [03:26<03:59,  4.88s/it] 52%|█████▏    | 52/100 [03:30<03:30,  4.39s/it] 53%|█████▎    | 53/100 [03:33<03:17,  4.20s/it] 54%|█████▍    | 54/100 [03:37<03:05,  4.03s/it] 55%|█████▌    | 55/100 [03:41<02:53,  3.86s/it] 56%|█████▌    | 56/100 [03:44<02:47,  3.80s/it] 57%|█████▋    | 57/100 [03:48<02:40,  3.72s/it] 58%|█████▊    | 58/100 [03:51<02:35,  3.69s/it] 59%|█████▉    | 59/100 [03:55<02:30,  3.68s/it] 60%|██████    | 60/100 [03:59<02:27,  3.69s/it] 61%|██████    | 61/100 [04:02<02:22,  3.65s/it] 62%|██████▏   | 62/100 [04:06<02:20,  3.69s/it] 63%|██████▎   | 63/100 [04:10<02:16,  3.68s/it] 64%|██████▍   | 64/100 [04:13<02:10,  3.63s/it] 65%|██████▌   | 65/100 [04:17<02:06,  3.62s/it] 66%|██████▌   | 66/100 [04:20<02:01,  3.57s/it] 67%|██████▋   | 67/100 [04:24<01:57,  3.56s/it] 68%|██████▊   | 68/100 [04:27<01:52,  3.52s/it] 69%|██████▉   | 69/100 [04:31<01:50,  3.58s/it] 70%|███████   | 70/100 [04:35<01:49,  3.64s/it] 71%|███████   | 71/100 [04:39<01:46,  3.68s/it] 72%|███████▏  | 72/100 [04:42<01:42,  3.66s/it] 73%|███████▎  | 73/100 [04:46<01:37,  3.62s/it] 74%|███████▍  | 74/100 [04:49<01:34,  3.62s/it] 75%|███████▌  | 75/100 [04:53<01:28,  3.53s/it] 76%|███████▌  | 76/100 [04:56<01:25,  3.56s/it] 77%|███████▋  | 77/100 [05:00<01:22,  3.57s/it] 78%|███████▊  | 78/100 [05:03<01:18,  3.57s/it] 79%|███████▉  | 79/100 [05:07<01:17,  3.68s/it] 80%|████████  | 80/100 [05:12<01:17,  3.85s/it] 81%|████████  | 81/100 [05:16<01:13,  3.89s/it] 82%|████████▏ | 82/100 [05:20<01:12,  4.01s/it] 83%|████████▎ | 83/100 [05:23<01:05,  3.87s/it] 84%|████████▍ | 84/100 [05:27<01:02,  3.90s/it] 85%|████████▌ | 85/100 [05:31<00:58,  3.93s/it] 86%|████████▌ | 86/100 [05:35<00:52,  3.76s/it] 87%|████████▋ | 87/100 [05:39<00:50,  3.86s/it] 88%|████████▊ | 88/100 [05:43<00:46,  3.91s/it] 89%|████████▉ | 89/100 [05:47<00:42,  3.89s/it] 90%|█████████ | 90/100 [05:51<00:41,  4.15s/it] 91%|█████████ | 91/100 [05:57<00:40,  4.47s/it] 92%|█████████▏| 92/100 [06:02<00:37,  4.68s/it] 93%|█████████▎| 93/100 [06:06<00:31,  4.45s/it] 94%|█████████▍| 94/100 [06:10<00:25,  4.30s/it] 95%|█████████▌| 95/100 [06:14<00:21,  4.27s/it] 96%|█████████▌| 96/100 [06:18<00:16,  4.19s/it] 97%|█████████▋| 97/100 [06:22<00:12,  4.04s/it] 98%|█████████▊| 98/100 [06:25<00:07,  3.86s/it] 99%|█████████▉| 99/100 [06:29<00:03,  3.78s/it]100%|██████████| 100/100 [06:32<00:00,  3.78s/it]100%|██████████| 100/100 [06:33<00:00,  3.93s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer6
=) 43.35% for 6
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:55,  1.79s/it]  3%|▎         | 3/100 [00:07<04:18,  2.67s/it]  4%|▍         | 4/100 [00:11<04:47,  3.00s/it]  5%|▌         | 5/100 [00:14<05:10,  3.27s/it]  6%|▌         | 6/100 [00:19<05:39,  3.61s/it]  7%|▋         | 7/100 [00:22<05:43,  3.69s/it]  8%|▊         | 8/100 [00:28<06:30,  4.24s/it]  9%|▉         | 9/100 [00:32<06:09,  4.06s/it] 10%|█         | 10/100 [00:35<05:48,  3.88s/it] 11%|█         | 11/100 [00:39<05:36,  3.78s/it] 12%|█▏        | 12/100 [00:42<05:25,  3.70s/it] 13%|█▎        | 13/100 [00:46<05:25,  3.74s/it] 14%|█▍        | 14/100 [00:50<05:37,  3.93s/it] 15%|█▌        | 15/100 [00:54<05:24,  3.82s/it] 16%|█▌        | 16/100 [00:57<05:13,  3.73s/it] 17%|█▋        | 17/100 [01:01<05:18,  3.84s/it] 18%|█▊        | 18/100 [01:06<05:30,  4.03s/it] 19%|█▉        | 19/100 [01:09<05:14,  3.88s/it] 20%|██        | 20/100 [01:13<05:03,  3.80s/it] 21%|██        | 21/100 [01:17<05:06,  3.88s/it] 22%|██▏       | 22/100 [01:21<05:02,  3.88s/it] 23%|██▎       | 23/100 [01:25<04:49,  3.76s/it] 24%|██▍       | 24/100 [01:28<04:44,  3.75s/it] 25%|██▌       | 25/100 [01:32<04:37,  3.70s/it] 26%|██▌       | 26/100 [01:35<04:27,  3.62s/it] 27%|██▋       | 27/100 [01:39<04:32,  3.74s/it] 28%|██▊       | 28/100 [01:43<04:33,  3.80s/it] 29%|██▉       | 29/100 [01:47<04:25,  3.74s/it] 30%|███       | 30/100 [01:51<04:28,  3.84s/it] 31%|███       | 31/100 [01:55<04:35,  4.00s/it] 32%|███▏      | 32/100 [01:59<04:32,  4.01s/it] 33%|███▎      | 33/100 [02:04<04:38,  4.15s/it] 34%|███▍      | 34/100 [02:07<04:19,  3.94s/it] 35%|███▌      | 35/100 [02:11<04:06,  3.79s/it] 36%|███▌      | 36/100 [02:14<03:57,  3.72s/it] 37%|███▋      | 37/100 [02:18<04:04,  3.88s/it] 38%|███▊      | 38/100 [02:23<04:05,  3.96s/it] 39%|███▉      | 39/100 [02:29<04:40,  4.60s/it] 40%|████      | 40/100 [02:34<04:40,  4.67s/it] 41%|████      | 41/100 [02:38<04:29,  4.57s/it] 42%|████▏     | 42/100 [02:44<04:44,  4.90s/it] 43%|████▎     | 43/100 [02:48<04:27,  4.70s/it] 44%|████▍     | 44/100 [02:51<04:02,  4.34s/it] 45%|████▌     | 45/100 [02:55<03:43,  4.07s/it] 46%|████▌     | 46/100 [02:58<03:33,  3.96s/it] 47%|████▋     | 47/100 [03:02<03:27,  3.92s/it] 48%|████▊     | 48/100 [03:06<03:18,  3.82s/it] 49%|████▉     | 49/100 [03:09<03:11,  3.75s/it] 50%|█████     | 50/100 [03:13<03:03,  3.66s/it] 51%|█████     | 51/100 [03:16<02:57,  3.63s/it] 52%|█████▏    | 52/100 [03:20<02:51,  3.57s/it] 53%|█████▎    | 53/100 [03:24<02:49,  3.61s/it] 54%|█████▍    | 54/100 [03:27<02:43,  3.55s/it] 55%|█████▌    | 55/100 [03:31<02:40,  3.56s/it] 56%|█████▌    | 56/100 [03:34<02:38,  3.60s/it] 57%|█████▋    | 57/100 [03:38<02:37,  3.66s/it] 58%|█████▊    | 58/100 [03:42<02:33,  3.64s/it] 59%|█████▉    | 59/100 [03:45<02:30,  3.66s/it] 60%|██████    | 60/100 [03:49<02:27,  3.70s/it] 61%|██████    | 61/100 [03:53<02:21,  3.62s/it] 62%|██████▏   | 62/100 [03:56<02:16,  3.60s/it] 63%|██████▎   | 63/100 [04:00<02:12,  3.58s/it] 64%|██████▍   | 64/100 [04:03<02:07,  3.54s/it] 65%|██████▌   | 65/100 [04:07<02:04,  3.55s/it] 66%|██████▌   | 66/100 [04:10<02:00,  3.53s/it] 67%|██████▋   | 67/100 [04:14<01:56,  3.54s/it] 68%|██████▊   | 68/100 [04:17<01:52,  3.51s/it] 69%|██████▉   | 69/100 [04:21<01:49,  3.55s/it] 70%|███████   | 70/100 [04:25<01:49,  3.64s/it] 71%|███████   | 71/100 [04:28<01:45,  3.62s/it] 72%|███████▏  | 72/100 [04:32<01:40,  3.60s/it] 73%|███████▎  | 73/100 [04:35<01:37,  3.61s/it] 74%|███████▍  | 74/100 [04:39<01:35,  3.67s/it] 75%|███████▌  | 75/100 [04:43<01:29,  3.59s/it] 76%|███████▌  | 76/100 [04:46<01:27,  3.66s/it] 77%|███████▋  | 77/100 [04:50<01:25,  3.71s/it] 78%|███████▊  | 78/100 [04:54<01:22,  3.77s/it] 79%|███████▉  | 79/100 [04:58<01:20,  3.82s/it] 80%|████████  | 80/100 [05:02<01:13,  3.69s/it] 81%|████████  | 81/100 [05:05<01:09,  3.68s/it] 82%|████████▏ | 82/100 [05:09<01:07,  3.72s/it] 83%|████████▎ | 83/100 [05:13<01:02,  3.69s/it] 84%|████████▍ | 84/100 [05:16<00:58,  3.65s/it] 85%|████████▌ | 85/100 [05:20<00:55,  3.68s/it] 86%|████████▌ | 86/100 [05:24<00:53,  3.81s/it] 87%|████████▋ | 87/100 [05:28<00:50,  3.87s/it] 88%|████████▊ | 88/100 [05:34<00:52,  4.40s/it] 89%|████████▉ | 89/100 [05:38<00:47,  4.34s/it] 90%|█████████ | 90/100 [05:43<00:45,  4.60s/it] 91%|█████████ | 91/100 [05:48<00:42,  4.75s/it] 92%|█████████▏| 92/100 [05:52<00:35,  4.48s/it] 93%|█████████▎| 93/100 [05:56<00:29,  4.19s/it] 94%|█████████▍| 94/100 [05:59<00:24,  4.04s/it] 95%|█████████▌| 95/100 [06:03<00:19,  3.89s/it] 96%|█████████▌| 96/100 [06:06<00:15,  3.80s/it] 97%|█████████▋| 97/100 [06:10<00:11,  3.69s/it] 98%|█████████▊| 98/100 [06:13<00:07,  3.68s/it] 99%|█████████▉| 99/100 [06:17<00:03,  3.64s/it]100%|██████████| 100/100 [06:21<00:00,  3.71s/it]100%|██████████| 100/100 [06:21<00:00,  3.82s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer7
=) 16.92% for 7
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:52,  1.76s/it]  3%|▎         | 3/100 [00:07<04:11,  2.60s/it]  4%|▍         | 4/100 [00:10<04:46,  2.98s/it]  5%|▌         | 5/100 [00:14<05:13,  3.30s/it]  6%|▌         | 6/100 [00:18<05:34,  3.55s/it]  7%|▋         | 7/100 [00:22<05:40,  3.66s/it]  8%|▊         | 8/100 [00:26<05:39,  3.69s/it]  9%|▉         | 9/100 [00:30<05:30,  3.63s/it] 10%|█         | 10/100 [00:33<05:33,  3.71s/it] 11%|█         | 11/100 [00:37<05:29,  3.71s/it] 12%|█▏        | 12/100 [00:41<05:23,  3.68s/it] 13%|█▎        | 13/100 [00:45<05:47,  4.00s/it] 14%|█▍        | 14/100 [00:50<05:48,  4.05s/it] 15%|█▌        | 15/100 [00:53<05:35,  3.95s/it] 16%|█▌        | 16/100 [00:57<05:19,  3.80s/it] 17%|█▋        | 17/100 [01:01<05:24,  3.91s/it] 18%|█▊        | 18/100 [01:06<05:38,  4.13s/it] 19%|█▉        | 19/100 [01:09<05:13,  3.87s/it] 20%|██        | 20/100 [01:13<05:09,  3.87s/it] 21%|██        | 21/100 [01:17<05:06,  3.88s/it] 22%|██▏       | 22/100 [01:21<05:10,  3.99s/it] 23%|██▎       | 23/100 [01:24<04:53,  3.81s/it] 24%|██▍       | 24/100 [01:28<04:42,  3.72s/it] 25%|██▌       | 25/100 [01:31<04:36,  3.69s/it] 26%|██▌       | 26/100 [01:35<04:26,  3.61s/it] 27%|██▋       | 27/100 [01:39<04:27,  3.67s/it] 28%|██▊       | 28/100 [01:42<04:20,  3.62s/it] 29%|██▉       | 29/100 [01:47<04:33,  3.85s/it] 30%|███       | 30/100 [01:52<04:56,  4.24s/it] 31%|███       | 31/100 [01:57<05:09,  4.48s/it] 32%|███▏      | 32/100 [02:02<05:25,  4.79s/it] 33%|███▎      | 33/100 [02:07<05:15,  4.70s/it] 34%|███▍      | 34/100 [02:10<04:48,  4.37s/it] 35%|███▌      | 35/100 [02:14<04:25,  4.09s/it] 36%|███▌      | 36/100 [02:17<04:09,  3.90s/it] 37%|███▋      | 37/100 [02:21<04:04,  3.88s/it] 38%|███▊      | 38/100 [02:26<04:11,  4.06s/it] 39%|███▉      | 39/100 [02:31<04:40,  4.60s/it] 40%|████      | 40/100 [02:37<04:57,  4.96s/it] 41%|████      | 41/100 [02:42<04:51,  4.95s/it] 42%|████▏     | 42/100 [02:46<04:29,  4.65s/it] 43%|████▎     | 43/100 [02:50<04:06,  4.33s/it] 44%|████▍     | 44/100 [02:53<03:46,  4.04s/it] 45%|████▌     | 45/100 [02:57<03:38,  3.97s/it] 46%|████▌     | 46/100 [03:00<03:29,  3.88s/it] 47%|████▋     | 47/100 [03:05<03:31,  4.00s/it] 48%|████▊     | 48/100 [03:08<03:21,  3.87s/it] 49%|████▉     | 49/100 [03:12<03:14,  3.82s/it] 50%|█████     | 50/100 [03:16<03:14,  3.89s/it] 51%|█████     | 51/100 [03:20<03:10,  3.89s/it] 52%|█████▏    | 52/100 [03:25<03:20,  4.18s/it] 53%|█████▎    | 53/100 [03:29<03:15,  4.15s/it] 54%|█████▍    | 54/100 [03:32<03:03,  3.99s/it] 55%|█████▌    | 55/100 [03:36<02:56,  3.92s/it] 56%|█████▌    | 56/100 [03:40<02:48,  3.83s/it] 57%|█████▋    | 57/100 [03:43<02:40,  3.73s/it] 58%|█████▊    | 58/100 [03:47<02:35,  3.69s/it] 59%|█████▉    | 59/100 [03:51<02:31,  3.69s/it] 60%|██████    | 60/100 [03:55<02:29,  3.73s/it] 61%|██████    | 61/100 [03:58<02:21,  3.64s/it] 62%|██████▏   | 62/100 [04:01<02:16,  3.60s/it] 63%|██████▎   | 63/100 [04:05<02:11,  3.55s/it] 64%|██████▍   | 64/100 [04:09<02:08,  3.57s/it] 65%|██████▌   | 65/100 [04:12<02:05,  3.58s/it] 66%|██████▌   | 66/100 [04:16<02:00,  3.56s/it] 67%|██████▋   | 67/100 [04:19<01:57,  3.56s/it] 68%|██████▊   | 68/100 [04:23<01:55,  3.62s/it] 69%|██████▉   | 69/100 [04:27<01:53,  3.65s/it] 70%|███████   | 70/100 [04:30<01:49,  3.65s/it] 71%|███████   | 71/100 [04:34<01:45,  3.64s/it] 72%|███████▏  | 72/100 [04:38<01:44,  3.74s/it] 73%|███████▎  | 73/100 [04:42<01:42,  3.78s/it] 74%|███████▍  | 74/100 [04:47<01:48,  4.18s/it] 75%|███████▌  | 75/100 [04:51<01:41,  4.06s/it] 76%|███████▌  | 76/100 [04:54<01:33,  3.92s/it] 77%|███████▋  | 77/100 [04:58<01:26,  3.75s/it] 78%|███████▊  | 78/100 [05:01<01:21,  3.73s/it] 79%|███████▉  | 79/100 [05:05<01:18,  3.73s/it] 80%|████████  | 80/100 [05:09<01:14,  3.73s/it] 81%|████████  | 81/100 [05:12<01:10,  3.73s/it] 82%|████████▏ | 82/100 [05:16<01:07,  3.77s/it] 83%|████████▎ | 83/100 [05:20<01:02,  3.70s/it] 84%|████████▍ | 84/100 [05:23<00:58,  3.64s/it] 85%|████████▌ | 85/100 [05:27<00:55,  3.67s/it] 86%|████████▌ | 86/100 [05:31<00:50,  3.60s/it] 87%|████████▋ | 87/100 [05:35<00:48,  3.71s/it] 88%|████████▊ | 88/100 [05:39<00:45,  3.81s/it] 89%|████████▉ | 89/100 [05:42<00:40,  3.70s/it] 90%|█████████ | 90/100 [05:46<00:37,  3.78s/it] 91%|█████████ | 91/100 [05:50<00:33,  3.74s/it] 92%|█████████▏| 92/100 [05:53<00:29,  3.65s/it] 93%|█████████▎| 93/100 [05:57<00:26,  3.74s/it] 94%|█████████▍| 94/100 [06:02<00:24,  4.09s/it] 95%|█████████▌| 95/100 [06:07<00:21,  4.26s/it] 96%|█████████▌| 96/100 [06:12<00:18,  4.74s/it] 97%|█████████▋| 97/100 [06:18<00:15,  5.06s/it] 98%|█████████▊| 98/100 [06:22<00:09,  4.55s/it] 99%|█████████▉| 99/100 [06:25<00:04,  4.24s/it]100%|██████████| 100/100 [06:29<00:00,  4.08s/it]100%|██████████| 100/100 [06:29<00:00,  3.90s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer8
=) 22.42% for 8
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:00,  1.84s/it]  3%|▎         | 3/100 [00:07<04:12,  2.60s/it]  4%|▍         | 4/100 [00:10<04:45,  2.97s/it]  5%|▌         | 5/100 [00:14<05:02,  3.18s/it]  6%|▌         | 6/100 [00:18<05:10,  3.31s/it]  7%|▋         | 7/100 [00:21<05:17,  3.41s/it]  8%|▊         | 8/100 [00:25<05:24,  3.53s/it]  9%|▉         | 9/100 [00:29<05:30,  3.64s/it] 10%|█         | 10/100 [00:33<05:28,  3.65s/it] 11%|█         | 11/100 [00:36<05:22,  3.63s/it] 12%|█▏        | 12/100 [00:41<05:49,  3.97s/it] 13%|█▎        | 13/100 [00:46<06:18,  4.35s/it] 14%|█▍        | 14/100 [00:52<06:55,  4.84s/it] 15%|█▌        | 15/100 [00:57<07:01,  4.96s/it] 16%|█▌        | 16/100 [01:01<06:14,  4.46s/it] 17%|█▋        | 17/100 [01:04<05:48,  4.20s/it] 18%|█▊        | 18/100 [01:08<05:35,  4.09s/it] 19%|█▉        | 19/100 [01:12<05:32,  4.10s/it] 20%|██        | 20/100 [01:17<05:38,  4.23s/it] 21%|██        | 21/100 [01:21<05:38,  4.28s/it] 22%|██▏       | 22/100 [01:26<05:49,  4.48s/it] 23%|██▎       | 23/100 [01:31<05:53,  4.59s/it] 24%|██▍       | 24/100 [01:34<05:25,  4.28s/it] 25%|██▌       | 25/100 [01:38<05:01,  4.02s/it] 26%|██▌       | 26/100 [01:42<04:49,  3.91s/it] 27%|██▋       | 27/100 [01:45<04:43,  3.88s/it] 28%|██▊       | 28/100 [01:49<04:33,  3.80s/it] 29%|██▉       | 29/100 [01:53<04:25,  3.74s/it] 30%|███       | 30/100 [01:57<04:29,  3.85s/it] 31%|███       | 31/100 [02:00<04:18,  3.75s/it] 32%|███▏      | 32/100 [02:04<04:14,  3.74s/it] 33%|███▎      | 33/100 [02:08<04:25,  3.96s/it] 34%|███▍      | 34/100 [02:12<04:13,  3.84s/it] 35%|███▌      | 35/100 [02:15<04:02,  3.73s/it] 36%|███▌      | 36/100 [02:19<03:54,  3.66s/it] 37%|███▋      | 37/100 [02:23<03:54,  3.72s/it] 38%|███▊      | 38/100 [02:27<03:55,  3.80s/it] 39%|███▉      | 39/100 [02:31<03:54,  3.85s/it] 40%|████      | 40/100 [02:34<03:47,  3.79s/it] 41%|████      | 41/100 [02:38<03:41,  3.75s/it] 42%|████▏     | 42/100 [02:43<03:51,  4.00s/it] 43%|████▎     | 43/100 [02:47<04:01,  4.24s/it] 44%|████▍     | 44/100 [02:51<03:51,  4.13s/it] 45%|████▌     | 45/100 [02:55<03:43,  4.06s/it] 46%|████▌     | 46/100 [02:59<03:37,  4.03s/it] 47%|████▋     | 47/100 [03:03<03:34,  4.05s/it] 48%|████▊     | 48/100 [03:07<03:22,  3.89s/it] 49%|████▉     | 49/100 [03:11<03:20,  3.94s/it] 50%|█████     | 50/100 [03:14<03:11,  3.83s/it] 51%|█████     | 51/100 [03:18<03:04,  3.76s/it] 52%|█████▏    | 52/100 [03:22<03:01,  3.78s/it] 53%|█████▎    | 53/100 [03:25<02:55,  3.74s/it] 54%|█████▍    | 54/100 [03:29<02:49,  3.68s/it] 55%|█████▌    | 55/100 [03:33<02:45,  3.67s/it] 56%|█████▌    | 56/100 [03:37<02:55,  4.00s/it] 57%|█████▋    | 57/100 [03:43<03:13,  4.51s/it] 58%|█████▊    | 58/100 [03:48<03:20,  4.78s/it] 59%|█████▉    | 59/100 [03:54<03:22,  4.95s/it] 60%|██████    | 60/100 [03:59<03:25,  5.14s/it] 61%|██████    | 61/100 [04:04<03:08,  4.83s/it] 62%|██████▏   | 62/100 [04:07<02:51,  4.51s/it] 63%|██████▎   | 63/100 [04:11<02:41,  4.36s/it] 64%|██████▍   | 64/100 [04:17<02:50,  4.75s/it] 65%|██████▌   | 65/100 [04:22<02:44,  4.70s/it] 66%|██████▌   | 66/100 [04:25<02:30,  4.42s/it] 67%|██████▋   | 67/100 [04:29<02:17,  4.15s/it] 68%|██████▊   | 68/100 [04:32<02:07,  3.97s/it] 69%|██████▉   | 69/100 [04:36<01:59,  3.85s/it] 70%|███████   | 70/100 [04:40<01:55,  3.86s/it] 71%|███████   | 71/100 [04:44<01:51,  3.85s/it] 72%|███████▏  | 72/100 [04:48<01:55,  4.14s/it] 73%|███████▎  | 73/100 [04:53<01:56,  4.31s/it] 74%|███████▍  | 74/100 [05:00<02:09,  4.99s/it] 75%|███████▌  | 75/100 [05:06<02:12,  5.30s/it] 76%|███████▌  | 76/100 [05:11<02:06,  5.25s/it] 77%|███████▋  | 77/100 [05:14<01:47,  4.67s/it] 78%|███████▊  | 78/100 [05:18<01:36,  4.40s/it] 79%|███████▉  | 79/100 [05:22<01:27,  4.16s/it] 80%|████████  | 80/100 [05:25<01:18,  3.94s/it] 81%|████████  | 81/100 [05:29<01:12,  3.83s/it] 82%|████████▏ | 82/100 [05:32<01:08,  3.79s/it] 83%|████████▎ | 83/100 [05:36<01:02,  3.69s/it] 84%|████████▍ | 84/100 [05:39<00:57,  3.62s/it] 85%|████████▌ | 85/100 [05:44<00:57,  3.82s/it] 86%|████████▌ | 86/100 [05:48<00:56,  4.07s/it] 87%|████████▋ | 87/100 [05:53<00:54,  4.19s/it] 88%|████████▊ | 88/100 [05:56<00:47,  3.97s/it] 89%|████████▉ | 89/100 [06:00<00:41,  3.81s/it] 90%|█████████ | 90/100 [06:03<00:38,  3.83s/it] 91%|█████████ | 91/100 [06:07<00:34,  3.79s/it] 92%|█████████▏| 92/100 [06:11<00:30,  3.76s/it] 93%|█████████▎| 93/100 [06:14<00:26,  3.73s/it] 94%|█████████▍| 94/100 [06:18<00:22,  3.78s/it] 95%|█████████▌| 95/100 [06:22<00:18,  3.70s/it] 96%|█████████▌| 96/100 [06:25<00:14,  3.61s/it] 97%|█████████▋| 97/100 [06:29<00:10,  3.56s/it] 98%|█████████▊| 98/100 [06:32<00:07,  3.55s/it] 99%|█████████▉| 99/100 [06:36<00:03,  3.54s/it]100%|██████████| 100/100 [06:40<00:00,  3.66s/it]100%|██████████| 100/100 [06:40<00:00,  4.00s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer9
=( -1.82% for 9
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:55,  1.79s/it]  3%|▎         | 3/100 [00:07<04:14,  2.63s/it]  4%|▍         | 4/100 [00:11<04:58,  3.11s/it]  5%|▌         | 5/100 [00:15<05:22,  3.39s/it]  6%|▌         | 6/100 [00:19<05:56,  3.79s/it]  7%|▋         | 7/100 [00:23<05:41,  3.67s/it]  8%|▊         | 8/100 [00:27<05:41,  3.71s/it]  9%|▉         | 9/100 [00:30<05:32,  3.65s/it] 10%|█         | 10/100 [00:34<05:34,  3.72s/it] 11%|█         | 11/100 [00:39<06:19,  4.27s/it] 12%|█▏        | 12/100 [00:45<06:49,  4.65s/it] 13%|█▎        | 13/100 [00:49<06:20,  4.37s/it] 14%|█▍        | 14/100 [00:52<05:56,  4.15s/it] 15%|█▌        | 15/100 [00:57<05:54,  4.17s/it] 16%|█▌        | 16/100 [01:00<05:32,  3.96s/it] 17%|█▋        | 17/100 [01:04<05:28,  3.95s/it] 18%|█▊        | 18/100 [01:08<05:27,  4.00s/it] 19%|█▉        | 19/100 [01:12<05:23,  3.99s/it] 20%|██        | 20/100 [01:16<05:14,  3.93s/it] 21%|██        | 21/100 [01:19<05:03,  3.84s/it] 22%|██▏       | 22/100 [01:23<04:53,  3.77s/it] 23%|██▎       | 23/100 [01:26<04:40,  3.65s/it] 24%|██▍       | 24/100 [01:30<04:34,  3.61s/it] 25%|██▌       | 25/100 [01:34<04:32,  3.63s/it] 26%|██▌       | 26/100 [01:37<04:33,  3.70s/it] 27%|██▋       | 27/100 [01:41<04:27,  3.66s/it] 28%|██▊       | 28/100 [01:45<04:22,  3.65s/it] 29%|██▉       | 29/100 [01:48<04:19,  3.65s/it] 30%|███       | 30/100 [01:52<04:26,  3.80s/it] 31%|███       | 31/100 [01:56<04:19,  3.77s/it] 32%|███▏      | 32/100 [02:00<04:15,  3.75s/it] 33%|███▎      | 33/100 [02:04<04:16,  3.83s/it] 34%|███▍      | 34/100 [02:07<04:04,  3.71s/it] 35%|███▌      | 35/100 [02:11<03:59,  3.68s/it] 36%|███▌      | 36/100 [02:15<04:07,  3.87s/it] 37%|███▋      | 37/100 [02:20<04:12,  4.00s/it] 38%|███▊      | 38/100 [02:24<04:19,  4.19s/it] 39%|███▉      | 39/100 [02:30<04:39,  4.58s/it] 40%|████      | 40/100 [02:35<04:45,  4.75s/it] 41%|████      | 41/100 [02:39<04:31,  4.60s/it] 42%|████▏     | 42/100 [02:43<04:14,  4.39s/it] 43%|████▎     | 43/100 [02:47<03:56,  4.16s/it] 44%|████▍     | 44/100 [02:51<03:55,  4.21s/it] 45%|████▌     | 45/100 [02:56<03:58,  4.34s/it] 46%|████▌     | 46/100 [03:00<03:54,  4.34s/it] 47%|████▋     | 47/100 [03:05<03:55,  4.44s/it] 48%|████▊     | 48/100 [03:09<03:45,  4.33s/it] 49%|████▉     | 49/100 [03:12<03:25,  4.03s/it] 50%|█████     | 50/100 [03:15<03:12,  3.86s/it] 51%|█████     | 51/100 [03:19<03:03,  3.75s/it] 52%|█████▏    | 52/100 [03:23<03:02,  3.81s/it] 53%|█████▎    | 53/100 [03:27<03:06,  3.96s/it] 54%|█████▍    | 54/100 [03:32<03:09,  4.12s/it] 55%|█████▌    | 55/100 [03:37<03:19,  4.43s/it] 56%|█████▌    | 56/100 [03:43<03:32,  4.84s/it] 57%|█████▋    | 57/100 [03:47<03:26,  4.81s/it] 58%|█████▊    | 58/100 [03:51<03:07,  4.46s/it] 59%|█████▉    | 59/100 [03:56<03:08,  4.59s/it] 60%|██████    | 60/100 [04:01<03:08,  4.71s/it] 61%|██████    | 61/100 [04:04<02:48,  4.33s/it] 62%|██████▏   | 62/100 [04:08<02:35,  4.08s/it] 63%|██████▎   | 63/100 [04:11<02:23,  3.88s/it] 64%|██████▍   | 64/100 [04:15<02:16,  3.80s/it] 65%|██████▌   | 65/100 [04:19<02:11,  3.76s/it] 66%|██████▌   | 66/100 [04:22<02:06,  3.73s/it] 67%|██████▋   | 67/100 [04:26<02:03,  3.73s/it] 68%|██████▊   | 68/100 [04:30<01:58,  3.70s/it] 69%|██████▉   | 69/100 [04:33<01:53,  3.67s/it] 70%|███████   | 70/100 [04:38<01:57,  3.92s/it] 71%|███████   | 71/100 [04:43<02:04,  4.29s/it] 72%|███████▏  | 72/100 [04:48<02:04,  4.44s/it] 73%|███████▎  | 73/100 [04:52<01:56,  4.33s/it] 74%|███████▍  | 74/100 [04:55<01:47,  4.12s/it] 75%|███████▌  | 75/100 [04:59<01:37,  3.89s/it] 76%|███████▌  | 76/100 [05:02<01:30,  3.77s/it] 77%|███████▋  | 77/100 [05:06<01:25,  3.73s/it] 78%|███████▊  | 78/100 [05:10<01:22,  3.74s/it] 79%|███████▉  | 79/100 [05:13<01:17,  3.70s/it] 80%|████████  | 80/100 [05:17<01:13,  3.69s/it] 81%|████████  | 81/100 [05:20<01:09,  3.67s/it] 82%|████████▏ | 82/100 [05:24<01:06,  3.67s/it] 83%|████████▎ | 83/100 [05:28<01:01,  3.62s/it] 84%|████████▍ | 84/100 [05:31<00:58,  3.64s/it] 85%|████████▌ | 85/100 [05:35<00:56,  3.73s/it] 86%|████████▌ | 86/100 [05:39<00:51,  3.65s/it] 87%|████████▋ | 87/100 [05:42<00:47,  3.65s/it] 88%|████████▊ | 88/100 [05:46<00:43,  3.61s/it] 89%|████████▉ | 89/100 [05:49<00:39,  3.56s/it] 90%|█████████ | 90/100 [05:53<00:35,  3.59s/it] 91%|█████████ | 91/100 [05:57<00:32,  3.63s/it] 92%|█████████▏| 92/100 [06:00<00:28,  3.58s/it] 93%|█████████▎| 93/100 [06:04<00:24,  3.54s/it] 94%|█████████▍| 94/100 [06:07<00:21,  3.57s/it] 95%|█████████▌| 95/100 [06:11<00:18,  3.66s/it] 96%|█████████▌| 96/100 [06:15<00:14,  3.59s/it] 97%|█████████▋| 97/100 [06:18<00:10,  3.54s/it] 98%|█████████▊| 98/100 [06:22<00:07,  3.55s/it] 99%|█████████▉| 99/100 [06:25<00:03,  3.61s/it]100%|██████████| 100/100 [06:29<00:00,  3.71s/it]100%|██████████| 100/100 [06:30<00:00,  3.90s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer10
=) 18.78% for 10
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:04<03:19,  2.04s/it]  3%|▎         | 3/100 [00:09<05:15,  3.25s/it]  4%|▍         | 4/100 [00:12<05:20,  3.33s/it]  5%|▌         | 5/100 [00:16<05:49,  3.68s/it]  6%|▌         | 6/100 [00:20<05:59,  3.83s/it]  7%|▋         | 7/100 [00:24<05:46,  3.73s/it]  8%|▊         | 8/100 [00:28<05:41,  3.71s/it]  9%|▉         | 9/100 [00:31<05:31,  3.65s/it] 10%|█         | 10/100 [00:35<05:31,  3.68s/it] 11%|█         | 11/100 [00:39<05:31,  3.73s/it] 12%|█▏        | 12/100 [00:42<05:27,  3.72s/it] 13%|█▎        | 13/100 [00:46<05:28,  3.77s/it] 14%|█▍        | 14/100 [00:50<05:21,  3.74s/it] 15%|█▌        | 15/100 [00:54<05:28,  3.86s/it] 16%|█▌        | 16/100 [00:58<05:23,  3.85s/it] 17%|█▋        | 17/100 [01:02<05:22,  3.88s/it] 18%|█▊        | 18/100 [01:05<05:09,  3.78s/it] 19%|█▉        | 19/100 [01:09<04:56,  3.66s/it] 20%|██        | 20/100 [01:12<04:50,  3.63s/it] 21%|██        | 21/100 [01:16<04:41,  3.56s/it] 22%|██▏       | 22/100 [01:19<04:38,  3.57s/it] 23%|██▎       | 23/100 [01:23<04:33,  3.55s/it] 24%|██▍       | 24/100 [01:27<04:35,  3.62s/it] 25%|██▌       | 25/100 [01:30<04:31,  3.62s/it] 26%|██▌       | 26/100 [01:34<04:23,  3.56s/it] 27%|██▋       | 27/100 [01:38<04:25,  3.63s/it] 28%|██▊       | 28/100 [01:41<04:23,  3.66s/it] 29%|██▉       | 29/100 [01:45<04:19,  3.66s/it] 30%|███       | 30/100 [01:49<04:27,  3.82s/it] 31%|███       | 31/100 [01:53<04:30,  3.92s/it] 32%|███▏      | 32/100 [01:58<04:37,  4.08s/it] 33%|███▎      | 33/100 [02:01<04:26,  3.98s/it] 34%|███▍      | 34/100 [02:05<04:15,  3.87s/it] 35%|███▌      | 35/100 [02:09<04:19,  3.99s/it] 36%|███▌      | 36/100 [02:14<04:22,  4.10s/it] 37%|███▋      | 37/100 [02:17<04:11,  3.99s/it] 38%|███▊      | 38/100 [02:22<04:23,  4.26s/it] 39%|███▉      | 39/100 [02:28<04:49,  4.74s/it] 40%|████      | 40/100 [02:34<05:11,  5.18s/it] 41%|████      | 41/100 [02:40<05:19,  5.41s/it] 42%|████▏     | 42/100 [02:45<04:55,  5.10s/it] 43%|████▎     | 43/100 [02:48<04:25,  4.65s/it] 44%|████▍     | 44/100 [02:52<04:01,  4.31s/it] 45%|████▌     | 45/100 [02:55<03:45,  4.09s/it] 46%|████▌     | 46/100 [02:59<03:40,  4.08s/it] 47%|████▋     | 47/100 [03:03<03:32,  4.01s/it] 48%|████▊     | 48/100 [03:07<03:20,  3.86s/it] 49%|████▉     | 49/100 [03:10<03:13,  3.80s/it] 50%|█████     | 50/100 [03:14<03:06,  3.72s/it] 51%|█████     | 51/100 [03:17<02:58,  3.65s/it] 52%|█████▏    | 52/100 [03:21<02:51,  3.58s/it] 53%|█████▎    | 53/100 [03:25<02:50,  3.62s/it] 54%|█████▍    | 54/100 [03:28<02:43,  3.56s/it] 55%|█████▌    | 55/100 [03:32<02:39,  3.54s/it] 56%|█████▌    | 56/100 [03:35<02:39,  3.64s/it] 57%|█████▋    | 57/100 [03:39<02:34,  3.60s/it] 58%|█████▊    | 58/100 [03:43<02:32,  3.62s/it] 59%|█████▉    | 59/100 [03:46<02:29,  3.65s/it] 60%|██████    | 60/100 [03:50<02:26,  3.67s/it] 61%|██████    | 61/100 [03:53<02:19,  3.59s/it] 62%|██████▏   | 62/100 [03:57<02:15,  3.57s/it] 63%|██████▎   | 63/100 [04:00<02:11,  3.56s/it] 64%|██████▍   | 64/100 [04:04<02:10,  3.62s/it] 65%|██████▌   | 65/100 [04:08<02:08,  3.66s/it] 66%|██████▌   | 66/100 [04:12<02:05,  3.70s/it] 67%|██████▋   | 67/100 [04:16<02:03,  3.75s/it] 68%|██████▊   | 68/100 [04:19<01:59,  3.74s/it] 69%|██████▉   | 69/100 [04:23<01:56,  3.75s/it] 70%|███████   | 70/100 [04:27<01:54,  3.83s/it] 71%|███████   | 71/100 [04:31<01:51,  3.83s/it] 72%|███████▏  | 72/100 [04:35<01:52,  4.03s/it] 73%|███████▎  | 73/100 [04:39<01:48,  4.02s/it] 74%|███████▍  | 74/100 [04:43<01:44,  4.00s/it] 75%|███████▌  | 75/100 [04:47<01:36,  3.88s/it] 76%|███████▌  | 76/100 [04:51<01:31,  3.79s/it] 77%|███████▋  | 77/100 [04:54<01:24,  3.69s/it] 78%|███████▊  | 78/100 [04:58<01:21,  3.71s/it] 79%|███████▉  | 79/100 [05:01<01:17,  3.69s/it] 80%|████████  | 80/100 [05:05<01:13,  3.67s/it] 81%|████████  | 81/100 [05:09<01:10,  3.70s/it] 82%|████████▏ | 82/100 [05:13<01:06,  3.71s/it] 83%|████████▎ | 83/100 [05:17<01:04,  3.79s/it] 84%|████████▍ | 84/100 [05:20<00:58,  3.66s/it] 85%|████████▌ | 85/100 [05:24<00:55,  3.67s/it] 86%|████████▌ | 86/100 [05:28<00:53,  3.84s/it] 87%|████████▋ | 87/100 [05:32<00:51,  3.95s/it] 88%|████████▊ | 88/100 [05:36<00:48,  4.08s/it] 89%|████████▉ | 89/100 [05:40<00:42,  3.88s/it] 90%|█████████ | 90/100 [05:44<00:38,  3.88s/it] 91%|█████████ | 91/100 [05:47<00:34,  3.81s/it] 92%|█████████▏| 92/100 [05:51<00:29,  3.71s/it] 93%|█████████▎| 93/100 [05:54<00:25,  3.64s/it] 94%|█████████▍| 94/100 [05:58<00:21,  3.64s/it] 95%|█████████▌| 95/100 [06:02<00:18,  3.60s/it] 96%|█████████▌| 96/100 [06:05<00:14,  3.60s/it] 97%|█████████▋| 97/100 [06:09<00:10,  3.64s/it] 98%|█████████▊| 98/100 [06:12<00:07,  3.65s/it] 99%|█████████▉| 99/100 [06:17<00:03,  3.82s/it]100%|██████████| 100/100 [06:22<00:00,  4.18s/it]100%|██████████| 100/100 [06:22<00:00,  3.82s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer11
=) 21.00% for 11
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:01,  1.85s/it]  3%|▎         | 3/100 [00:07<04:22,  2.71s/it]  4%|▍         | 4/100 [00:12<05:29,  3.43s/it]  5%|▌         | 5/100 [00:16<05:49,  3.68s/it]  6%|▌         | 6/100 [00:20<05:53,  3.76s/it]  7%|▋         | 7/100 [00:23<05:43,  3.69s/it]  8%|▊         | 8/100 [00:27<05:43,  3.73s/it]  9%|▉         | 9/100 [00:31<05:40,  3.75s/it] 10%|█         | 10/100 [00:35<05:42,  3.81s/it] 11%|█         | 11/100 [00:38<05:30,  3.71s/it] 12%|█▏        | 12/100 [00:42<05:26,  3.71s/it] 13%|█▎        | 13/100 [00:47<05:44,  3.96s/it] 14%|█▍        | 14/100 [00:51<05:41,  3.97s/it] 15%|█▌        | 15/100 [00:54<05:34,  3.93s/it] 16%|█▌        | 16/100 [00:58<05:17,  3.78s/it] 17%|█▋        | 17/100 [01:01<05:06,  3.70s/it] 18%|█▊        | 18/100 [01:05<05:05,  3.72s/it] 19%|█▉        | 19/100 [01:09<04:58,  3.69s/it] 20%|██        | 20/100 [01:13<04:55,  3.70s/it] 21%|██        | 21/100 [01:16<04:45,  3.61s/it] 22%|██▏       | 22/100 [01:20<04:40,  3.60s/it] 23%|██▎       | 23/100 [01:23<04:32,  3.54s/it] 24%|██▍       | 24/100 [01:26<04:29,  3.54s/it] 25%|██▌       | 25/100 [01:30<04:31,  3.61s/it] 26%|██▌       | 26/100 [01:34<04:24,  3.57s/it] 27%|██▋       | 27/100 [01:38<04:28,  3.68s/it] 28%|██▊       | 28/100 [01:41<04:28,  3.73s/it] 29%|██▉       | 29/100 [01:45<04:24,  3.72s/it] 30%|███       | 30/100 [01:50<04:45,  4.08s/it] 31%|███       | 31/100 [01:54<04:40,  4.07s/it] 32%|███▏      | 32/100 [01:58<04:25,  3.91s/it] 33%|███▎      | 33/100 [02:02<04:20,  3.90s/it] 34%|███▍      | 34/100 [02:05<04:15,  3.86s/it] 35%|███▌      | 35/100 [02:09<04:11,  3.87s/it] 36%|███▌      | 36/100 [02:14<04:32,  4.25s/it] 37%|███▋      | 37/100 [02:20<04:49,  4.60s/it] 38%|███▊      | 38/100 [02:26<05:21,  5.19s/it] 39%|███▉      | 39/100 [02:31<05:03,  4.97s/it] 40%|████      | 40/100 [02:36<04:56,  4.94s/it] 41%|████      | 41/100 [02:40<04:33,  4.63s/it] 42%|████▏     | 42/100 [02:43<04:08,  4.29s/it] 43%|████▎     | 43/100 [02:47<03:53,  4.10s/it] 44%|████▍     | 44/100 [02:51<03:53,  4.17s/it] 45%|████▌     | 45/100 [02:55<03:42,  4.04s/it] 46%|████▌     | 46/100 [02:59<03:33,  3.95s/it] 47%|████▋     | 47/100 [03:03<03:35,  4.07s/it] 48%|████▊     | 48/100 [03:06<03:21,  3.88s/it] 49%|████▉     | 49/100 [03:10<03:11,  3.76s/it] 50%|█████     | 50/100 [03:14<03:07,  3.76s/it] 51%|█████     | 51/100 [03:17<03:03,  3.74s/it] 52%|█████▏    | 52/100 [03:21<03:04,  3.85s/it] 53%|█████▎    | 53/100 [03:26<03:14,  4.14s/it] 54%|█████▍    | 54/100 [03:31<03:15,  4.25s/it] 55%|█████▌    | 55/100 [03:34<03:00,  4.01s/it] 56%|█████▌    | 56/100 [03:38<02:50,  3.88s/it] 57%|█████▋    | 57/100 [03:41<02:42,  3.78s/it] 58%|█████▊    | 58/100 [03:45<02:35,  3.70s/it] 59%|█████▉    | 59/100 [03:48<02:31,  3.70s/it] 60%|██████    | 60/100 [03:52<02:29,  3.74s/it] 61%|██████    | 61/100 [03:56<02:23,  3.67s/it] 62%|██████▏   | 62/100 [04:00<02:21,  3.73s/it] 63%|██████▎   | 63/100 [04:03<02:17,  3.71s/it] 64%|██████▍   | 64/100 [04:07<02:11,  3.64s/it] 65%|██████▌   | 65/100 [04:11<02:09,  3.69s/it] 66%|██████▌   | 66/100 [04:15<02:09,  3.82s/it] 67%|██████▋   | 67/100 [04:20<02:21,  4.28s/it] 68%|██████▊   | 68/100 [04:25<02:23,  4.48s/it] 69%|██████▉   | 69/100 [04:30<02:27,  4.76s/it] 70%|███████   | 70/100 [04:36<02:27,  4.93s/it] 71%|███████   | 71/100 [04:39<02:09,  4.47s/it] 72%|███████▏  | 72/100 [04:43<01:58,  4.23s/it] 73%|███████▎  | 73/100 [04:47<01:51,  4.12s/it] 74%|███████▍  | 74/100 [04:50<01:44,  4.01s/it] 75%|███████▌  | 75/100 [04:54<01:37,  3.91s/it] 76%|███████▌  | 76/100 [04:59<01:40,  4.20s/it] 77%|███████▋  | 77/100 [05:03<01:33,  4.07s/it] 78%|███████▊  | 78/100 [05:07<01:29,  4.05s/it] 79%|███████▉  | 79/100 [05:11<01:26,  4.12s/it] 80%|████████  | 80/100 [05:15<01:19,  3.97s/it] 81%|████████  | 81/100 [05:19<01:17,  4.06s/it] 82%|████████▏ | 82/100 [05:23<01:11,  3.95s/it] 83%|████████▎ | 83/100 [05:26<01:05,  3.83s/it] 84%|████████▍ | 84/100 [05:30<01:02,  3.91s/it] 85%|████████▌ | 85/100 [05:34<00:59,  3.97s/it] 86%|████████▌ | 86/100 [05:38<00:55,  3.99s/it] 87%|████████▋ | 87/100 [05:42<00:50,  3.87s/it] 88%|████████▊ | 88/100 [05:46<00:45,  3.81s/it] 89%|████████▉ | 89/100 [05:49<00:41,  3.78s/it] 90%|█████████ | 90/100 [05:53<00:37,  3.72s/it] 91%|█████████ | 91/100 [05:56<00:32,  3.66s/it] 92%|█████████▏| 92/100 [06:00<00:28,  3.61s/it] 93%|█████████▎| 93/100 [06:04<00:25,  3.64s/it] 94%|█████████▍| 94/100 [06:08<00:22,  3.73s/it] 95%|█████████▌| 95/100 [06:11<00:18,  3.70s/it] 96%|█████████▌| 96/100 [06:15<00:15,  3.82s/it] 97%|█████████▋| 97/100 [06:19<00:11,  3.81s/it] 98%|█████████▊| 98/100 [06:23<00:07,  3.73s/it] 99%|█████████▉| 99/100 [06:26<00:03,  3.66s/it]100%|██████████| 100/100 [06:30<00:00,  3.72s/it]100%|██████████| 100/100 [06:30<00:00,  3.91s/it]
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:55,  1.79s/it]  3%|▎         | 3/100 [00:08<04:48,  2.98s/it]  4%|▍         | 4/100 [00:12<05:38,  3.53s/it]  5%|▌         | 5/100 [00:16<05:32,  3.50s/it]  6%|▌         | 6/100 [00:20<05:48,  3.71s/it]  7%|▋         | 7/100 [00:25<06:16,  4.05s/it]  8%|▊         | 8/100 [00:30<06:50,  4.46s/it]  9%|▉         | 9/100 [00:34<06:31,  4.31s/it] 10%|█         | 10/100 [00:37<06:09,  4.11s/it] 11%|█         | 11/100 [00:41<05:48,  3.92s/it] 12%|█▏        | 12/100 [00:44<05:32,  3.78s/it] 13%|█▎        | 13/100 [00:48<05:30,  3.80s/it] 14%|█▍        | 14/100 [00:52<05:25,  3.79s/it] 15%|█▌        | 15/100 [00:56<05:19,  3.75s/it] 16%|█▌        | 16/100 [00:59<05:09,  3.68s/it] 17%|█▋        | 17/100 [01:03<05:00,  3.62s/it] 18%|█▊        | 18/100 [01:06<04:56,  3.62s/it] 19%|█▉        | 19/100 [01:10<04:49,  3.57s/it] 20%|██        | 20/100 [01:13<04:45,  3.56s/it] 21%|██        | 21/100 [01:17<04:42,  3.57s/it] 22%|██▏       | 22/100 [01:21<04:40,  3.60s/it] 23%|██▎       | 23/100 [01:24<04:34,  3.56s/it] 24%|██▍       | 24/100 [01:28<04:33,  3.60s/it] 25%|██▌       | 25/100 [01:32<04:33,  3.64s/it] 26%|██▌       | 26/100 [01:35<04:31,  3.67s/it] 27%|██▋       | 27/100 [01:39<04:26,  3.65s/it] 28%|██▊       | 28/100 [01:43<04:28,  3.73s/it] 29%|██▉       | 29/100 [01:47<04:43,  4.00s/it] 30%|███       | 30/100 [01:52<04:50,  4.15s/it] 31%|███       | 31/100 [01:55<04:35,  3.99s/it] 32%|███▏      | 32/100 [01:59<04:28,  3.95s/it] 33%|███▎      | 33/100 [02:04<04:39,  4.17s/it] 34%|███▍      | 34/100 [02:09<04:58,  4.52s/it] 35%|███▌      | 35/100 [02:13<04:35,  4.23s/it] 36%|███▌      | 36/100 [02:16<04:15,  3.99s/it] 37%|███▋      | 37/100 [02:20<03:59,  3.81s/it] 38%|███▊      | 38/100 [02:24<03:56,  3.82s/it] 39%|███▉      | 39/100 [02:28<04:00,  3.94s/it] 40%|████      | 40/100 [02:33<04:24,  4.41s/it] 41%|████      | 41/100 [02:38<04:33,  4.63s/it] 42%|████▏     | 42/100 [02:44<04:36,  4.77s/it] 43%|████▎     | 43/100 [02:48<04:32,  4.78s/it] 44%|████▍     | 44/100 [02:52<04:04,  4.36s/it] 45%|████▌     | 45/100 [02:55<03:44,  4.08s/it] 46%|████▌     | 46/100 [02:59<03:34,  3.97s/it] 47%|████▋     | 47/100 [03:04<03:46,  4.28s/it] 48%|████▊     | 48/100 [03:09<03:50,  4.44s/it] 49%|████▉     | 49/100 [03:14<04:01,  4.74s/it] 50%|█████     | 50/100 [03:20<04:06,  4.93s/it] 51%|█████     | 51/100 [03:25<04:02,  4.94s/it] 52%|█████▏    | 52/100 [03:29<03:54,  4.88s/it] 53%|█████▎    | 53/100 [03:34<03:52,  4.94s/it] 54%|█████▍    | 54/100 [03:38<03:33,  4.65s/it] 55%|█████▌    | 55/100 [03:43<03:24,  4.54s/it] 56%|█████▌    | 56/100 [03:47<03:15,  4.43s/it] 57%|█████▋    | 57/100 [03:51<03:13,  4.50s/it] 58%|█████▊    | 58/100 [03:57<03:21,  4.81s/it] 59%|█████▉    | 59/100 [04:03<03:36,  5.27s/it] 60%|██████    | 60/100 [04:10<03:46,  5.66s/it] 61%|██████    | 61/100 [04:15<03:37,  5.57s/it] 62%|██████▏   | 62/100 [04:19<03:13,  5.10s/it] 63%|██████▎   | 63/100 [04:23<02:54,  4.73s/it] 64%|██████▍   | 64/100 [04:28<02:49,  4.70s/it] 65%|██████▌   | 65/100 [04:31<02:32,  4.35s/it] 66%|██████▌   | 66/100 [04:35<02:22,  4.18s/it] 67%|██████▋   | 67/100 [04:38<02:09,  3.92s/it] 68%|██████▊   | 68/100 [04:42<01:58,  3.71s/it] 69%|██████▉   | 69/100 [04:45<01:49,  3.54s/it] 70%|███████   | 70/100 [04:48<01:44,  3.48s/it] 71%|███████   | 71/100 [04:51<01:39,  3.42s/it] 72%|███████▏  | 72/100 [04:55<01:34,  3.39s/it] 73%|███████▎  | 73/100 [04:58<01:31,  3.37s/it] 74%|███████▍  | 74/100 [05:01<01:27,  3.36s/it] 75%|███████▌  | 75/100 [05:04<01:22,  3.29s/it] 76%|███████▌  | 76/100 [05:08<01:18,  3.27s/it] 77%|███████▋  | 77/100 [05:11<01:14,  3.26s/it] 78%|███████▊  | 78/100 [05:14<01:12,  3.28s/it] 79%|███████▉  | 79/100 [05:18<01:09,  3.33s/it] 80%|████████  | 80/100 [05:21<01:05,  3.28s/it] 81%|████████  | 81/100 [05:24<01:02,  3.31s/it] 82%|████████▏ | 82/100 [05:28<00:59,  3.33s/it] 83%|████████▎ | 83/100 [05:31<00:56,  3.30s/it] 84%|████████▍ | 84/100 [05:34<00:52,  3.27s/it] 85%|████████▌ | 85/100 [05:37<00:48,  3.27s/it] 86%|████████▌ | 86/100 [05:40<00:45,  3.24s/it] 87%|████████▋ | 87/100 [05:44<00:42,  3.24s/it] 88%|████████▊ | 88/100 [05:47<00:39,  3.27s/it] 89%|████████▉ | 89/100 [05:50<00:35,  3.25s/it] 90%|█████████ | 90/100 [05:54<00:32,  3.27s/it] 91%|█████████ | 91/100 [05:57<00:29,  3.29s/it] 92%|█████████▏| 92/100 [06:00<00:26,  3.28s/it] 93%|█████████▎| 93/100 [06:03<00:22,  3.27s/it] 94%|█████████▍| 94/100 [06:07<00:19,  3.31s/it] 95%|█████████▌| 95/100 [06:10<00:16,  3.32s/it] 96%|█████████▌| 96/100 [06:13<00:13,  3.29s/it] 97%|█████████▋| 97/100 [06:17<00:09,  3.27s/it] 98%|█████████▊| 98/100 [06:20<00:06,  3.29s/it] 99%|█████████▉| 99/100 [06:23<00:03,  3.29s/it]100%|██████████| 100/100 [06:27<00:00,  3.35s/it]100%|██████████| 100/100 [06:27<00:00,  3.87s/it]
***** eval metrics *****
  eval_accuracy           =     0.7903
  eval_loss               =     0.8215
  eval_runtime            = 0:06:31.33
  eval_samples_per_second =    127.767
  eval_steps_per_second   =      0.256
