INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino
06/19/2023 19:14:09 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
Resolving data files:   0%|          | 0/172443 [00:00<?, ?it/s]Resolving data files:   3%|▎         | 5430/172443 [00:00<00:03, 53505.31it/s]Resolving data files:   8%|▊         | 13634/172443 [00:00<00:02, 64306.48it/s]Resolving data files:  13%|█▎        | 22126/172443 [00:00<00:02, 70230.75it/s]Resolving data files:  18%|█▊        | 30328/172443 [00:00<00:02, 68320.76it/s]Resolving data files:  22%|██▏       | 37812/172443 [00:00<00:02, 61601.72it/s]Resolving data files:  26%|██▋       | 45503/172443 [00:00<00:01, 66046.08it/s]Resolving data files:  30%|███       | 52238/172443 [00:00<00:01, 62432.53it/s]Resolving data files:  55%|█████▌    | 95280/172443 [00:00<00:00, 166375.44it/s]Resolving data files:  92%|█████████▏| 158556/172443 [00:01<00:00, 300461.16it/s]Resolving data files: 100%|██████████| 172443/172443 [00:01<00:00, 165920.17it/s]
Resolving data files:   0%|          | 0/50000 [00:00<?, ?it/s]Resolving data files:  79%|███████▊  | 39297/50000 [00:00<00:00, 342219.59it/s]Resolving data files: 100%|██████████| 50000/50000 [00:01<00:00, 34965.77it/s] 
Downloading and preparing dataset imagefolder/default to /home/nlyaly/.cache/huggingface/datasets/imagefolder/default-c87315aba1f5f9fe/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...
Downloading data files:   0%|          | 0/172443 [00:00<?, ?it/s]Downloading data files:   7%|▋         | 12301/172443 [00:00<00:01, 123003.67it/s]Downloading data files:  15%|█▍        | 25087/172443 [00:00<00:01, 125856.11it/s]Downloading data files:  22%|██▏       | 37884/172443 [00:00<00:01, 126818.76it/s]Downloading data files:  29%|██▉       | 50786/172443 [00:00<00:00, 127683.72it/s]Downloading data files:  37%|███▋      | 63703/172443 [00:00<00:00, 128214.75it/s]Downloading data files:  44%|████▍     | 76607/172443 [00:00<00:00, 128491.40it/s]Downloading data files:  52%|█████▏    | 89478/172443 [00:00<00:00, 128560.40it/s]Downloading data files:  59%|█████▉    | 102478/172443 [00:00<00:00, 129014.60it/s]Downloading data files:  67%|██████▋   | 115380/172443 [00:00<00:00, 128745.07it/s]Downloading data files:  74%|███████▍  | 128255/172443 [00:01<00:00, 128551.43it/s]Downloading data files:  82%|████████▏ | 141113/172443 [00:01<00:00, 128557.20it/s]Downloading data files:  89%|████████▉ | 153969/172443 [00:01<00:00, 128450.27it/s]Downloading data files:  97%|█████████▋| 166872/172443 [00:01<00:00, 128622.77it/s]Downloading data files: 100%|██████████| 172443/172443 [00:01<00:00, 128214.36it/s]
Downloading data files: 0it [00:00, ?it/s]Downloading data files: 0it [00:00, ?it/s]
Extracting data files: 0it [00:00, ?it/s]Extracting data files: 0it [00:00, ?it/s]
Downloading data files:   0%|          | 0/50000 [00:00<?, ?it/s]Downloading data files:  25%|██▌       | 12551/50000 [00:00<00:00, 125498.45it/s]Downloading data files:  51%|█████     | 25254/50000 [00:00<00:00, 126393.55it/s]Downloading data files:  76%|███████▌  | 38091/50000 [00:00<00:00, 127293.84it/s]Downloading data files: 100%|██████████| 50000/50000 [00:00<00:00, 127255.91it/s]
Downloading data files: 0it [00:00, ?it/s]Downloading data files: 0it [00:00, ?it/s]
Extracting data files: 0it [00:00, ?it/s]Extracting data files: 0it [00:00, ?it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 2004 examples [00:00, 18826.95 examples/s]Generating train split: 4068 examples [00:00, 19861.23 examples/s]Generating train split: 6135 examples [00:00, 20224.57 examples/s]Generating train split: 8194 examples [00:00, 20363.88 examples/s]Generating train split: 11250 examples [00:00, 20361.75 examples/s]Generating train split: 13291 examples [00:00, 20374.27 examples/s]Generating train split: 16332 examples [00:00, 20330.63 examples/s]Generating train split: 19386 examples [00:00, 20336.22 examples/s]Generating train split: 21459 examples [00:01, 20432.65 examples/s]Generating train split: 23520 examples [00:01, 20478.70 examples/s]Generating train split: 26581 examples [00:01, 20449.25 examples/s]Generating train split: 28640 examples [00:01, 20481.90 examples/s]Generating train split: 30698 examples [00:01, 20506.60 examples/s]Generating train split: 33749 examples [00:01, 20440.37 examples/s]Generating train split: 35822 examples [00:01, 20512.62 examples/s]Generating train split: 37884 examples [00:01, 20540.01 examples/s]Generating train split: 40950 examples [00:02, 20499.97 examples/s]Generating train split: 44028 examples [00:02, 20453.95 examples/s]Generating train split: 47070 examples [00:02, 20390.88 examples/s]Generating train split: 50120 examples [00:02, 20368.99 examples/s]Generating train split: 52176 examples [00:02, 20413.31 examples/s]Generating train split: 54224 examples [00:02, 20425.68 examples/s]Generating train split: 57297 examples [00:02, 20443.44 examples/s]Generating train split: 59350 examples [00:02, 20462.99 examples/s]Generating train split: 61399 examples [00:03, 20466.51 examples/s]Generating train split: 64444 examples [00:03, 20403.79 examples/s]Generating train split: 66515 examples [00:03, 20480.00 examples/s]Generating train split: 68586 examples [00:03, 20539.73 examples/s]Generating train split: 70645 examples [00:03, 20551.75 examples/s]Generating train split: 72707 examples [00:03, 20569.34 examples/s]Generating train split: 75766 examples [00:03, 20497.78 examples/s]Generating train split: 78821 examples [00:03, 20449.85 examples/s]Generating train split: 81867 examples [00:04, 20396.26 examples/s]Generating train split: 84916 examples [00:04, 20369.83 examples/s]Generating train split: 86965 examples [00:04, 20395.60 examples/s]Generating train split: 89031 examples [00:04, 20369.45 examples/s]Generating train split: 91074 examples [00:04, 20382.61 examples/s]Generating train split: 93124 examples [00:04, 20410.96 examples/s]Generating train split: 96194 examples [00:04, 20427.06 examples/s]Generating train split: 98263 examples [00:04, 20493.12 examples/s]Generating train split: 100324 examples [00:04, 20520.92 examples/s]Generating train split: 103395 examples [00:05, 20501.20 examples/s]Generating train split: 105453 examples [00:05, 20519.47 examples/s]Generating train split: 108506 examples [00:05, 20455.86 examples/s]Generating train split: 111571 examples [00:05, 20444.65 examples/s]Generating train split: 113638 examples [00:05, 20498.43 examples/s]Generating train split: 116696 examples [00:05, 20458.28 examples/s]Generating train split: 118747 examples [00:05, 20469.16 examples/s]Generating train split: 121804 examples [00:05, 20434.78 examples/s]Generating train split: 123864 examples [00:06, 20474.37 examples/s]Generating train split: 126896 examples [00:06, 20379.69 examples/s]Generating train split: 129958 examples [00:06, 20387.46 examples/s]Generating train split: 132000 examples [00:06, 20342.10 examples/s]Generating train split: 134047 examples [00:06, 20374.30 examples/s]Generating train split: 136106 examples [00:06, 20429.09 examples/s]Generating train split: 138172 examples [00:06, 20490.51 examples/s]Generating train split: 141229 examples [00:06, 20445.12 examples/s]Generating train split: 144304 examples [00:07, 20460.97 examples/s]Generating train split: 146359 examples [00:07, 20480.36 examples/s]Generating train split: 149421 examples [00:07, 20453.79 examples/s]Generating train split: 151469 examples [00:07, 20457.50 examples/s]Generating train split: 154493 examples [00:07, 20350.78 examples/s]Generating train split: 157513 examples [00:07, 20274.70 examples/s]Generating train split: 159550 examples [00:07, 20296.37 examples/s]Generating train split: 162614 examples [00:07, 20337.32 examples/s]Generating train split: 164664 examples [00:08, 20373.92 examples/s]Generating train split: 166716 examples [00:08, 20409.04 examples/s]Generating train split: 168759 examples [00:08, 20412.09 examples/s]Generating train split: 171815 examples [00:08, 20395.59 examples/s]                                                                    Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 2013 examples [00:00, 19877.75 examples/s]Generating validation split: 4055 examples [00:00, 20187.75 examples/s]Generating validation split: 6080 examples [00:00, 20214.30 examples/s]Generating validation split: 8140 examples [00:00, 20359.71 examples/s]Generating validation split: 10194 examples [00:00, 20418.87 examples/s]Generating validation split: 12237 examples [00:00, 20420.21 examples/s]Generating validation split: 14301 examples [00:00, 20488.23 examples/s]Generating validation split: 17343 examples [00:00, 20399.78 examples/s]Generating validation split: 20378 examples [00:01, 20336.23 examples/s]Generating validation split: 23395 examples [00:01, 20253.76 examples/s]Generating validation split: 26406 examples [00:01, 20192.96 examples/s]Generating validation split: 28428 examples [00:01, 20195.63 examples/s]Generating validation split: 31422 examples [00:01, 20112.11 examples/s]Generating validation split: 34425 examples [00:01, 20079.62 examples/s]Generating validation split: 37425 examples [00:01, 20052.11 examples/s]Generating validation split: 39451 examples [00:01, 20098.61 examples/s]Generating validation split: 41486 examples [00:02, 20158.89 examples/s]Generating validation split: 43540 examples [00:02, 20256.32 examples/s]Generating validation split: 45577 examples [00:02, 20285.69 examples/s]Generating validation split: 47611 examples [00:02, 20297.94 examples/s]Generating validation split: 50000 examples [00:02, 20210.48 examples/s]                                                                        Dataset imagefolder downloaded and prepared to /home/nlyaly/.cache/huggingface/datasets/imagefolder/default-c87315aba1f5f9fe/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  1.59it/s]100%|██████████| 2/2 [00:01<00:00,  1.62it/s]100%|██████████| 2/2 [00:01<00:00,  1.62it/s]
Casting the dataset:   0%|          | 0/172443 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 172443/172443 [00:00<00:00, 267454.04 examples/s]                                                                                       Casting the dataset:   0%|          | 0/50000 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 50000/50000 [00:00<00:00, 79459.40 examples/s]                                                                                    create_compressed_model

/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.
  warnings.warn(
ViTForImageClassification(
  (vit): ViTModel(
    (embeddings): ViTEmbeddings(
      (patch_embeddings): ViTPatchEmbeddings(
        (projection): NNCFConv2d(
          3, 768, kernel_size=(16, 16), stride=(16, 16)
          (pre_ops): ModuleDict()
          (post_ops): ModuleDict()
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): ViTEncoder(
      (layer): ModuleList(
        (0): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (1): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (2): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (3): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (4): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (5): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (6): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (7): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (8): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (9): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (10): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
        (11): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (key): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (value): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): NNCFLinear(
                in_features=768, out_features=768, bias=True
                (pre_ops): ModuleDict(
                  (0): UpdateWeight(
                    (op): SymmetricQuantizer(bit=8, ch=True)
                  )
                )
                (post_ops): ModuleDict()
              )
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): NNCFLinear(
              in_features=768, out_features=3072, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): NNCFLinear(
              in_features=3072, out_features=768, bias=True
              (pre_ops): ModuleDict(
                (0): UpdateWeight(
                  (op): SymmetricQuantizer(bit=8, ch=True)
                )
              )
              (post_ops): ModuleDict()
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
          (layernorm_after): NNCFLayerNorm(
            (768,), eps=1e-12, elementwise_affine=True
            (pre_ops): ModuleDict()
            (post_ops): ModuleDict()
          )
        )
      )
    )
    (layernorm): NNCFLayerNorm(
      (768,), eps=1e-12, elementwise_affine=True
      (pre_ops): ModuleDict()
      (post_ops): ModuleDict()
    )
  )
  (classifier): NNCFLinear(
    in_features=768, out_features=1000, bias=True
    (pre_ops): ModuleDict(
      (0): UpdateWeight(
        (op): SymmetricQuantizer(bit=8, ch=True)
      )
    )
    (post_ops): ModuleDict()
  )
  (_nncf): NNCFNetworkInterface(
    (external_quantizers): ModuleDict(
      (ViTForImageClassification/ViTModel[vit]/NNCFLayerNorm[layernorm]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[0]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[10]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[11]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[1]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[2]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[3]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[4]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[5]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[6]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[7]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[8]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/NNCFLayerNorm[layernorm_after]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/NNCFLayerNorm[layernorm_before]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[key]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTAttention[attention]/ViTSelfAttention[attention]/NNCFLinear[query]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTAttention[attention]/ViTSelfAttention[attention]/matmul_1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
      (ViTForImageClassification/ViTModel[vit]/ViTEncoder[encoder]/ModuleList[layer]/ViTLayer[9]/ViTIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)
    )
  )
)/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(

process layer0
 With wd:
attention.attention.query.weight
attention.attention.key.weight
attention.attention.value.weight
attention.output.dense.weight
intermediate.dense.weight
output.dense.weight
layernorm_before.weight
layernorm_after.weight
 No_wd:
attention.attention.query.bias
attention.attention.key.bias
attention.attention.value.bias
attention.output.dense.bias
intermediate.dense.bias
output.dense.bias
layernorm_before.bias
layernorm_after.bias
=) 45.77% for 0
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:05,  1.89s/it]  3%|▎         | 3/100 [00:07<04:23,  2.71s/it]  4%|▍         | 4/100 [00:12<05:40,  3.55s/it]  5%|▌         | 5/100 [00:18<06:48,  4.30s/it]  6%|▌         | 6/100 [00:23<07:10,  4.58s/it]  7%|▋         | 7/100 [00:27<07:01,  4.53s/it]  8%|▊         | 8/100 [00:32<07:03,  4.60s/it]  9%|▉         | 9/100 [00:36<06:29,  4.28s/it] 10%|█         | 10/100 [00:39<06:05,  4.06s/it] 11%|█         | 11/100 [00:43<05:50,  3.93s/it] 12%|█▏        | 12/100 [00:47<05:38,  3.85s/it] 13%|█▎        | 13/100 [00:50<05:35,  3.85s/it] 14%|█▍        | 14/100 [00:54<05:24,  3.77s/it] 15%|█▌        | 15/100 [00:58<05:15,  3.71s/it] 16%|█▌        | 16/100 [01:01<05:04,  3.62s/it] 17%|█▋        | 17/100 [01:05<04:57,  3.59s/it] 18%|█▊        | 18/100 [01:08<04:54,  3.59s/it] 19%|█▉        | 19/100 [01:12<04:47,  3.55s/it] 20%|██        | 20/100 [01:15<04:42,  3.53s/it] 21%|██        | 21/100 [01:18<04:36,  3.50s/it] 22%|██▏       | 22/100 [01:22<04:35,  3.53s/it] 23%|██▎       | 23/100 [01:26<04:30,  3.51s/it] 24%|██▍       | 24/100 [01:29<04:27,  3.52s/it] 25%|██▌       | 25/100 [01:33<04:31,  3.62s/it] 26%|██▌       | 26/100 [01:36<04:23,  3.56s/it] 27%|██▋       | 27/100 [01:40<04:24,  3.63s/it] 28%|██▊       | 28/100 [01:44<04:18,  3.59s/it] 29%|██▉       | 29/100 [01:47<04:15,  3.59s/it] 30%|███       | 30/100 [01:51<04:14,  3.63s/it] 31%|███       | 31/100 [01:55<04:12,  3.66s/it] 32%|███▏      | 32/100 [01:58<04:09,  3.67s/it] 33%|███▎      | 33/100 [02:02<04:06,  3.68s/it] 34%|███▍      | 34/100 [02:05<03:57,  3.60s/it] 35%|███▌      | 35/100 [02:09<03:53,  3.60s/it] 36%|███▌      | 36/100 [02:13<03:50,  3.60s/it] 37%|███▋      | 37/100 [02:16<03:43,  3.54s/it] 38%|███▊      | 38/100 [02:20<03:48,  3.68s/it] 39%|███▉      | 39/100 [02:24<03:50,  3.78s/it] 40%|████      | 40/100 [02:28<03:45,  3.75s/it] 41%|████      | 41/100 [02:31<03:36,  3.68s/it] 42%|████▏     | 42/100 [02:36<03:45,  3.88s/it] 43%|████▎     | 43/100 [02:39<03:38,  3.83s/it] 44%|████▍     | 44/100 [02:43<03:27,  3.71s/it] 45%|████▌     | 45/100 [02:46<03:19,  3.63s/it] 46%|████▌     | 46/100 [02:50<03:17,  3.67s/it] 47%|████▋     | 47/100 [02:54<03:16,  3.71s/it] 48%|████▊     | 48/100 [02:57<03:09,  3.64s/it] 49%|████▉     | 49/100 [03:01<03:02,  3.58s/it] 50%|█████     | 50/100 [03:04<03:00,  3.61s/it] 51%|█████     | 51/100 [03:08<02:56,  3.60s/it] 52%|█████▏    | 52/100 [03:11<02:48,  3.52s/it] 53%|█████▎    | 53/100 [03:15<02:47,  3.56s/it] 54%|█████▍    | 54/100 [03:18<02:42,  3.54s/it] 55%|█████▌    | 55/100 [03:22<02:40,  3.56s/it] 56%|█████▌    | 56/100 [03:26<02:36,  3.55s/it] 57%|█████▋    | 57/100 [03:29<02:31,  3.53s/it] 58%|█████▊    | 58/100 [03:33<02:27,  3.52s/it] 59%|█████▉    | 59/100 [03:36<02:26,  3.57s/it] 60%|██████    | 60/100 [03:40<02:24,  3.61s/it] 61%|██████    | 61/100 [03:43<02:18,  3.55s/it] 62%|██████▏   | 62/100 [03:47<02:14,  3.53s/it] 63%|██████▎   | 63/100 [03:50<02:09,  3.49s/it] 64%|██████▍   | 64/100 [03:54<02:05,  3.48s/it] 65%|██████▌   | 65/100 [03:57<02:02,  3.51s/it] 66%|██████▌   | 66/100 [04:01<01:59,  3.51s/it] 67%|██████▋   | 67/100 [04:04<01:57,  3.57s/it] 68%|██████▊   | 68/100 [04:09<02:01,  3.80s/it] 69%|██████▉   | 69/100 [04:13<02:01,  3.93s/it] 70%|███████   | 70/100 [04:18<02:03,  4.12s/it] 71%|███████   | 71/100 [04:21<01:54,  3.96s/it] 72%|███████▏  | 72/100 [04:25<01:47,  3.84s/it] 73%|███████▎  | 73/100 [04:28<01:41,  3.75s/it] 74%|███████▍  | 74/100 [04:32<01:37,  3.75s/it] 75%|███████▌  | 75/100 [04:35<01:31,  3.65s/it] 76%|███████▌  | 76/100 [04:40<01:31,  3.81s/it] 77%|███████▋  | 77/100 [04:43<01:25,  3.70s/it] 78%|███████▊  | 78/100 [04:47<01:20,  3.66s/it] 79%|███████▉  | 79/100 [04:50<01:16,  3.65s/it] 80%|████████  | 80/100 [04:54<01:11,  3.57s/it] 81%|████████  | 81/100 [04:57<01:07,  3.57s/it] 82%|████████▏ | 82/100 [05:01<01:04,  3.60s/it] 83%|████████▎ | 83/100 [05:05<01:04,  3.80s/it] 84%|████████▍ | 84/100 [05:09<00:58,  3.68s/it] 85%|████████▌ | 85/100 [05:12<00:54,  3.66s/it] 86%|████████▌ | 86/100 [05:17<00:54,  3.86s/it] 87%|████████▋ | 87/100 [05:20<00:50,  3.85s/it] 88%|████████▊ | 88/100 [05:24<00:45,  3.80s/it] 89%|████████▉ | 89/100 [05:28<00:40,  3.72s/it] 90%|█████████ | 90/100 [05:31<00:36,  3.67s/it] 91%|█████████ | 91/100 [05:35<00:32,  3.66s/it] 92%|█████████▏| 92/100 [05:38<00:28,  3.61s/it] 93%|█████████▎| 93/100 [05:42<00:25,  3.57s/it] 94%|█████████▍| 94/100 [05:45<00:21,  3.60s/it] 95%|█████████▌| 95/100 [05:49<00:17,  3.58s/it] 96%|█████████▌| 96/100 [05:52<00:14,  3.53s/it] 97%|█████████▋| 97/100 [05:56<00:10,  3.50s/it] 98%|█████████▊| 98/100 [06:00<00:07,  3.81s/it] 99%|█████████▉| 99/100 [06:04<00:03,  3.84s/it]100%|██████████| 100/100 [06:08<00:00,  3.87s/it]100%|██████████| 100/100 [06:08<00:00,  3.69s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer1
=) 28.28% for 1
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:03,  1.87s/it]  3%|▎         | 3/100 [00:07<04:29,  2.78s/it]  4%|▍         | 4/100 [00:11<05:09,  3.23s/it]  5%|▌         | 5/100 [00:15<05:30,  3.48s/it]  6%|▌         | 6/100 [00:19<05:30,  3.51s/it]  7%|▋         | 7/100 [00:22<05:25,  3.50s/it]  8%|▊         | 8/100 [00:26<05:32,  3.61s/it]  9%|▉         | 9/100 [00:30<05:27,  3.60s/it] 10%|█         | 10/100 [00:33<05:26,  3.63s/it] 11%|█         | 11/100 [00:37<05:20,  3.60s/it] 12%|█▏        | 12/100 [00:41<05:19,  3.63s/it] 13%|█▎        | 13/100 [00:44<05:21,  3.70s/it] 14%|█▍        | 14/100 [00:48<05:17,  3.69s/it] 15%|█▌        | 15/100 [00:52<05:07,  3.62s/it] 16%|█▌        | 16/100 [00:55<05:01,  3.59s/it] 17%|█▋        | 17/100 [00:59<04:59,  3.61s/it] 18%|█▊        | 18/100 [01:03<04:58,  3.64s/it] 19%|█▉        | 19/100 [01:06<04:52,  3.61s/it] 20%|██        | 20/100 [01:10<04:47,  3.59s/it] 21%|██        | 21/100 [01:13<04:40,  3.55s/it] 22%|██▏       | 22/100 [01:17<04:38,  3.57s/it] 23%|██▎       | 23/100 [01:20<04:34,  3.56s/it] 24%|██▍       | 24/100 [01:24<04:29,  3.54s/it] 25%|██▌       | 25/100 [01:27<04:24,  3.52s/it] 26%|██▌       | 26/100 [01:31<04:19,  3.50s/it] 27%|██▋       | 27/100 [01:34<04:20,  3.56s/it] 28%|██▊       | 28/100 [01:38<04:16,  3.56s/it] 29%|██▉       | 29/100 [01:42<04:33,  3.86s/it] 30%|███       | 30/100 [01:48<05:09,  4.42s/it] 31%|███       | 31/100 [01:52<04:56,  4.30s/it] 32%|███▏      | 32/100 [01:56<04:37,  4.09s/it] 33%|███▎      | 33/100 [02:00<04:26,  3.98s/it] 34%|███▍      | 34/100 [02:03<04:10,  3.80s/it] 35%|███▌      | 35/100 [02:07<04:05,  3.77s/it] 36%|███▌      | 36/100 [02:10<03:56,  3.70s/it] 37%|███▋      | 37/100 [02:14<03:57,  3.76s/it] 38%|███▊      | 38/100 [02:18<03:53,  3.77s/it] 39%|███▉      | 39/100 [02:22<03:52,  3.81s/it] 40%|████      | 40/100 [02:26<03:48,  3.80s/it] 41%|████      | 41/100 [02:30<03:50,  3.91s/it] 42%|████▏     | 42/100 [02:34<03:48,  3.95s/it] 43%|████▎     | 43/100 [02:37<03:36,  3.80s/it] 44%|████▍     | 44/100 [02:41<03:31,  3.77s/it] 45%|████▌     | 45/100 [02:45<03:32,  3.86s/it] 46%|████▌     | 46/100 [02:50<03:41,  4.10s/it] 47%|████▋     | 47/100 [02:53<03:34,  4.04s/it] 48%|████▊     | 48/100 [02:57<03:20,  3.86s/it] 49%|████▉     | 49/100 [03:00<03:11,  3.76s/it] 50%|█████     | 50/100 [03:04<03:03,  3.67s/it] 51%|█████     | 51/100 [03:08<03:00,  3.69s/it] 52%|█████▏    | 52/100 [03:11<02:58,  3.71s/it] 53%|█████▎    | 53/100 [03:15<02:55,  3.72s/it] 54%|█████▍    | 54/100 [03:19<02:52,  3.76s/it] 55%|█████▌    | 55/100 [03:22<02:44,  3.66s/it] 56%|█████▌    | 56/100 [03:26<02:40,  3.65s/it] 57%|█████▋    | 57/100 [03:30<02:35,  3.61s/it] 58%|█████▊    | 58/100 [03:33<02:30,  3.58s/it] 59%|█████▉    | 59/100 [03:37<02:28,  3.62s/it] 60%|██████    | 60/100 [03:41<02:26,  3.67s/it] 61%|██████    | 61/100 [03:44<02:20,  3.61s/it] 62%|██████▏   | 62/100 [03:48<02:16,  3.60s/it] 63%|██████▎   | 63/100 [03:51<02:11,  3.55s/it] 64%|██████▍   | 64/100 [03:55<02:11,  3.67s/it] 65%|██████▌   | 65/100 [04:00<02:24,  4.13s/it] 66%|██████▌   | 66/100 [04:05<02:23,  4.22s/it] 67%|██████▋   | 67/100 [04:08<02:11,  3.98s/it] 68%|██████▊   | 68/100 [04:12<02:05,  3.93s/it] 69%|██████▉   | 69/100 [04:16<02:03,  3.97s/it] 70%|███████   | 70/100 [04:20<01:56,  3.90s/it] 71%|███████   | 71/100 [04:23<01:50,  3.80s/it] 72%|███████▏  | 72/100 [04:27<01:44,  3.73s/it] 73%|███████▎  | 73/100 [04:30<01:39,  3.68s/it] 74%|███████▍  | 74/100 [04:34<01:34,  3.64s/it] 75%|███████▌  | 75/100 [04:37<01:28,  3.54s/it] 76%|███████▌  | 76/100 [04:41<01:24,  3.53s/it] 77%|███████▋  | 77/100 [04:44<01:22,  3.59s/it] 78%|███████▊  | 78/100 [04:48<01:19,  3.60s/it] 79%|███████▉  | 79/100 [04:52<01:15,  3.61s/it] 80%|████████  | 80/100 [04:55<01:11,  3.55s/it] 81%|████████  | 81/100 [04:59<01:08,  3.59s/it] 82%|████████▏ | 82/100 [05:02<01:04,  3.60s/it] 83%|████████▎ | 83/100 [05:06<01:00,  3.56s/it] 84%|████████▍ | 84/100 [05:09<00:56,  3.54s/it] 85%|████████▌ | 85/100 [05:13<00:54,  3.61s/it] 86%|████████▌ | 86/100 [05:17<00:49,  3.55s/it] 87%|████████▋ | 87/100 [05:20<00:45,  3.53s/it] 88%|████████▊ | 88/100 [05:24<00:43,  3.66s/it] 89%|████████▉ | 89/100 [05:28<00:40,  3.64s/it] 90%|█████████ | 90/100 [05:31<00:36,  3.62s/it] 91%|█████████ | 91/100 [05:35<00:32,  3.62s/it] 92%|█████████▏| 92/100 [05:38<00:28,  3.58s/it] 93%|█████████▎| 93/100 [05:42<00:25,  3.69s/it] 94%|█████████▍| 94/100 [05:46<00:22,  3.79s/it] 95%|█████████▌| 95/100 [05:51<00:20,  4.10s/it] 96%|█████████▌| 96/100 [05:56<00:16,  4.20s/it] 97%|█████████▋| 97/100 [05:59<00:12,  4.03s/it] 98%|█████████▊| 98/100 [06:04<00:08,  4.33s/it] 99%|█████████▉| 99/100 [06:08<00:04,  4.03s/it]100%|██████████| 100/100 [06:11<00:00,  3.99s/it]100%|██████████| 100/100 [06:12<00:00,  3.72s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer2
=) 29.70% for 2
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:55,  1.79s/it]  3%|▎         | 3/100 [00:08<04:51,  3.00s/it]  4%|▍         | 4/100 [00:12<05:18,  3.32s/it]  5%|▌         | 5/100 [00:15<05:29,  3.47s/it]  6%|▌         | 6/100 [00:19<05:34,  3.55s/it]  7%|▋         | 7/100 [00:23<05:37,  3.62s/it]  8%|▊         | 8/100 [00:27<05:44,  3.75s/it]  9%|▉         | 9/100 [00:31<05:40,  3.74s/it] 10%|█         | 10/100 [00:34<05:36,  3.74s/it] 11%|█         | 11/100 [00:38<05:30,  3.72s/it] 12%|█▏        | 12/100 [00:42<05:32,  3.78s/it] 13%|█▎        | 13/100 [00:47<06:06,  4.21s/it] 14%|█▍        | 14/100 [00:52<06:30,  4.54s/it] 15%|█▌        | 15/100 [00:56<06:07,  4.32s/it] 16%|█▌        | 16/100 [01:00<05:39,  4.04s/it] 17%|█▋        | 17/100 [01:03<05:26,  3.93s/it] 18%|█▊        | 18/100 [01:07<05:18,  3.89s/it] 19%|█▉        | 19/100 [01:11<05:04,  3.76s/it] 20%|██        | 20/100 [01:14<04:55,  3.69s/it] 21%|██        | 21/100 [01:18<04:47,  3.64s/it] 22%|██▏       | 22/100 [01:21<04:49,  3.71s/it] 23%|██▎       | 23/100 [01:25<04:46,  3.72s/it] 24%|██▍       | 24/100 [01:29<04:43,  3.73s/it] 25%|██▌       | 25/100 [01:33<04:38,  3.72s/it] 26%|██▌       | 26/100 [01:36<04:33,  3.70s/it] 27%|██▋       | 27/100 [01:41<04:58,  4.09s/it] 28%|██▊       | 28/100 [01:47<05:27,  4.55s/it] 29%|██▉       | 29/100 [01:52<05:39,  4.78s/it] 30%|███       | 30/100 [01:58<06:03,  5.20s/it] 31%|███       | 31/100 [02:02<05:33,  4.83s/it] 32%|███▏      | 32/100 [02:07<05:22,  4.74s/it] 33%|███▎      | 33/100 [02:14<06:11,  5.55s/it] 34%|███▍      | 34/100 [02:21<06:36,  6.01s/it] 35%|███▌      | 35/100 [02:26<06:07,  5.66s/it] 36%|███▌      | 36/100 [02:30<05:27,  5.11s/it] 37%|███▋      | 37/100 [02:34<04:48,  4.58s/it] 38%|███▊      | 38/100 [02:37<04:29,  4.35s/it] 39%|███▉      | 39/100 [02:41<04:15,  4.20s/it] 40%|████      | 40/100 [02:46<04:21,  4.37s/it] 41%|████      | 41/100 [02:49<04:03,  4.13s/it] 42%|████▏     | 42/100 [02:53<03:52,  4.01s/it] 43%|████▎     | 43/100 [02:57<03:52,  4.09s/it] 44%|████▍     | 44/100 [03:01<03:36,  3.87s/it] 45%|████▌     | 45/100 [03:04<03:28,  3.79s/it] 46%|████▌     | 46/100 [03:09<03:38,  4.05s/it] 47%|████▋     | 47/100 [03:13<03:36,  4.09s/it] 48%|████▊     | 48/100 [03:17<03:26,  3.97s/it] 49%|████▉     | 49/100 [03:21<03:15,  3.84s/it] 50%|█████     | 50/100 [03:24<03:06,  3.73s/it] 51%|█████     | 51/100 [03:27<02:58,  3.65s/it] 52%|█████▏    | 52/100 [03:31<02:58,  3.73s/it] 53%|█████▎    | 53/100 [03:35<03:00,  3.84s/it] 54%|█████▍    | 54/100 [03:39<02:51,  3.72s/it] 55%|█████▌    | 55/100 [03:42<02:44,  3.65s/it] 56%|█████▌    | 56/100 [03:46<02:43,  3.72s/it] 57%|█████▋    | 57/100 [03:50<02:41,  3.75s/it] 58%|█████▊    | 58/100 [03:54<02:37,  3.76s/it] 59%|█████▉    | 59/100 [03:59<02:52,  4.22s/it] 60%|██████    | 60/100 [04:04<02:52,  4.31s/it] 61%|██████    | 61/100 [04:07<02:37,  4.04s/it] 62%|██████▏   | 62/100 [04:11<02:26,  3.86s/it] 63%|██████▎   | 63/100 [04:14<02:18,  3.73s/it] 64%|██████▍   | 64/100 [04:18<02:14,  3.74s/it] 65%|██████▌   | 65/100 [04:22<02:16,  3.91s/it] 66%|██████▌   | 66/100 [04:26<02:17,  4.03s/it] 67%|██████▋   | 67/100 [04:31<02:21,  4.28s/it] 68%|██████▊   | 68/100 [04:36<02:22,  4.46s/it] 69%|██████▉   | 69/100 [04:41<02:26,  4.74s/it] 70%|███████   | 70/100 [04:45<02:13,  4.44s/it] 71%|███████   | 71/100 [04:49<02:01,  4.18s/it] 72%|███████▏  | 72/100 [04:53<01:53,  4.07s/it] 73%|███████▎  | 73/100 [04:57<01:54,  4.25s/it] 74%|███████▍  | 74/100 [05:01<01:49,  4.22s/it] 75%|███████▌  | 75/100 [05:05<01:40,  4.04s/it] 76%|███████▌  | 76/100 [05:10<01:41,  4.21s/it] 77%|███████▋  | 77/100 [05:15<01:42,  4.48s/it] 78%|███████▊  | 78/100 [05:20<01:44,  4.73s/it] 79%|███████▉  | 79/100 [05:25<01:37,  4.67s/it] 80%|████████  | 80/100 [05:28<01:25,  4.29s/it] 81%|████████  | 81/100 [05:32<01:19,  4.19s/it] 82%|████████▏ | 82/100 [05:36<01:12,  4.01s/it] 83%|████████▎ | 83/100 [05:40<01:08,  4.03s/it] 84%|████████▍ | 84/100 [05:43<01:02,  3.93s/it] 85%|████████▌ | 85/100 [05:47<00:57,  3.82s/it] 86%|████████▌ | 86/100 [05:50<00:51,  3.71s/it] 87%|████████▋ | 87/100 [05:54<00:47,  3.69s/it] 88%|████████▊ | 88/100 [05:58<00:45,  3.77s/it] 89%|████████▉ | 89/100 [06:02<00:42,  3.88s/it] 90%|█████████ | 90/100 [06:06<00:38,  3.81s/it] 91%|█████████ | 91/100 [06:09<00:33,  3.77s/it] 92%|█████████▏| 92/100 [06:13<00:29,  3.75s/it] 93%|█████████▎| 93/100 [06:17<00:25,  3.66s/it] 94%|█████████▍| 94/100 [06:20<00:21,  3.65s/it] 95%|█████████▌| 95/100 [06:24<00:18,  3.69s/it] 96%|█████████▌| 96/100 [06:27<00:14,  3.62s/it] 97%|█████████▋| 97/100 [06:31<00:10,  3.56s/it] 98%|█████████▊| 98/100 [06:34<00:07,  3.56s/it] 99%|█████████▉| 99/100 [06:38<00:03,  3.60s/it]100%|██████████| 100/100 [06:42<00:00,  3.76s/it]100%|██████████| 100/100 [06:42<00:00,  4.03s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer3
=) 32.15% for 3
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:56,  1.80s/it]  3%|▎         | 3/100 [00:07<04:14,  2.62s/it]  4%|▍         | 4/100 [00:11<04:58,  3.11s/it]  5%|▌         | 5/100 [00:15<05:33,  3.51s/it]  6%|▌         | 6/100 [00:19<05:35,  3.57s/it]  7%|▋         | 7/100 [00:23<05:37,  3.63s/it]  8%|▊         | 8/100 [00:27<05:44,  3.75s/it]  9%|▉         | 9/100 [00:30<05:39,  3.74s/it] 10%|█         | 10/100 [00:34<05:35,  3.73s/it] 11%|█         | 11/100 [00:38<05:30,  3.72s/it] 12%|█▏        | 12/100 [00:42<05:32,  3.78s/it] 13%|█▎        | 13/100 [00:46<05:34,  3.84s/it] 14%|█▍        | 14/100 [00:49<05:23,  3.77s/it] 15%|█▌        | 15/100 [00:53<05:18,  3.75s/it] 16%|█▌        | 16/100 [00:56<05:10,  3.69s/it] 17%|█▋        | 17/100 [01:00<05:01,  3.64s/it] 18%|█▊        | 18/100 [01:03<04:57,  3.63s/it] 19%|█▉        | 19/100 [01:07<04:50,  3.59s/it] 20%|██        | 20/100 [01:11<04:56,  3.71s/it] 21%|██        | 21/100 [01:15<04:54,  3.72s/it] 22%|██▏       | 22/100 [01:18<04:49,  3.71s/it] 23%|██▎       | 23/100 [01:22<04:41,  3.65s/it] 24%|██▍       | 24/100 [01:26<04:36,  3.63s/it] 25%|██▌       | 25/100 [01:29<04:36,  3.69s/it] 26%|██▌       | 26/100 [01:33<04:34,  3.71s/it] 27%|██▋       | 27/100 [01:37<04:28,  3.68s/it] 28%|██▊       | 28/100 [01:41<04:27,  3.72s/it] 29%|██▉       | 29/100 [01:44<04:24,  3.73s/it] 30%|███       | 30/100 [01:49<04:49,  4.13s/it] 31%|███       | 31/100 [01:54<04:59,  4.35s/it] 32%|███▏      | 32/100 [01:58<04:41,  4.15s/it] 33%|███▎      | 33/100 [02:02<04:31,  4.05s/it] 34%|███▍      | 34/100 [02:05<04:14,  3.86s/it] 35%|███▌      | 35/100 [02:09<04:03,  3.75s/it] 36%|███▌      | 36/100 [02:12<03:58,  3.72s/it] 37%|███▋      | 37/100 [02:17<04:11,  3.99s/it] 38%|███▊      | 38/100 [02:23<04:40,  4.52s/it] 39%|███▉      | 39/100 [02:28<04:56,  4.86s/it] 40%|████      | 40/100 [02:33<04:43,  4.73s/it] 41%|████      | 41/100 [02:36<04:13,  4.30s/it] 42%|████▏     | 42/100 [02:39<03:55,  4.05s/it] 43%|████▎     | 43/100 [02:43<03:44,  3.94s/it] 44%|████▍     | 44/100 [02:47<03:32,  3.79s/it] 45%|████▌     | 45/100 [02:50<03:24,  3.72s/it] 46%|████▌     | 46/100 [02:54<03:21,  3.74s/it] 47%|████▋     | 47/100 [02:59<03:32,  4.00s/it] 48%|████▊     | 48/100 [03:02<03:24,  3.93s/it] 49%|████▉     | 49/100 [03:06<03:14,  3.81s/it] 50%|█████     | 50/100 [03:10<03:12,  3.85s/it] 51%|█████     | 51/100 [03:13<03:03,  3.74s/it] 52%|█████▏    | 52/100 [03:17<02:55,  3.65s/it] 53%|█████▎    | 53/100 [03:21<02:55,  3.74s/it] 54%|█████▍    | 54/100 [03:24<02:47,  3.65s/it] 55%|█████▌    | 55/100 [03:28<02:42,  3.61s/it] 56%|█████▌    | 56/100 [03:31<02:38,  3.60s/it] 57%|█████▋    | 57/100 [03:35<02:34,  3.60s/it] 58%|█████▊    | 58/100 [03:38<02:29,  3.56s/it] 59%|█████▉    | 59/100 [03:42<02:28,  3.62s/it] 60%|██████    | 60/100 [03:46<02:29,  3.74s/it] 61%|██████    | 61/100 [03:50<02:23,  3.68s/it] 62%|██████▏   | 62/100 [03:53<02:19,  3.67s/it] 63%|██████▎   | 63/100 [03:57<02:13,  3.61s/it] 64%|██████▍   | 64/100 [04:00<02:10,  3.62s/it] 65%|██████▌   | 65/100 [04:04<02:06,  3.62s/it] 66%|██████▌   | 66/100 [04:07<02:01,  3.58s/it] 67%|██████▋   | 67/100 [04:11<01:57,  3.57s/it] 68%|██████▊   | 68/100 [04:14<01:53,  3.54s/it] 69%|██████▉   | 69/100 [04:18<01:49,  3.52s/it] 70%|███████   | 70/100 [04:22<01:46,  3.54s/it] 71%|███████   | 71/100 [04:25<01:43,  3.55s/it] 72%|███████▏  | 72/100 [04:29<01:42,  3.65s/it] 73%|███████▎  | 73/100 [04:33<01:42,  3.78s/it] 74%|███████▍  | 74/100 [04:37<01:36,  3.72s/it] 75%|███████▌  | 75/100 [04:40<01:30,  3.62s/it] 76%|███████▌  | 76/100 [04:43<01:25,  3.57s/it] 77%|███████▋  | 77/100 [04:47<01:22,  3.60s/it] 78%|███████▊  | 78/100 [04:51<01:22,  3.76s/it] 79%|███████▉  | 79/100 [04:55<01:19,  3.80s/it] 80%|████████  | 80/100 [04:59<01:15,  3.78s/it] 81%|████████  | 81/100 [05:02<01:10,  3.71s/it] 82%|████████▏ | 82/100 [05:06<01:06,  3.70s/it] 83%|████████▎ | 83/100 [05:10<01:01,  3.62s/it] 84%|████████▍ | 84/100 [05:13<00:56,  3.56s/it] 85%|████████▌ | 85/100 [05:17<00:53,  3.57s/it] 86%|████████▌ | 86/100 [05:20<00:49,  3.55s/it] 87%|████████▋ | 87/100 [05:24<00:46,  3.55s/it] 88%|████████▊ | 88/100 [05:27<00:42,  3.57s/it] 89%|████████▉ | 89/100 [05:31<00:39,  3.61s/it] 90%|█████████ | 90/100 [05:35<00:36,  3.61s/it] 91%|█████████ | 91/100 [05:38<00:32,  3.62s/it] 92%|█████████▏| 92/100 [05:42<00:29,  3.64s/it] 93%|█████████▎| 93/100 [05:46<00:25,  3.66s/it] 94%|█████████▍| 94/100 [05:49<00:22,  3.72s/it] 95%|█████████▌| 95/100 [05:53<00:18,  3.66s/it] 96%|█████████▌| 96/100 [05:56<00:14,  3.61s/it] 97%|█████████▋| 97/100 [06:00<00:10,  3.60s/it] 98%|█████████▊| 98/100 [06:04<00:07,  3.81s/it] 99%|█████████▉| 99/100 [06:09<00:04,  4.11s/it]100%|██████████| 100/100 [06:16<00:00,  4.91s/it]100%|██████████| 100/100 [06:16<00:00,  3.77s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer4
=( -41.33% for 4
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:56,  1.81s/it]  3%|▎         | 3/100 [00:07<04:13,  2.62s/it]  4%|▍         | 4/100 [00:10<04:46,  2.98s/it]  5%|▌         | 5/100 [00:14<05:01,  3.17s/it]  6%|▌         | 6/100 [00:18<05:28,  3.49s/it]  7%|▋         | 7/100 [00:22<05:25,  3.50s/it]  8%|▊         | 8/100 [00:26<05:34,  3.63s/it]  9%|▉         | 9/100 [00:29<05:25,  3.58s/it] 10%|█         | 10/100 [00:33<05:24,  3.61s/it] 11%|█         | 11/100 [00:36<05:20,  3.60s/it] 12%|█▏        | 12/100 [00:40<05:21,  3.65s/it] 13%|█▎        | 13/100 [00:44<05:28,  3.77s/it] 14%|█▍        | 14/100 [00:48<05:26,  3.80s/it] 15%|█▌        | 15/100 [00:52<05:20,  3.77s/it] 16%|█▌        | 16/100 [00:55<05:12,  3.72s/it] 17%|█▋        | 17/100 [00:59<05:14,  3.79s/it] 18%|█▊        | 18/100 [01:03<05:10,  3.78s/it] 19%|█▉        | 19/100 [01:07<05:02,  3.73s/it] 20%|██        | 20/100 [01:10<04:54,  3.68s/it] 21%|██        | 21/100 [01:14<04:46,  3.62s/it] 22%|██▏       | 22/100 [01:17<04:41,  3.61s/it] 23%|██▎       | 23/100 [01:21<04:36,  3.59s/it] 24%|██▍       | 24/100 [01:25<04:36,  3.64s/it] 25%|██▌       | 25/100 [01:28<04:38,  3.71s/it] 26%|██▌       | 26/100 [01:32<04:29,  3.64s/it] 27%|██▋       | 27/100 [01:35<04:25,  3.63s/it] 28%|██▊       | 28/100 [01:39<04:19,  3.61s/it] 29%|██▉       | 29/100 [01:43<04:17,  3.63s/it] 30%|███       | 30/100 [01:47<04:19,  3.70s/it] 31%|███       | 31/100 [01:50<04:18,  3.74s/it] 32%|███▏      | 32/100 [01:55<04:31,  4.00s/it] 33%|███▎      | 33/100 [02:00<04:41,  4.20s/it] 34%|███▍      | 34/100 [02:04<04:42,  4.27s/it] 35%|███▌      | 35/100 [02:08<04:23,  4.05s/it] 36%|███▌      | 36/100 [02:11<04:09,  3.91s/it] 37%|███▋      | 37/100 [02:15<04:06,  3.91s/it] 38%|███▊      | 38/100 [02:19<03:59,  3.86s/it] 39%|███▉      | 39/100 [02:23<03:59,  3.92s/it] 40%|████      | 40/100 [02:27<03:50,  3.84s/it] 41%|████      | 41/100 [02:30<03:44,  3.81s/it] 42%|████▏     | 42/100 [02:35<03:47,  3.92s/it] 43%|████▎     | 43/100 [02:39<03:56,  4.16s/it] 44%|████▍     | 44/100 [02:43<03:39,  3.92s/it] 45%|████▌     | 45/100 [02:46<03:27,  3.78s/it] 46%|████▌     | 46/100 [02:50<03:23,  3.77s/it] 47%|████▋     | 47/100 [02:54<03:23,  3.84s/it] 48%|████▊     | 48/100 [02:59<03:45,  4.34s/it] 49%|████▉     | 49/100 [03:05<04:07,  4.86s/it] 50%|█████     | 50/100 [03:10<03:55,  4.71s/it] 51%|█████     | 51/100 [03:13<03:36,  4.42s/it] 52%|█████▏    | 52/100 [03:17<03:16,  4.10s/it] 53%|█████▎    | 53/100 [03:21<03:06,  3.98s/it] 54%|█████▍    | 54/100 [03:24<02:54,  3.80s/it] 55%|█████▌    | 55/100 [03:28<02:51,  3.80s/it] 56%|█████▌    | 56/100 [03:32<02:48,  3.83s/it] 57%|█████▋    | 57/100 [03:36<02:53,  4.03s/it] 58%|█████▊    | 58/100 [03:40<02:50,  4.05s/it] 59%|█████▉    | 59/100 [03:46<03:01,  4.42s/it] 60%|██████    | 60/100 [03:51<03:10,  4.75s/it] 61%|██████    | 61/100 [03:55<02:57,  4.55s/it] 62%|██████▏   | 62/100 [03:59<02:45,  4.36s/it] 63%|██████▎   | 63/100 [04:02<02:30,  4.07s/it] 64%|██████▍   | 64/100 [04:07<02:27,  4.09s/it] 65%|██████▌   | 65/100 [04:10<02:20,  4.01s/it] 66%|██████▌   | 66/100 [04:14<02:14,  3.96s/it] 67%|██████▋   | 67/100 [04:18<02:07,  3.87s/it] 68%|██████▊   | 68/100 [04:21<01:59,  3.74s/it] 69%|██████▉   | 69/100 [04:25<01:55,  3.71s/it] 70%|███████   | 70/100 [04:29<01:50,  3.67s/it] 71%|███████   | 71/100 [04:32<01:45,  3.63s/it] 72%|███████▏  | 72/100 [04:36<01:40,  3.60s/it] 73%|███████▎  | 73/100 [04:39<01:37,  3.62s/it] 74%|███████▍  | 74/100 [04:43<01:33,  3.60s/it] 75%|███████▌  | 75/100 [04:47<01:30,  3.62s/it] 76%|███████▌  | 76/100 [04:50<01:27,  3.66s/it] 77%|███████▋  | 77/100 [04:54<01:23,  3.62s/it] 78%|███████▊  | 78/100 [04:58<01:20,  3.65s/it] 79%|███████▉  | 79/100 [05:02<01:19,  3.79s/it] 80%|████████  | 80/100 [05:05<01:13,  3.68s/it] 81%|████████  | 81/100 [05:09<01:09,  3.67s/it] 82%|████████▏ | 82/100 [05:12<01:05,  3.66s/it] 83%|████████▎ | 83/100 [05:16<01:01,  3.60s/it] 84%|████████▍ | 84/100 [05:19<00:57,  3.58s/it] 85%|████████▌ | 85/100 [05:23<00:53,  3.59s/it] 86%|████████▌ | 86/100 [05:27<00:50,  3.59s/it] 87%|████████▋ | 87/100 [05:30<00:47,  3.62s/it] 88%|████████▊ | 88/100 [05:35<00:46,  3.90s/it] 89%|████████▉ | 89/100 [05:38<00:41,  3.77s/it] 90%|█████████ | 90/100 [05:42<00:37,  3.71s/it] 91%|█████████ | 91/100 [05:45<00:33,  3.67s/it] 92%|█████████▏| 92/100 [05:49<00:29,  3.67s/it] 93%|█████████▎| 93/100 [05:53<00:25,  3.68s/it] 94%|█████████▍| 94/100 [05:57<00:22,  3.72s/it] 95%|█████████▌| 95/100 [06:00<00:18,  3.67s/it] 96%|█████████▌| 96/100 [06:04<00:14,  3.64s/it] 97%|█████████▋| 97/100 [06:07<00:10,  3.57s/it] 98%|█████████▊| 98/100 [06:11<00:07,  3.56s/it] 99%|█████████▉| 99/100 [06:14<00:03,  3.56s/it]100%|██████████| 100/100 [06:18<00:00,  3.63s/it]100%|██████████| 100/100 [06:18<00:00,  3.79s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer5
=) 39.72% for 5
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:05<04:12,  2.57s/it]  3%|▎         | 3/100 [00:12<07:30,  4.65s/it]  4%|▍         | 4/100 [00:19<08:45,  5.48s/it]  5%|▌         | 5/100 [00:24<08:29,  5.36s/it]  6%|▌         | 6/100 [00:28<07:31,  4.80s/it]  7%|▋         | 7/100 [00:31<06:48,  4.39s/it]  8%|▊         | 8/100 [00:35<06:23,  4.17s/it]  9%|▉         | 9/100 [00:39<06:04,  4.00s/it] 10%|█         | 10/100 [00:42<05:46,  3.85s/it] 11%|█         | 11/100 [00:46<05:33,  3.75s/it] 12%|█▏        | 12/100 [00:49<05:22,  3.67s/it] 13%|█▎        | 13/100 [00:53<05:21,  3.70s/it] 14%|█▍        | 14/100 [00:57<05:15,  3.67s/it] 15%|█▌        | 15/100 [01:00<05:14,  3.70s/it] 16%|█▌        | 16/100 [01:04<05:11,  3.71s/it] 17%|█▋        | 17/100 [01:08<05:08,  3.72s/it] 18%|█▊        | 18/100 [01:12<05:09,  3.78s/it] 19%|█▉        | 19/100 [01:15<05:04,  3.75s/it] 20%|██        | 20/100 [01:19<05:01,  3.76s/it] 21%|██        | 21/100 [01:23<04:54,  3.73s/it] 22%|██▏       | 22/100 [01:27<04:49,  3.71s/it] 23%|██▎       | 23/100 [01:30<04:38,  3.62s/it] 24%|██▍       | 24/100 [01:34<04:38,  3.67s/it] 25%|██▌       | 25/100 [01:37<04:32,  3.63s/it] 26%|██▌       | 26/100 [01:41<04:30,  3.66s/it] 27%|██▋       | 27/100 [01:45<04:28,  3.68s/it] 28%|██▊       | 28/100 [01:48<04:24,  3.68s/it] 29%|██▉       | 29/100 [01:52<04:19,  3.65s/it] 30%|███       | 30/100 [01:56<04:20,  3.72s/it] 31%|███       | 31/100 [01:59<04:12,  3.67s/it] 32%|███▏      | 32/100 [02:03<04:10,  3.69s/it] 33%|███▎      | 33/100 [02:08<04:27,  3.99s/it] 34%|███▍      | 34/100 [02:11<04:11,  3.80s/it] 35%|███▌      | 35/100 [02:15<04:10,  3.85s/it] 36%|███▌      | 36/100 [02:20<04:25,  4.15s/it] 37%|███▋      | 37/100 [02:25<04:43,  4.50s/it] 38%|███▊      | 38/100 [02:30<04:38,  4.50s/it] 39%|███▉      | 39/100 [02:34<04:18,  4.24s/it] 40%|████      | 40/100 [02:37<04:07,  4.13s/it] 41%|████      | 41/100 [02:41<03:56,  4.00s/it] 42%|████▏     | 42/100 [02:46<04:01,  4.17s/it] 43%|████▎     | 43/100 [02:50<04:01,  4.24s/it] 44%|████▍     | 44/100 [02:53<03:43,  3.99s/it] 45%|████▌     | 45/100 [02:57<03:32,  3.87s/it] 46%|████▌     | 46/100 [03:01<03:27,  3.84s/it] 47%|████▋     | 47/100 [03:05<03:21,  3.81s/it] 48%|████▊     | 48/100 [03:08<03:13,  3.73s/it] 49%|████▉     | 49/100 [03:12<03:07,  3.67s/it] 50%|█████     | 50/100 [03:15<03:01,  3.63s/it] 51%|█████     | 51/100 [03:19<03:05,  3.78s/it] 52%|█████▏    | 52/100 [03:24<03:08,  3.93s/it] 53%|█████▎    | 53/100 [03:28<03:08,  4.00s/it] 54%|█████▍    | 54/100 [03:31<02:57,  3.87s/it] 55%|█████▌    | 55/100 [03:35<02:54,  3.88s/it] 56%|█████▌    | 56/100 [03:40<02:57,  4.03s/it] 57%|█████▋    | 57/100 [03:44<02:57,  4.13s/it] 58%|█████▊    | 58/100 [03:50<03:15,  4.66s/it] 59%|█████▉    | 59/100 [03:54<03:10,  4.66s/it] 60%|██████    | 60/100 [03:58<02:54,  4.35s/it] 61%|██████    | 61/100 [04:02<02:39,  4.10s/it] 62%|██████▏   | 62/100 [04:06<02:43,  4.30s/it] 63%|██████▎   | 63/100 [04:11<02:44,  4.45s/it] 64%|██████▍   | 64/100 [04:15<02:37,  4.37s/it] 65%|██████▌   | 65/100 [04:19<02:26,  4.19s/it] 66%|██████▌   | 66/100 [04:23<02:22,  4.19s/it] 67%|██████▋   | 67/100 [04:27<02:15,  4.10s/it] 68%|██████▊   | 68/100 [04:31<02:04,  3.88s/it] 69%|██████▉   | 69/100 [04:35<02:03,  3.99s/it] 70%|███████   | 70/100 [04:40<02:07,  4.26s/it] 71%|███████   | 71/100 [04:45<02:10,  4.48s/it] 72%|███████▏  | 72/100 [04:50<02:15,  4.83s/it] 73%|███████▎  | 73/100 [04:55<02:12,  4.90s/it] 74%|███████▍  | 74/100 [04:59<02:00,  4.64s/it] 75%|███████▌  | 75/100 [05:03<01:47,  4.31s/it] 76%|███████▌  | 76/100 [05:06<01:37,  4.06s/it] 77%|███████▋  | 77/100 [05:10<01:29,  3.90s/it] 78%|███████▊  | 78/100 [05:14<01:23,  3.80s/it] 79%|███████▉  | 79/100 [05:17<01:18,  3.74s/it] 80%|████████  | 80/100 [05:21<01:12,  3.64s/it] 81%|████████  | 81/100 [05:24<01:09,  3.65s/it] 82%|████████▏ | 82/100 [05:28<01:06,  3.68s/it] 83%|████████▎ | 83/100 [05:32<01:01,  3.62s/it] 84%|████████▍ | 84/100 [05:36<01:00,  3.79s/it] 85%|████████▌ | 85/100 [05:39<00:55,  3.71s/it] 86%|████████▌ | 86/100 [05:43<00:50,  3.61s/it] 87%|████████▋ | 87/100 [05:46<00:46,  3.57s/it] 88%|████████▊ | 88/100 [05:50<00:42,  3.58s/it] 89%|████████▉ | 89/100 [05:53<00:38,  3.54s/it] 90%|█████████ | 90/100 [05:57<00:35,  3.54s/it] 91%|█████████ | 91/100 [06:00<00:32,  3.56s/it] 92%|█████████▏| 92/100 [06:04<00:28,  3.59s/it] 93%|█████████▎| 93/100 [06:08<00:25,  3.63s/it] 94%|█████████▍| 94/100 [06:12<00:22,  3.70s/it] 95%|█████████▌| 95/100 [06:15<00:18,  3.65s/it] 96%|█████████▌| 96/100 [06:19<00:14,  3.59s/it] 97%|█████████▋| 97/100 [06:22<00:10,  3.55s/it] 98%|█████████▊| 98/100 [06:26<00:07,  3.56s/it] 99%|█████████▉| 99/100 [06:29<00:03,  3.55s/it]100%|██████████| 100/100 [06:33<00:00,  3.61s/it]100%|██████████| 100/100 [06:33<00:00,  3.94s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer6
=) 41.66% for 6
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<02:58,  1.82s/it]  3%|▎         | 3/100 [00:07<04:11,  2.60s/it]  4%|▍         | 4/100 [00:11<04:50,  3.03s/it]  5%|▌         | 5/100 [00:15<05:27,  3.45s/it]  6%|▌         | 6/100 [00:20<06:09,  3.94s/it]  7%|▋         | 7/100 [00:24<06:08,  3.97s/it]  8%|▊         | 8/100 [00:29<06:38,  4.33s/it]  9%|▉         | 9/100 [00:33<06:19,  4.18s/it] 10%|█         | 10/100 [00:36<05:56,  3.97s/it] 11%|█         | 11/100 [00:40<05:45,  3.88s/it] 12%|█▏        | 12/100 [00:45<06:03,  4.13s/it] 13%|█▎        | 13/100 [00:49<06:11,  4.27s/it] 14%|█▍        | 14/100 [00:54<06:10,  4.30s/it] 15%|█▌        | 15/100 [00:58<05:57,  4.20s/it] 16%|█▌        | 16/100 [01:01<05:42,  4.08s/it] 17%|█▋        | 17/100 [01:05<05:40,  4.10s/it] 18%|█▊        | 18/100 [01:09<05:27,  3.99s/it] 19%|█▉        | 19/100 [01:13<05:12,  3.86s/it] 20%|██        | 20/100 [01:16<05:04,  3.80s/it] 21%|██        | 21/100 [01:20<05:02,  3.83s/it] 22%|██▏       | 22/100 [01:24<05:00,  3.85s/it] 23%|██▎       | 23/100 [01:28<04:54,  3.83s/it] 24%|██▍       | 24/100 [01:32<04:48,  3.80s/it] 25%|██▌       | 25/100 [01:35<04:42,  3.77s/it] 26%|██▌       | 26/100 [01:39<04:37,  3.75s/it] 27%|██▋       | 27/100 [01:43<04:43,  3.88s/it] 28%|██▊       | 28/100 [01:47<04:34,  3.82s/it] 29%|██▉       | 29/100 [01:51<04:35,  3.88s/it] 30%|███       | 30/100 [01:55<04:28,  3.83s/it] 31%|███       | 31/100 [01:58<04:21,  3.78s/it] 32%|███▏      | 32/100 [02:03<04:25,  3.91s/it] 33%|███▎      | 33/100 [02:07<04:27,  4.00s/it] 34%|███▍      | 34/100 [02:12<04:42,  4.28s/it] 35%|███▌      | 35/100 [02:16<04:32,  4.19s/it] 36%|███▌      | 36/100 [02:19<04:13,  3.95s/it] 37%|███▋      | 37/100 [02:23<03:58,  3.79s/it] 38%|███▊      | 38/100 [02:27<04:08,  4.01s/it] 39%|███▉      | 39/100 [02:32<04:17,  4.22s/it] 40%|████      | 40/100 [02:35<04:00,  4.00s/it] 41%|████      | 41/100 [02:39<03:51,  3.92s/it] 42%|████▏     | 42/100 [02:44<03:58,  4.12s/it] 43%|████▎     | 43/100 [02:48<03:57,  4.17s/it] 44%|████▍     | 44/100 [02:51<03:42,  3.97s/it] 45%|████▌     | 45/100 [02:55<03:31,  3.84s/it] 46%|████▌     | 46/100 [02:59<03:32,  3.94s/it] 47%|████▋     | 47/100 [03:03<03:25,  3.88s/it] 48%|████▊     | 48/100 [03:06<03:16,  3.78s/it] 49%|████▉     | 49/100 [03:10<03:12,  3.77s/it] 50%|█████     | 50/100 [03:14<03:03,  3.66s/it] 51%|█████     | 51/100 [03:17<02:56,  3.59s/it] 52%|█████▏    | 52/100 [03:20<02:49,  3.54s/it] 53%|█████▎    | 53/100 [03:24<02:49,  3.60s/it] 54%|█████▍    | 54/100 [03:28<02:50,  3.71s/it] 55%|█████▌    | 55/100 [03:32<02:53,  3.85s/it] 56%|█████▌    | 56/100 [03:37<03:05,  4.21s/it] 57%|█████▋    | 57/100 [03:41<02:59,  4.17s/it] 58%|█████▊    | 58/100 [03:45<02:46,  3.96s/it] 59%|█████▉    | 59/100 [03:49<02:39,  3.89s/it] 60%|██████    | 60/100 [03:52<02:34,  3.86s/it] 61%|██████    | 61/100 [03:56<02:29,  3.82s/it] 62%|██████▏   | 62/100 [04:00<02:31,  3.98s/it] 63%|██████▎   | 63/100 [04:04<02:19,  3.78s/it] 64%|██████▍   | 64/100 [04:07<02:13,  3.72s/it] 65%|██████▌   | 65/100 [04:12<02:18,  3.96s/it] 66%|██████▌   | 66/100 [04:16<02:12,  3.90s/it] 67%|██████▋   | 67/100 [04:21<02:21,  4.28s/it] 68%|██████▊   | 68/100 [04:26<02:23,  4.49s/it] 69%|██████▉   | 69/100 [04:29<02:09,  4.17s/it] 70%|███████   | 70/100 [04:33<01:59,  3.99s/it] 71%|███████   | 71/100 [04:36<01:52,  3.88s/it] 72%|███████▏  | 72/100 [04:40<01:46,  3.79s/it] 73%|███████▎  | 73/100 [04:44<01:41,  3.75s/it] 74%|███████▍  | 74/100 [04:47<01:37,  3.74s/it] 75%|███████▌  | 75/100 [04:51<01:31,  3.65s/it] 76%|███████▌  | 76/100 [04:54<01:27,  3.63s/it] 77%|███████▋  | 77/100 [04:58<01:23,  3.65s/it] 78%|███████▊  | 78/100 [05:02<01:19,  3.63s/it] 79%|███████▉  | 79/100 [05:05<01:16,  3.62s/it] 80%|████████  | 80/100 [05:09<01:11,  3.56s/it] 81%|████████  | 81/100 [05:12<01:08,  3.59s/it] 82%|████████▏ | 82/100 [05:16<01:04,  3.60s/it] 83%|████████▎ | 83/100 [05:19<01:00,  3.55s/it] 84%|████████▍ | 84/100 [05:23<00:57,  3.58s/it] 85%|████████▌ | 85/100 [05:27<00:55,  3.67s/it] 86%|████████▌ | 86/100 [05:31<00:51,  3.68s/it] 87%|████████▋ | 87/100 [05:34<00:48,  3.71s/it] 88%|████████▊ | 88/100 [05:38<00:43,  3.66s/it] 89%|████████▉ | 89/100 [05:41<00:39,  3.59s/it] 90%|█████████ | 90/100 [05:45<00:35,  3.58s/it] 91%|█████████ | 91/100 [05:48<00:32,  3.58s/it] 92%|█████████▏| 92/100 [05:52<00:28,  3.54s/it] 93%|█████████▎| 93/100 [05:55<00:24,  3.52s/it] 94%|█████████▍| 94/100 [05:59<00:21,  3.56s/it] 95%|█████████▌| 95/100 [06:03<00:18,  3.67s/it] 96%|█████████▌| 96/100 [06:06<00:14,  3.62s/it] 97%|█████████▋| 97/100 [06:10<00:10,  3.58s/it] 98%|█████████▊| 98/100 [06:14<00:07,  3.58s/it] 99%|█████████▉| 99/100 [06:17<00:03,  3.61s/it]100%|██████████| 100/100 [06:21<00:00,  3.64s/it]100%|██████████| 100/100 [06:21<00:00,  3.82s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer7
=) 25.63% for 7
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:07,  1.91s/it]  3%|▎         | 3/100 [00:08<04:37,  2.86s/it]  4%|▍         | 4/100 [00:11<05:13,  3.27s/it]  5%|▌         | 5/100 [00:15<05:27,  3.45s/it]  6%|▌         | 6/100 [00:19<05:31,  3.53s/it]  7%|▋         | 7/100 [00:22<05:26,  3.52s/it]  8%|▊         | 8/100 [00:27<05:42,  3.73s/it]  9%|▉         | 9/100 [00:31<05:48,  3.83s/it] 10%|█         | 10/100 [00:35<06:12,  4.14s/it] 11%|█         | 11/100 [00:39<05:53,  3.98s/it] 12%|█▏        | 12/100 [00:43<05:41,  3.88s/it] 13%|█▎        | 13/100 [00:47<05:40,  3.92s/it] 14%|█▍        | 14/100 [00:51<05:44,  4.00s/it] 15%|█▌        | 15/100 [00:55<05:36,  3.96s/it] 16%|█▌        | 16/100 [00:58<05:22,  3.84s/it] 17%|█▋        | 17/100 [01:02<05:09,  3.73s/it] 18%|█▊        | 18/100 [01:05<05:01,  3.68s/it] 19%|█▉        | 19/100 [01:09<04:53,  3.62s/it] 20%|██        | 20/100 [01:12<04:47,  3.60s/it] 21%|██        | 21/100 [01:16<04:40,  3.56s/it] 22%|██▏       | 22/100 [01:20<04:37,  3.56s/it] 23%|██▎       | 23/100 [01:23<04:31,  3.53s/it] 24%|██▍       | 24/100 [01:27<04:31,  3.57s/it] 25%|██▌       | 25/100 [01:30<04:32,  3.63s/it] 26%|██▌       | 26/100 [01:34<04:34,  3.71s/it] 27%|██▋       | 27/100 [01:38<04:37,  3.80s/it] 28%|██▊       | 28/100 [01:42<04:35,  3.82s/it] 29%|██▉       | 29/100 [01:46<04:34,  3.87s/it] 30%|███       | 30/100 [01:50<04:36,  3.94s/it] 31%|███       | 31/100 [01:54<04:35,  3.99s/it] 32%|███▏      | 32/100 [01:58<04:27,  3.93s/it] 33%|███▎      | 33/100 [02:02<04:27,  3.99s/it] 34%|███▍      | 34/100 [02:06<04:11,  3.81s/it] 35%|███▌      | 35/100 [02:09<04:07,  3.81s/it] 36%|███▌      | 36/100 [02:13<04:05,  3.83s/it] 37%|███▋      | 37/100 [02:17<04:05,  3.90s/it] 38%|███▊      | 38/100 [02:21<04:00,  3.89s/it] 39%|███▉      | 39/100 [02:25<03:58,  3.90s/it] 40%|████      | 40/100 [02:29<03:55,  3.92s/it] 41%|████      | 41/100 [02:33<03:56,  4.01s/it] 42%|████▏     | 42/100 [02:38<04:03,  4.20s/it] 43%|████▎     | 43/100 [02:41<03:46,  3.97s/it] 44%|████▍     | 44/100 [02:45<03:33,  3.82s/it] 45%|████▌     | 45/100 [02:49<03:41,  4.03s/it] 46%|████▌     | 46/100 [02:55<03:56,  4.37s/it] 47%|████▋     | 47/100 [03:01<04:23,  4.98s/it] 48%|████▊     | 48/100 [03:05<04:07,  4.77s/it] 49%|████▉     | 49/100 [03:09<03:45,  4.41s/it] 50%|█████     | 50/100 [03:12<03:26,  4.12s/it] 51%|█████     | 51/100 [03:17<03:24,  4.17s/it] 52%|█████▏    | 52/100 [03:21<03:18,  4.14s/it] 53%|█████▎    | 53/100 [03:24<03:08,  4.02s/it] 54%|█████▍    | 54/100 [03:28<02:56,  3.83s/it] 55%|█████▌    | 55/100 [03:31<02:47,  3.72s/it] 56%|█████▌    | 56/100 [03:35<02:43,  3.71s/it] 57%|█████▋    | 57/100 [03:39<02:38,  3.68s/it] 58%|█████▊    | 58/100 [03:42<02:36,  3.72s/it] 59%|█████▉    | 59/100 [03:47<02:39,  3.89s/it] 60%|██████    | 60/100 [03:52<02:51,  4.30s/it] 61%|██████    | 61/100 [03:57<02:55,  4.50s/it] 62%|██████▏   | 62/100 [04:01<02:48,  4.43s/it] 63%|██████▎   | 63/100 [04:06<02:43,  4.41s/it] 64%|██████▍   | 64/100 [04:09<02:29,  4.15s/it] 65%|██████▌   | 65/100 [04:13<02:20,  4.01s/it] 66%|██████▌   | 66/100 [04:16<02:10,  3.85s/it] 67%|██████▋   | 67/100 [04:20<02:06,  3.83s/it] 68%|██████▊   | 68/100 [04:24<02:02,  3.83s/it] 69%|██████▉   | 69/100 [04:27<01:53,  3.66s/it] 70%|███████   | 70/100 [04:32<01:58,  3.94s/it] 71%|███████   | 71/100 [04:36<02:01,  4.19s/it] 72%|███████▏  | 72/100 [04:41<01:58,  4.23s/it] 73%|███████▎  | 73/100 [04:45<01:53,  4.19s/it] 74%|███████▍  | 74/100 [04:49<01:45,  4.05s/it] 75%|███████▌  | 75/100 [04:52<01:35,  3.83s/it] 76%|███████▌  | 76/100 [04:55<01:30,  3.75s/it] 77%|███████▋  | 77/100 [05:00<01:28,  3.84s/it] 78%|███████▊  | 78/100 [05:04<01:28,  4.04s/it] 79%|███████▉  | 79/100 [05:09<01:30,  4.32s/it] 80%|████████  | 80/100 [05:13<01:22,  4.10s/it] 81%|████████  | 81/100 [05:17<01:20,  4.23s/it] 82%|████████▏ | 82/100 [05:21<01:14,  4.13s/it] 83%|████████▎ | 83/100 [05:24<01:06,  3.90s/it] 84%|████████▍ | 84/100 [05:28<00:59,  3.75s/it] 85%|████████▌ | 85/100 [05:31<00:55,  3.69s/it] 86%|████████▌ | 86/100 [05:35<00:51,  3.67s/it] 87%|████████▋ | 87/100 [05:39<00:47,  3.68s/it] 88%|████████▊ | 88/100 [05:42<00:43,  3.64s/it] 89%|████████▉ | 89/100 [05:46<00:39,  3.61s/it] 90%|█████████ | 90/100 [05:50<00:37,  3.73s/it] 91%|█████████ | 91/100 [05:53<00:33,  3.68s/it] 92%|█████████▏| 92/100 [05:57<00:28,  3.60s/it] 93%|█████████▎| 93/100 [06:00<00:24,  3.55s/it] 94%|█████████▍| 94/100 [06:04<00:21,  3.58s/it] 95%|█████████▌| 95/100 [06:07<00:17,  3.57s/it] 96%|█████████▌| 96/100 [06:11<00:14,  3.52s/it] 97%|█████████▋| 97/100 [06:14<00:10,  3.51s/it] 98%|█████████▊| 98/100 [06:18<00:07,  3.58s/it] 99%|█████████▉| 99/100 [06:22<00:03,  3.67s/it]100%|██████████| 100/100 [06:26<00:00,  3.69s/it]100%|██████████| 100/100 [06:26<00:00,  3.86s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer8
=) 16.94% for 8
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:04<03:45,  2.31s/it]  3%|▎         | 3/100 [00:09<05:08,  3.18s/it]  4%|▍         | 4/100 [00:12<05:25,  3.39s/it]  5%|▌         | 5/100 [00:16<05:36,  3.54s/it]  6%|▌         | 6/100 [00:20<05:43,  3.66s/it]  7%|▋         | 7/100 [00:24<05:38,  3.63s/it]  8%|▊         | 8/100 [00:27<05:42,  3.73s/it]  9%|▉         | 9/100 [00:31<05:32,  3.66s/it] 10%|█         | 10/100 [00:35<05:26,  3.63s/it] 11%|█         | 11/100 [00:38<05:23,  3.63s/it] 12%|█▏        | 12/100 [00:42<05:16,  3.60s/it] 13%|█▎        | 13/100 [00:46<05:34,  3.84s/it] 14%|█▍        | 14/100 [00:50<05:25,  3.79s/it] 15%|█▌        | 15/100 [00:53<05:12,  3.67s/it] 16%|█▌        | 16/100 [00:57<05:11,  3.71s/it] 17%|█▋        | 17/100 [01:01<05:07,  3.70s/it] 18%|█▊        | 18/100 [01:04<05:00,  3.66s/it] 19%|█▉        | 19/100 [01:09<05:14,  3.88s/it] 20%|██        | 20/100 [01:12<05:01,  3.77s/it] 21%|██        | 21/100 [01:16<04:51,  3.69s/it] 22%|██▏       | 22/100 [01:20<05:02,  3.88s/it] 23%|██▎       | 23/100 [01:25<05:27,  4.26s/it] 24%|██▍       | 24/100 [01:31<06:09,  4.86s/it] 25%|██▌       | 25/100 [01:35<05:38,  4.51s/it] 26%|██▌       | 26/100 [01:39<05:11,  4.22s/it] 27%|██▋       | 27/100 [01:42<04:54,  4.03s/it] 28%|██▊       | 28/100 [01:46<04:44,  3.95s/it] 29%|██▉       | 29/100 [01:50<04:39,  3.93s/it] 30%|███       | 30/100 [01:54<04:37,  3.97s/it] 31%|███       | 31/100 [01:58<04:27,  3.88s/it] 32%|███▏      | 32/100 [02:01<04:23,  3.88s/it] 33%|███▎      | 33/100 [02:07<04:45,  4.27s/it] 34%|███▍      | 34/100 [02:11<04:44,  4.31s/it] 35%|███▌      | 35/100 [02:15<04:23,  4.06s/it] 36%|███▌      | 36/100 [02:18<04:11,  3.92s/it] 37%|███▋      | 37/100 [02:22<04:00,  3.82s/it] 38%|███▊      | 38/100 [02:25<03:55,  3.81s/it] 39%|███▉      | 39/100 [02:29<03:54,  3.84s/it] 40%|████      | 40/100 [02:33<03:52,  3.88s/it] 41%|████      | 41/100 [02:37<03:40,  3.74s/it] 42%|████▏     | 42/100 [02:41<03:51,  4.00s/it] 43%|████▎     | 43/100 [02:47<04:11,  4.42s/it] 44%|████▍     | 44/100 [02:52<04:20,  4.65s/it] 45%|████▌     | 45/100 [02:58<04:46,  5.21s/it] 46%|████▌     | 46/100 [03:05<05:00,  5.57s/it] 47%|████▋     | 47/100 [03:10<04:45,  5.38s/it] 48%|████▊     | 48/100 [03:14<04:15,  4.91s/it] 49%|████▉     | 49/100 [03:18<04:01,  4.74s/it] 50%|█████     | 50/100 [03:22<03:50,  4.60s/it] 51%|█████     | 51/100 [03:26<03:37,  4.44s/it] 52%|█████▏    | 52/100 [03:30<03:18,  4.14s/it] 53%|█████▎    | 53/100 [03:34<03:13,  4.12s/it] 54%|█████▍    | 54/100 [03:38<03:10,  4.15s/it] 55%|█████▌    | 55/100 [03:42<03:08,  4.20s/it] 56%|█████▌    | 56/100 [03:46<02:56,  4.02s/it] 57%|█████▋    | 57/100 [03:50<02:46,  3.88s/it] 58%|█████▊    | 58/100 [03:53<02:38,  3.78s/it] 59%|█████▉    | 59/100 [03:57<02:37,  3.83s/it] 60%|██████    | 60/100 [04:01<02:37,  3.93s/it] 61%|██████    | 61/100 [04:05<02:26,  3.75s/it] 62%|██████▏   | 62/100 [04:08<02:19,  3.68s/it] 63%|██████▎   | 63/100 [04:12<02:16,  3.68s/it] 64%|██████▍   | 64/100 [04:16<02:14,  3.74s/it] 65%|██████▌   | 65/100 [04:20<02:16,  3.91s/it] 66%|██████▌   | 66/100 [04:23<02:09,  3.82s/it] 67%|██████▋   | 67/100 [04:28<02:10,  3.95s/it] 68%|██████▊   | 68/100 [04:31<02:00,  3.77s/it] 69%|██████▉   | 69/100 [04:34<01:52,  3.63s/it] 70%|███████   | 70/100 [04:39<01:56,  3.89s/it] 71%|███████   | 71/100 [04:43<01:55,  3.99s/it] 72%|███████▏  | 72/100 [04:48<02:00,  4.30s/it] 73%|███████▎  | 73/100 [04:52<01:55,  4.26s/it] 74%|███████▍  | 74/100 [04:56<01:44,  4.02s/it] 75%|███████▌  | 75/100 [04:59<01:35,  3.81s/it] 76%|███████▌  | 76/100 [05:03<01:30,  3.78s/it] 77%|███████▋  | 77/100 [05:07<01:26,  3.78s/it] 78%|███████▊  | 78/100 [05:12<01:33,  4.23s/it] 79%|███████▉  | 79/100 [05:16<01:30,  4.32s/it] 80%|████████  | 80/100 [05:20<01:21,  4.09s/it] 81%|████████  | 81/100 [05:24<01:15,  3.95s/it] 82%|████████▏ | 82/100 [05:27<01:10,  3.91s/it] 83%|████████▎ | 83/100 [05:31<01:03,  3.76s/it] 84%|████████▍ | 84/100 [05:34<00:58,  3.66s/it] 85%|████████▌ | 85/100 [05:38<00:55,  3.67s/it] 86%|████████▌ | 86/100 [05:41<00:50,  3.61s/it] 87%|████████▋ | 87/100 [05:45<00:47,  3.63s/it] 88%|████████▊ | 88/100 [05:50<00:46,  3.90s/it] 89%|████████▉ | 89/100 [05:53<00:42,  3.86s/it] 90%|█████████ | 90/100 [05:57<00:38,  3.81s/it] 91%|█████████ | 91/100 [06:01<00:35,  3.99s/it] 92%|█████████▏| 92/100 [06:06<00:32,  4.00s/it] 93%|█████████▎| 93/100 [06:09<00:27,  3.89s/it] 94%|█████████▍| 94/100 [06:13<00:23,  3.93s/it] 95%|█████████▌| 95/100 [06:18<00:21,  4.21s/it] 96%|█████████▌| 96/100 [06:22<00:17,  4.26s/it] 97%|█████████▋| 97/100 [06:26<00:11,  3.98s/it] 98%|█████████▊| 98/100 [06:30<00:07,  3.96s/it] 99%|█████████▉| 99/100 [06:34<00:04,  4.05s/it]100%|██████████| 100/100 [06:38<00:00,  4.07s/it]100%|██████████| 100/100 [06:38<00:00,  3.99s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer9
=( -1.83% for 9
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:04<04:00,  2.46s/it]  3%|▎         | 3/100 [00:12<07:09,  4.43s/it]  4%|▍         | 4/100 [00:18<08:26,  5.28s/it]  5%|▌         | 5/100 [00:24<08:29,  5.37s/it]  6%|▌         | 6/100 [00:27<07:28,  4.78s/it]  7%|▋         | 7/100 [00:31<06:46,  4.37s/it]  8%|▊         | 8/100 [00:35<06:23,  4.16s/it]  9%|▉         | 9/100 [00:38<06:00,  3.96s/it] 10%|█         | 10/100 [00:42<05:46,  3.86s/it] 11%|█         | 11/100 [00:45<05:33,  3.75s/it] 12%|█▏        | 12/100 [00:49<05:25,  3.70s/it] 13%|█▎        | 13/100 [00:54<05:59,  4.13s/it] 14%|█▍        | 14/100 [00:58<05:42,  3.99s/it] 15%|█▌        | 15/100 [01:01<05:27,  3.85s/it] 16%|█▌        | 16/100 [01:06<05:43,  4.09s/it] 17%|█▋        | 17/100 [01:10<05:43,  4.14s/it] 18%|█▊        | 18/100 [01:14<05:33,  4.07s/it] 19%|█▉        | 19/100 [01:18<05:16,  3.90s/it] 20%|██        | 20/100 [01:21<05:03,  3.79s/it] 21%|██        | 21/100 [01:25<04:55,  3.74s/it] 22%|██▏       | 22/100 [01:28<04:49,  3.72s/it] 23%|██▎       | 23/100 [01:32<04:47,  3.74s/it] 24%|██▍       | 24/100 [01:36<04:39,  3.68s/it] 25%|██▌       | 25/100 [01:39<04:32,  3.63s/it] 26%|██▌       | 26/100 [01:43<04:24,  3.57s/it] 27%|██▋       | 27/100 [01:46<04:22,  3.59s/it] 28%|██▊       | 28/100 [01:50<04:17,  3.58s/it] 29%|██▉       | 29/100 [01:54<04:19,  3.66s/it] 30%|███       | 30/100 [01:58<04:30,  3.86s/it] 31%|███       | 31/100 [02:02<04:26,  3.87s/it] 32%|███▏      | 32/100 [02:06<04:25,  3.91s/it] 33%|███▎      | 33/100 [02:10<04:25,  3.96s/it] 34%|███▍      | 34/100 [02:13<04:10,  3.79s/it] 35%|███▌      | 35/100 [02:17<04:00,  3.70s/it] 36%|███▌      | 36/100 [02:21<03:57,  3.71s/it] 37%|███▋      | 37/100 [02:25<04:02,  3.85s/it] 38%|███▊      | 38/100 [02:29<04:14,  4.10s/it] 39%|███▉      | 39/100 [02:35<04:36,  4.52s/it] 40%|████      | 40/100 [02:40<04:33,  4.57s/it] 41%|████      | 41/100 [02:43<04:06,  4.18s/it] 42%|████▏     | 42/100 [02:46<03:50,  3.98s/it] 43%|████▎     | 43/100 [02:50<03:44,  3.94s/it] 44%|████▍     | 44/100 [02:55<04:00,  4.29s/it] 45%|████▌     | 45/100 [03:01<04:21,  4.76s/it] 46%|████▌     | 46/100 [03:06<04:21,  4.85s/it] 47%|████▋     | 47/100 [03:10<04:01,  4.55s/it] 48%|████▊     | 48/100 [03:14<03:46,  4.37s/it] 49%|████▉     | 49/100 [03:18<03:36,  4.24s/it] 50%|█████     | 50/100 [03:22<03:29,  4.18s/it] 51%|█████     | 51/100 [03:26<03:24,  4.17s/it] 52%|█████▏    | 52/100 [03:30<03:14,  4.05s/it] 53%|█████▎    | 53/100 [03:35<03:22,  4.30s/it] 54%|█████▍    | 54/100 [03:39<03:20,  4.36s/it] 55%|█████▌    | 55/100 [03:43<03:03,  4.08s/it] 56%|█████▌    | 56/100 [03:46<02:52,  3.93s/it] 57%|█████▋    | 57/100 [03:50<02:44,  3.82s/it] 58%|█████▊    | 58/100 [03:54<02:40,  3.81s/it] 59%|█████▉    | 59/100 [03:57<02:35,  3.78s/it] 60%|██████    | 60/100 [04:01<02:32,  3.81s/it] 61%|██████    | 61/100 [04:05<02:23,  3.68s/it] 62%|██████▏   | 62/100 [04:08<02:20,  3.69s/it] 63%|██████▎   | 63/100 [04:13<02:26,  3.95s/it] 64%|██████▍   | 64/100 [04:16<02:17,  3.83s/it] 65%|██████▌   | 65/100 [04:20<02:13,  3.80s/it] 66%|██████▌   | 66/100 [04:24<02:13,  3.93s/it] 67%|██████▋   | 67/100 [04:29<02:17,  4.15s/it] 68%|██████▊   | 68/100 [04:32<02:05,  3.91s/it] 69%|██████▉   | 69/100 [04:36<01:56,  3.74s/it] 70%|███████   | 70/100 [04:40<01:55,  3.84s/it] 71%|███████   | 71/100 [04:44<01:55,  3.99s/it] 72%|███████▏  | 72/100 [04:48<01:49,  3.90s/it] 73%|███████▎  | 73/100 [04:52<01:44,  3.85s/it] 74%|███████▍  | 74/100 [04:56<01:42,  3.93s/it] 75%|███████▌  | 75/100 [05:00<01:41,  4.05s/it] 76%|███████▌  | 76/100 [05:05<01:42,  4.25s/it] 77%|███████▋  | 77/100 [05:09<01:34,  4.11s/it] 78%|███████▊  | 78/100 [05:12<01:26,  3.94s/it] 79%|███████▉  | 79/100 [05:16<01:23,  3.96s/it] 80%|████████  | 80/100 [05:20<01:18,  3.94s/it] 81%|████████  | 81/100 [05:24<01:17,  4.08s/it] 82%|████████▏ | 82/100 [05:28<01:12,  4.00s/it] 83%|████████▎ | 83/100 [05:32<01:05,  3.84s/it] 84%|████████▍ | 84/100 [05:35<00:59,  3.72s/it] 85%|████████▌ | 85/100 [05:39<00:56,  3.77s/it] 86%|████████▌ | 86/100 [05:43<00:53,  3.84s/it] 87%|████████▋ | 87/100 [05:47<00:50,  3.90s/it] 88%|████████▊ | 88/100 [05:52<00:50,  4.18s/it] 89%|████████▉ | 89/100 [05:55<00:43,  3.98s/it] 90%|█████████ | 90/100 [05:59<00:38,  3.87s/it] 91%|█████████ | 91/100 [06:03<00:36,  4.03s/it] 92%|█████████▏| 92/100 [06:07<00:31,  3.93s/it] 93%|█████████▎| 93/100 [06:11<00:27,  3.87s/it] 94%|█████████▍| 94/100 [06:15<00:23,  3.88s/it] 95%|█████████▌| 95/100 [06:18<00:18,  3.78s/it] 96%|█████████▌| 96/100 [06:22<00:14,  3.72s/it] 97%|█████████▋| 97/100 [06:25<00:10,  3.64s/it] 98%|█████████▊| 98/100 [06:29<00:07,  3.65s/it] 99%|█████████▉| 99/100 [06:33<00:03,  3.61s/it]100%|██████████| 100/100 [06:36<00:00,  3.66s/it]100%|██████████| 100/100 [06:37<00:00,  3.97s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer10
=) 21.52% for 10
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:05<04:11,  2.56s/it]  3%|▎         | 3/100 [00:10<05:50,  3.62s/it]  4%|▍         | 4/100 [00:14<06:08,  3.84s/it]  5%|▌         | 5/100 [00:18<06:06,  3.85s/it]  6%|▌         | 6/100 [00:22<05:57,  3.81s/it]  7%|▋         | 7/100 [00:25<05:43,  3.69s/it]  8%|▊         | 8/100 [00:29<05:38,  3.68s/it]  9%|▉         | 9/100 [00:32<05:30,  3.63s/it] 10%|█         | 10/100 [00:36<05:24,  3.60s/it] 11%|█         | 11/100 [00:39<05:21,  3.61s/it] 12%|█▏        | 12/100 [00:43<05:17,  3.61s/it] 13%|█▎        | 13/100 [00:47<05:18,  3.66s/it] 14%|█▍        | 14/100 [00:50<05:12,  3.64s/it] 15%|█▌        | 15/100 [00:54<05:04,  3.58s/it] 16%|█▌        | 16/100 [00:58<05:09,  3.69s/it] 17%|█▋        | 17/100 [01:01<05:06,  3.69s/it] 18%|█▊        | 18/100 [01:05<05:01,  3.67s/it] 19%|█▉        | 19/100 [01:08<04:52,  3.61s/it] 20%|██        | 20/100 [01:12<04:46,  3.58s/it] 21%|██        | 21/100 [01:16<04:44,  3.61s/it] 22%|██▏       | 22/100 [01:19<04:42,  3.62s/it] 23%|██▎       | 23/100 [01:23<04:34,  3.56s/it] 24%|██▍       | 24/100 [01:26<04:28,  3.53s/it] 25%|██▌       | 25/100 [01:30<04:24,  3.52s/it] 26%|██▌       | 26/100 [01:33<04:23,  3.56s/it] 27%|██▋       | 27/100 [01:37<04:25,  3.63s/it] 28%|██▊       | 28/100 [01:41<04:20,  3.62s/it] 29%|██▉       | 29/100 [01:44<04:19,  3.66s/it] 30%|███       | 30/100 [01:49<04:32,  3.90s/it] 31%|███       | 31/100 [01:53<04:26,  3.86s/it] 32%|███▏      | 32/100 [01:57<04:23,  3.88s/it] 33%|███▎      | 33/100 [02:01<04:22,  3.92s/it] 34%|███▍      | 34/100 [02:04<04:13,  3.84s/it] 35%|███▌      | 35/100 [02:08<04:07,  3.81s/it] 36%|███▌      | 36/100 [02:12<04:02,  3.79s/it] 37%|███▋      | 37/100 [02:15<03:52,  3.70s/it] 38%|███▊      | 38/100 [02:20<04:16,  4.13s/it] 39%|███▉      | 39/100 [02:27<04:54,  4.82s/it] 40%|████      | 40/100 [02:32<05:00,  5.00s/it] 41%|████      | 41/100 [02:36<04:27,  4.53s/it] 42%|████▏     | 42/100 [02:39<04:04,  4.21s/it] 43%|████▎     | 43/100 [02:43<03:54,  4.12s/it] 44%|████▍     | 44/100 [02:47<03:40,  3.93s/it] 45%|████▌     | 45/100 [02:50<03:30,  3.83s/it] 46%|████▌     | 46/100 [02:54<03:30,  3.90s/it] 47%|████▋     | 47/100 [02:58<03:24,  3.86s/it] 48%|████▊     | 48/100 [03:01<03:14,  3.74s/it] 49%|████▉     | 49/100 [03:06<03:16,  3.85s/it] 50%|█████     | 50/100 [03:09<03:10,  3.82s/it] 51%|█████     | 51/100 [03:13<03:00,  3.68s/it] 52%|█████▏    | 52/100 [03:16<02:58,  3.72s/it] 53%|█████▎    | 53/100 [03:21<03:01,  3.86s/it] 54%|█████▍    | 54/100 [03:24<02:55,  3.81s/it] 55%|█████▌    | 55/100 [03:28<02:49,  3.77s/it] 56%|█████▌    | 56/100 [03:32<02:43,  3.71s/it] 57%|█████▋    | 57/100 [03:35<02:38,  3.68s/it] 58%|█████▊    | 58/100 [03:39<02:32,  3.62s/it] 59%|█████▉    | 59/100 [03:42<02:30,  3.67s/it] 60%|██████    | 60/100 [03:46<02:27,  3.68s/it] 61%|██████    | 61/100 [03:50<02:19,  3.58s/it] 62%|██████▏   | 62/100 [03:53<02:18,  3.64s/it] 63%|██████▎   | 63/100 [03:57<02:12,  3.57s/it] 64%|██████▍   | 64/100 [04:01<02:12,  3.68s/it] 65%|██████▌   | 65/100 [04:04<02:09,  3.71s/it] 66%|██████▌   | 66/100 [04:08<02:04,  3.65s/it] 67%|██████▋   | 67/100 [04:12<02:02,  3.70s/it] 68%|██████▊   | 68/100 [04:16<02:05,  3.92s/it] 69%|██████▉   | 69/100 [04:21<02:07,  4.12s/it] 70%|███████   | 70/100 [04:27<02:19,  4.64s/it] 71%|███████   | 71/100 [04:31<02:11,  4.52s/it] 72%|███████▏  | 72/100 [04:35<02:04,  4.46s/it] 73%|███████▎  | 73/100 [04:39<01:58,  4.39s/it] 74%|███████▍  | 74/100 [04:44<01:52,  4.32s/it] 75%|███████▌  | 75/100 [04:47<01:40,  4.00s/it] 76%|███████▌  | 76/100 [04:52<01:42,  4.27s/it] 77%|███████▋  | 77/100 [04:58<01:48,  4.74s/it] 78%|███████▊  | 78/100 [05:02<01:44,  4.73s/it] 79%|███████▉  | 79/100 [05:07<01:37,  4.66s/it] 80%|████████  | 80/100 [05:10<01:27,  4.36s/it] 81%|████████  | 81/100 [05:14<01:18,  4.15s/it] 82%|████████▏ | 82/100 [05:18<01:11,  3.99s/it] 83%|████████▎ | 83/100 [05:21<01:05,  3.84s/it] 84%|████████▍ | 84/100 [05:25<00:59,  3.73s/it] 85%|████████▌ | 85/100 [05:28<00:56,  3.74s/it] 86%|████████▌ | 86/100 [05:32<00:51,  3.66s/it] 87%|████████▋ | 87/100 [05:36<00:47,  3.68s/it] 88%|████████▊ | 88/100 [05:39<00:43,  3.64s/it] 89%|████████▉ | 89/100 [05:43<00:39,  3.64s/it] 90%|█████████ | 90/100 [05:46<00:36,  3.61s/it] 91%|█████████ | 91/100 [05:50<00:32,  3.63s/it] 92%|█████████▏| 92/100 [05:54<00:28,  3.61s/it] 93%|█████████▎| 93/100 [05:57<00:24,  3.56s/it] 94%|█████████▍| 94/100 [06:01<00:21,  3.60s/it] 95%|█████████▌| 95/100 [06:04<00:18,  3.62s/it] 96%|█████████▌| 96/100 [06:08<00:14,  3.64s/it] 97%|█████████▋| 97/100 [06:12<00:10,  3.58s/it] 98%|█████████▊| 98/100 [06:15<00:07,  3.58s/it] 99%|█████████▉| 99/100 [06:19<00:03,  3.57s/it]100%|██████████| 100/100 [06:22<00:00,  3.63s/it]100%|██████████| 100/100 [06:23<00:00,  3.83s/it]
/home/nlyaly/env/optimum-fresh/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
process layer11
=( -118.08% for 11
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:01,  1.85s/it]  3%|▎         | 3/100 [00:07<04:25,  2.74s/it]  4%|▍         | 4/100 [00:11<05:05,  3.18s/it]  5%|▌         | 5/100 [00:15<05:12,  3.29s/it]  6%|▌         | 6/100 [00:18<05:22,  3.43s/it]  7%|▋         | 7/100 [00:22<05:27,  3.52s/it]  8%|▊         | 8/100 [00:26<05:42,  3.73s/it]  9%|▉         | 9/100 [00:30<05:50,  3.85s/it] 10%|█         | 10/100 [00:34<05:48,  3.87s/it] 11%|█         | 11/100 [00:38<05:37,  3.79s/it] 12%|█▏        | 12/100 [00:42<05:29,  3.75s/it] 13%|█▎        | 13/100 [00:45<05:26,  3.75s/it] 14%|█▍        | 14/100 [00:49<05:19,  3.71s/it] 15%|█▌        | 15/100 [00:52<05:09,  3.64s/it] 16%|█▌        | 16/100 [00:56<05:10,  3.69s/it] 17%|█▋        | 17/100 [01:00<05:13,  3.78s/it] 18%|█▊        | 18/100 [01:05<05:36,  4.10s/it] 19%|█▉        | 19/100 [01:10<05:53,  4.36s/it] 20%|██        | 20/100 [01:16<06:21,  4.76s/it] 21%|██        | 21/100 [01:19<05:47,  4.40s/it] 22%|██▏       | 22/100 [01:23<05:22,  4.14s/it] 23%|██▎       | 23/100 [01:26<05:01,  3.91s/it] 24%|██▍       | 24/100 [01:30<04:47,  3.78s/it] 25%|██▌       | 25/100 [01:33<04:36,  3.69s/it] 26%|██▌       | 26/100 [01:37<04:30,  3.65s/it] 27%|██▋       | 27/100 [01:40<04:25,  3.64s/it] 28%|██▊       | 28/100 [01:44<04:20,  3.62s/it] 29%|██▉       | 29/100 [01:49<04:44,  4.01s/it] 30%|███       | 30/100 [01:55<05:21,  4.59s/it] 31%|███       | 31/100 [01:59<05:07,  4.45s/it] 32%|███▏      | 32/100 [02:03<04:50,  4.27s/it] 33%|███▎      | 33/100 [02:07<04:40,  4.19s/it] 34%|███▍      | 34/100 [02:10<04:25,  4.02s/it] 35%|███▌      | 35/100 [02:14<04:16,  3.95s/it] 36%|███▌      | 36/100 [02:18<04:10,  3.92s/it] 37%|███▋      | 37/100 [02:22<04:12,  4.01s/it] 38%|███▊      | 38/100 [02:27<04:26,  4.29s/it] 39%|███▉      | 39/100 [02:32<04:34,  4.51s/it] 40%|████      | 40/100 [02:37<04:39,  4.66s/it] 41%|████      | 41/100 [02:40<04:08,  4.21s/it] 42%|████▏     | 42/100 [02:44<03:55,  4.05s/it] 43%|████▎     | 43/100 [02:49<04:03,  4.27s/it] 44%|████▍     | 44/100 [02:53<03:54,  4.18s/it] 45%|████▌     | 45/100 [02:57<03:45,  4.11s/it] 46%|████▌     | 46/100 [03:02<04:03,  4.51s/it] 47%|████▋     | 47/100 [03:08<04:14,  4.79s/it] 48%|████▊     | 48/100 [03:12<03:58,  4.59s/it] 49%|████▉     | 49/100 [03:15<03:40,  4.32s/it] 50%|█████     | 50/100 [03:19<03:24,  4.09s/it] 51%|█████     | 51/100 [03:23<03:21,  4.12s/it] 52%|█████▏    | 52/100 [03:27<03:16,  4.09s/it] 53%|█████▎    | 53/100 [03:31<03:07,  4.00s/it] 54%|█████▍    | 54/100 [03:34<02:55,  3.82s/it] 55%|█████▌    | 55/100 [03:38<02:47,  3.71s/it] 56%|█████▌    | 56/100 [03:42<02:43,  3.73s/it] 57%|█████▋    | 57/100 [03:45<02:37,  3.66s/it] 58%|█████▊    | 58/100 [03:49<02:31,  3.61s/it] 59%|█████▉    | 59/100 [03:52<02:28,  3.63s/it] 60%|██████    | 60/100 [03:56<02:27,  3.68s/it] 61%|██████    | 61/100 [03:59<02:20,  3.59s/it] 62%|██████▏   | 62/100 [04:03<02:19,  3.67s/it] 63%|██████▎   | 63/100 [04:07<02:19,  3.78s/it] 64%|██████▍   | 64/100 [04:11<02:12,  3.68s/it] 65%|██████▌   | 65/100 [04:14<02:08,  3.67s/it] 66%|██████▌   | 66/100 [04:18<02:06,  3.72s/it] 67%|██████▋   | 67/100 [04:23<02:09,  3.93s/it] 68%|██████▊   | 68/100 [04:27<02:08,  4.03s/it] 69%|██████▉   | 69/100 [04:31<02:06,  4.09s/it] 70%|███████   | 70/100 [04:36<02:09,  4.33s/it] 71%|███████   | 71/100 [04:41<02:14,  4.63s/it] 72%|███████▏  | 72/100 [04:47<02:15,  4.85s/it] 73%|███████▎  | 73/100 [04:51<02:10,  4.83s/it] 74%|███████▍  | 74/100 [04:55<01:54,  4.41s/it] 75%|███████▌  | 75/100 [04:59<01:44,  4.17s/it] 76%|███████▌  | 76/100 [05:03<01:38,  4.12s/it] 77%|███████▋  | 77/100 [05:07<01:38,  4.30s/it] 78%|███████▊  | 78/100 [05:12<01:37,  4.43s/it] 79%|███████▉  | 79/100 [05:17<01:36,  4.59s/it] 80%|████████  | 80/100 [05:20<01:25,  4.25s/it] 81%|████████  | 81/100 [05:24<01:17,  4.07s/it] 82%|████████▏ | 82/100 [05:28<01:10,  3.94s/it] 83%|████████▎ | 83/100 [05:31<01:05,  3.83s/it] 84%|████████▍ | 84/100 [05:35<01:01,  3.87s/it] 85%|████████▌ | 85/100 [05:39<00:59,  3.94s/it] 86%|████████▌ | 86/100 [05:43<00:55,  3.94s/it] 87%|████████▋ | 87/100 [05:47<00:49,  3.80s/it] 88%|████████▊ | 88/100 [05:50<00:44,  3.71s/it] 89%|████████▉ | 89/100 [05:54<00:40,  3.70s/it] 90%|█████████ | 90/100 [05:58<00:39,  3.94s/it] 91%|█████████ | 91/100 [06:02<00:34,  3.84s/it] 92%|█████████▏| 92/100 [06:05<00:29,  3.72s/it] 93%|█████████▎| 93/100 [06:09<00:25,  3.66s/it] 94%|█████████▍| 94/100 [06:13<00:22,  3.68s/it] 95%|█████████▌| 95/100 [06:16<00:18,  3.65s/it] 96%|█████████▌| 96/100 [06:20<00:14,  3.67s/it] 97%|█████████▋| 97/100 [06:24<00:10,  3.66s/it] 98%|█████████▊| 98/100 [06:27<00:07,  3.71s/it] 99%|█████████▉| 99/100 [06:31<00:03,  3.71s/it]100%|██████████| 100/100 [06:35<00:00,  3.75s/it]100%|██████████| 100/100 [06:35<00:00,  3.96s/it]
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:03<03:08,  1.93s/it]  3%|▎         | 3/100 [00:08<05:02,  3.12s/it]  4%|▍         | 4/100 [00:13<06:03,  3.78s/it]  5%|▌         | 5/100 [00:18<06:44,  4.26s/it]  6%|▌         | 6/100 [00:24<07:18,  4.66s/it]  7%|▋         | 7/100 [00:28<06:54,  4.46s/it]  8%|▊         | 8/100 [00:32<06:42,  4.38s/it]  9%|▉         | 9/100 [00:36<06:24,  4.22s/it] 10%|█         | 10/100 [00:39<05:58,  3.98s/it] 11%|█         | 11/100 [00:43<05:45,  3.88s/it] 12%|█▏        | 12/100 [00:47<05:49,  3.97s/it] 13%|█▎        | 13/100 [00:51<05:45,  3.97s/it] 14%|█▍        | 14/100 [00:55<05:40,  3.96s/it] 15%|█▌        | 15/100 [00:59<05:30,  3.89s/it] 16%|█▌        | 16/100 [01:03<05:26,  3.89s/it] 17%|█▋        | 17/100 [01:07<05:27,  3.95s/it] 18%|█▊        | 18/100 [01:12<05:58,  4.37s/it] 19%|█▉        | 19/100 [01:16<05:42,  4.23s/it] 20%|██        | 20/100 [01:19<05:19,  4.00s/it] 21%|██        | 21/100 [01:23<05:08,  3.91s/it] 22%|██▏       | 22/100 [01:27<05:10,  3.99s/it] 23%|██▎       | 23/100 [01:31<05:07,  3.99s/it] 24%|██▍       | 24/100 [01:35<05:00,  3.96s/it] 25%|██▌       | 25/100 [01:39<04:46,  3.82s/it] 26%|██▌       | 26/100 [01:42<04:36,  3.73s/it] 27%|██▋       | 27/100 [01:46<04:34,  3.77s/it] 28%|██▊       | 28/100 [01:51<04:59,  4.16s/it] 29%|██▉       | 29/100 [01:57<05:26,  4.60s/it] 30%|███       | 30/100 [02:03<05:52,  5.04s/it] 31%|███       | 31/100 [02:07<05:33,  4.83s/it] 32%|███▏      | 32/100 [02:11<05:07,  4.52s/it] 33%|███▎      | 33/100 [02:15<04:49,  4.31s/it] 34%|███▍      | 34/100 [02:18<04:28,  4.06s/it] 35%|███▌      | 35/100 [02:22<04:23,  4.06s/it] 36%|███▌      | 36/100 [02:26<04:11,  3.93s/it] 37%|███▋      | 37/100 [02:30<04:08,  3.94s/it] 38%|███▊      | 38/100 [02:36<04:41,  4.55s/it] 39%|███▉      | 39/100 [02:41<04:41,  4.62s/it] 40%|████      | 40/100 [02:46<04:50,  4.84s/it] 41%|████      | 41/100 [02:51<04:48,  4.88s/it] 42%|████▏     | 42/100 [02:56<04:47,  4.95s/it] 43%|████▎     | 43/100 [03:00<04:33,  4.79s/it] 44%|████▍     | 44/100 [03:04<04:04,  4.37s/it] 45%|████▌     | 45/100 [03:07<03:45,  4.09s/it] 46%|████▌     | 46/100 [03:11<03:39,  4.06s/it] 47%|████▋     | 47/100 [03:15<03:31,  3.98s/it] 48%|████▊     | 48/100 [03:19<03:22,  3.90s/it] 49%|████▉     | 49/100 [03:22<03:12,  3.77s/it] 50%|█████     | 50/100 [03:26<03:05,  3.71s/it] 51%|█████     | 51/100 [03:29<02:58,  3.64s/it] 52%|█████▏    | 52/100 [03:33<02:53,  3.61s/it] 53%|█████▎    | 53/100 [03:37<02:52,  3.67s/it] 54%|█████▍    | 54/100 [03:40<02:45,  3.59s/it] 55%|█████▌    | 55/100 [03:44<02:40,  3.56s/it] 56%|█████▌    | 56/100 [03:47<02:38,  3.60s/it] 57%|█████▋    | 57/100 [03:51<02:34,  3.59s/it] 58%|█████▊    | 58/100 [03:54<02:31,  3.61s/it] 59%|█████▉    | 59/100 [03:59<02:40,  3.93s/it] 60%|██████    | 60/100 [04:04<02:43,  4.09s/it] 61%|██████    | 61/100 [04:08<02:46,  4.26s/it] 62%|██████▏   | 62/100 [04:13<02:43,  4.30s/it] 63%|██████▎   | 63/100 [04:16<02:30,  4.07s/it] 64%|██████▍   | 64/100 [04:20<02:20,  3.91s/it] 65%|██████▌   | 65/100 [04:23<02:14,  3.83s/it] 66%|██████▌   | 66/100 [04:27<02:12,  3.89s/it] 67%|██████▋   | 67/100 [04:32<02:12,  4.02s/it] 68%|██████▊   | 68/100 [04:36<02:09,  4.06s/it] 69%|██████▉   | 69/100 [04:39<01:59,  3.84s/it] 70%|███████   | 70/100 [04:43<01:52,  3.76s/it] 71%|███████   | 71/100 [04:46<01:47,  3.71s/it] 72%|███████▏  | 72/100 [04:50<01:46,  3.82s/it] 73%|███████▎  | 73/100 [04:55<01:51,  4.14s/it] 74%|███████▍  | 74/100 [05:01<01:58,  4.56s/it] 75%|███████▌  | 75/100 [05:05<01:50,  4.41s/it] 76%|███████▌  | 76/100 [05:08<01:39,  4.16s/it] 77%|███████▋  | 77/100 [05:12<01:31,  3.97s/it] 78%|███████▊  | 78/100 [05:16<01:25,  3.90s/it] 79%|███████▉  | 79/100 [05:20<01:20,  3.85s/it] 80%|████████  | 80/100 [05:23<01:15,  3.76s/it] 81%|████████  | 81/100 [05:27<01:11,  3.77s/it] 82%|████████▏ | 82/100 [05:32<01:12,  4.05s/it] 83%|████████▎ | 83/100 [05:36<01:11,  4.20s/it] 84%|████████▍ | 84/100 [05:41<01:11,  4.46s/it] 85%|████████▌ | 85/100 [05:46<01:08,  4.59s/it] 86%|████████▌ | 86/100 [05:50<00:59,  4.27s/it] 87%|████████▋ | 87/100 [05:53<00:52,  4.03s/it] 88%|████████▊ | 88/100 [05:57<00:48,  4.05s/it] 89%|████████▉ | 89/100 [06:01<00:44,  4.06s/it] 90%|█████████ | 90/100 [06:05<00:41,  4.11s/it] 91%|█████████ | 91/100 [06:09<00:36,  4.09s/it] 92%|█████████▏| 92/100 [06:13<00:32,  4.05s/it] 93%|█████████▎| 93/100 [06:18<00:30,  4.34s/it] 94%|█████████▍| 94/100 [06:25<00:29,  4.95s/it] 95%|█████████▌| 95/100 [06:31<00:26,  5.34s/it] 96%|█████████▌| 96/100 [06:35<00:19,  4.93s/it] 97%|█████████▋| 97/100 [06:39<00:13,  4.51s/it] 98%|█████████▊| 98/100 [06:42<00:08,  4.24s/it] 99%|█████████▉| 99/100 [06:46<00:04,  4.10s/it]100%|██████████| 100/100 [06:50<00:00,  4.17s/it]100%|██████████| 100/100 [06:51<00:00,  4.11s/it]
***** eval metrics *****
  eval_accuracy           =     0.7895
  eval_loss               =     0.8253
  eval_runtime            = 0:06:54.85
  eval_samples_per_second =    120.525
  eval_steps_per_second   =      0.241
